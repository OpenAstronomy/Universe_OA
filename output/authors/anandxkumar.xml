<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Universe OpenAstronomy (Posts by anandxkumar)</title><link>http://openastronomy.org/Universe_OA/</link><description></description><atom:link href="http://openastronomy.org/Universe_OA/authors/anandxkumar.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Tue, 30 Jul 2024 01:04:27 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Chapter 5: Birds of a Feather</title><link>http://openastronomy.org/Universe_OA/posts/2021/08/20210823_1424_anandxkumar/</link><dc:creator>anandxkumar</dc:creator><description>&lt;p align="center"&gt;
&lt;span class="gatsby-resp-image-wrapper" style="display: block; margin-left: auto; margin-right: auto;"&gt;
&lt;a class="gatsby-resp-image-link" href="https://anandkumar-blog.netlify.app/static/91ec1b99c3601cea5ada6089b36f443e/63868/Radis.png" rel="noopener" style="display: block;" target="_blank"&gt;
&lt;!-- TEASER_END --&gt;
&lt;span class="gatsby-resp-image-background-image" style="padding-bottom: 100%; display: block;"&gt;&lt;/span&gt;
&lt;img alt="Radis.png" class="gatsby-resp-image-image" src="https://anandkumar-blog.netlify.app/static/91ec1b99c3601cea5ada6089b36f443e/63868/Radis.png" style="width: 100%; height: 100%; margin: 0; vertical-align: middle;" title="Radis.png"&gt;
&lt;/a&gt;
&lt;/span&gt;&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;So &lt;code class="language-text"&gt;GSoC 2021&lt;/code&gt; has officially ended and I can say without a doubt that what a journey it was. I recently concluded with my GSoC project, the final PR got merged and I’m quite satisfied with the outcome. &lt;/p&gt;
&lt;p&gt;Earlier we were able to find the time complexity of &lt;strong&gt;LBL&amp;gt;Voigt&lt;/strong&gt;, &lt;strong&gt;DIT&amp;gt;Voigt&lt;/strong&gt; and &lt;strong&gt;DIT&amp;gt;FFT&lt;/strong&gt; (Formely known as LDM&amp;gt;FFT). On a small test replacing &lt;code class="language-text"&gt;np.convolve&lt;/code&gt; with &lt;code class="language-text"&gt;scipy.signal.oaconvolve&lt;/code&gt;, we were able to achieve 2 to 30 times performance boost. So we re-ran the benchmarks and were able to confirm this fact.
You can see the result at &lt;a href="https://anandxkumar.github.io/Benchmark_Visualization_GSoC_2021/"&gt;Benchmark Visualization GSoC 2021&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The above results proved that &lt;code class="language-text"&gt;DIT&amp;gt;Voigt&lt;/code&gt; performs better than &lt;code class="language-text"&gt;DIT&amp;gt;FFT&lt;/code&gt; in almost every case. So we decided to use &lt;code class="language-text"&gt;DIT&amp;gt;Voigt&lt;/code&gt; as the default setting in &lt;code class="language-text"&gt;Radis&lt;/code&gt;. &lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;predict_time()&lt;/strong&gt; function was added, which computes the predicted time for &lt;strong&gt;LBL&amp;gt;Voigt&lt;/strong&gt;, &lt;strong&gt;DIT&amp;gt;Voigt&lt;/strong&gt; and &lt;strong&gt;DIT&amp;gt;FFT&lt;/strong&gt; using the derived time complexity, and on &lt;code class="language-text"&gt;verbose&amp;gt;=2&lt;/code&gt; shows the user the predicted time.&lt;/p&gt;
&lt;p&gt;Also we Bifurcated &lt;code class="language-text"&gt;broadening_max_width&lt;/code&gt; into 2 parameters:&lt;br&gt;
•  &lt;strong&gt;Truncation:&lt;/strong&gt; Used in truncation of Voigt method.&lt;br&gt;
•  &lt;strong&gt;neighbour_lines:&lt;/strong&gt; Increases Spectral range&lt;br&gt;&lt;/p&gt;
&lt;p&gt;So now users have a lot of flexibility. Based on Physics, the default value of &lt;strong&gt;truncation&lt;/strong&gt; is set to &lt;strong&gt;50cm-1&lt;/strong&gt; and the default value of &lt;strong&gt;neighbour_lines&lt;/strong&gt; is set to &lt;strong&gt;0 cm-1&lt;/strong&gt;. Apart from this, some minor improvements were done in the &lt;code class="language-text"&gt;Profiler class&lt;/code&gt; such as an improved algorithm is used to store data and now calculation time gets appended to the same key rather than overwriting it, which useful when we use &lt;code class="language-text"&gt;chunksize&lt;/code&gt; or DIT optimization for &lt;code class="language-text"&gt;Non_equilibrium&lt;/code&gt; conditions. &lt;/p&gt;
&lt;p&gt;So overall the code has been optimized and a user can expect a performance boost upto &lt;strong&gt;40x&lt;/strong&gt; in worst scenarios. &lt;/p&gt;
&lt;p&gt;You can find all my work during the GSoC period &lt;a href="https://github.com/radis/radis/projects/5"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It was a great experience contributing to &lt;strong&gt;Radis&lt;/strong&gt; and I definitely have learned alot along the way. And a big thanks to the great mentors at Radis especially &lt;a href="https://github.com/erwanp"&gt;Erwan Pannier&lt;/a&gt; who guided me at every stage of the program. The road doesn’t end here as I will stick around the organisation and will always find ways to contribute to Radis. One last thanks to &lt;strong&gt;GSoC&lt;/strong&gt; for providing such a wonderful opportunity.&lt;/p&gt;
&lt;p align="center"&gt;
Till we meet again, keep &lt;b&gt;Swinging for the fences.&lt;/b&gt;
&lt;br&gt;
&lt;img alt="/08e70d471ee717d1624f04f21c586cd4/spidermanMM_traversal.gif" src="https://anandkumar-blog.netlify.app/08e70d471ee717d1624f04f21c586cd4/spidermanMM_traversal.gif" width="500"&gt;&lt;br&gt;
&lt;/p&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2021/08/20210823_1424_anandxkumar/</guid><pubDate>Mon, 23 Aug 2021 13:24:32 GMT</pubDate></item><item><title>Chapter 4: The Other Side</title><link>http://openastronomy.org/Universe_OA/posts/2021/08/20210801_1424_anandxkumar/</link><dc:creator>anandxkumar</dc:creator><description>&lt;p&gt;A new month has started and I have started to see the light at the end of the tunnel. Good Morning and welcome back. Phase 2 has been rolling and let us look at the new findings.&lt;/p&gt;
&lt;p&gt;Earlier the complexity of Legacy method was determined. The complexity of LDM Voigt and LDM FFT was to be determined using similar approach. Upon executing several benchmarks based on Number of lines, Spectum range, wstep, broadening max width. Previously it was thought the complexity was: &lt;br&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;time(LDM_fft) ~ c2*Nlines + c3*(N_G*N_L + 1)*N_v*log(N_v) (where N_v =  Spectral Points)
&lt;!-- TEASER_END --&gt;
time(LDM_voigt) ~ c2*Nlines + c3'*(N_G*N_L + 1)*N_truncation*log(N_truncation) (where N_truncation = broadening width / wstep)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;But in actual Re running all benchmarks for &lt;strong&gt;LDM&amp;gt;Voigt&lt;/strong&gt; and &lt;strong&gt;LDM&amp;gt;FFT&lt;/strong&gt; with a &lt;code class="language-text"&gt;broadening max width = 300 cm-1&lt;/code&gt;. All benchmarks and visualizations can be found &lt;a href="https://anandxkumar.github.io/Benchmark_Visualization_GSoC_2021/"&gt;here&lt;/a&gt; we were able to conclude the followings:&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;FFT:&lt;/strong&gt;&lt;br&gt;
•  Complexity doesn’t depend on Nlines but rather wL x wG ; check this benchmark: &lt;a href="https://public.tableau.com/app/profile/anand.kumar4841/viz/LDMLinesvsCalculationTimeUpdatedCO2/Sheet1"&gt;link&lt;/a&gt;, it certainly looks like Complexity ∝ Nlines but its actually dependent on wL and wG, and gives same result on (wL x wG+ 1) x Spectral&lt;em&gt;Points x Log(Spectral&lt;/em&gt;Points).&lt;br&gt;
•  Upon implementing multiple linear regression for &lt;strong&gt;c1 x Nlines + c2 x (wL x wG+ 1)*Spectral&lt;em&gt;Points x Log(Spectral&lt;/em&gt;Points)&lt;/strong&gt; gives &lt;code class="language-text"&gt;c1=2.65e-07&lt;/code&gt;, &lt;code class="language-text"&gt;c2=4.48256e-08&lt;/code&gt; but their &lt;code class="language-text"&gt;p value = 0.648 and 0.00001&lt;/code&gt;, and &lt;code class="language-text"&gt;p&amp;gt;0.05&lt;/code&gt; are insignificant, thus Nlines is insignificant for determining the complexity.&lt;br&gt;
•  Since FFT is independent of broadening max width; benchmark: &lt;a href="https://public.tableau.com/app/profile/anand.kumar4841/viz/LDMVoigtandFFTBMW_NEW/Sheet1"&gt;link&lt;/a&gt;, so on comparing it Spectral point gives us same same time. Thus Spectral Point =  (wavenum max - wavenum max)/wstep instead of (wavenum maxcalc - wavenum min calc)/wstep&lt;br&gt;
•  &lt;strong&gt;Overall complexity =  4.48256897e-08 x (wL x wG+ 1) x Spectral&lt;em&gt;Points(without BMW) x Log(Spectral&lt;/em&gt;Points(without BMW))&lt;/strong&gt;  &lt;a href="https://anandxkumar.github.io/Benchmark_Visualization_GSoC_2021/LDM/Complexity_FFT_Final/Complexity_FFT_Final.html"&gt;link&lt;/a&gt; (with the help of multple linear regression using sklearn; is almost accurate)&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Voigt:&lt;/strong&gt;&lt;br&gt;
•  Similar to 1st point of FFT.&lt;br&gt;
•  Upon doing multiple linear regression for &lt;strong&gt;c1 x N lines + c2 x (wL x wG + 1) x SpectralPoints x BMW xLog(SpectralPoints x (BMW) )&lt;/strong&gt; gives &lt;code class="language-text"&gt;c1=-1.9392e-06, c2=1.28256e-09&lt;/code&gt; but their &lt;code class="language-text"&gt;p value = 0.848 and 0.00001&lt;/code&gt;, and &lt;code class="language-text"&gt;p&amp;gt;0.05&lt;/code&gt; are insignificant, thus N&lt;em&gt;lines is insignificant for determining the complexity.&lt;br&gt;
•  Calculation time is dependent `Broadening&lt;/em&gt;Max&lt;em&gt;width`, but upon inspections with Spectral Points, we have the exact same plot. So complexity is dependent only on Spectral Points but with broadening&lt;/em&gt;max&lt;em&gt;width i.e. wavenum&lt;/em&gt;calc, which causes the increase in computational time on increasing broadening&lt;em&gt;max&lt;/em&gt;width.&lt;br&gt;
•  &lt;strong&gt;Overall complexity = 5.26795e-07 * (wL x wG+ 1)*Spectral Points x Log(Spectral Points)&lt;/strong&gt; &lt;a href="https://anandxkumar.github.io/Benchmark_Visualization_GSoC_2021/LDM/Complexity_Voigt_Final/Complexity_Voigt_Final.html"&gt;link&lt;/a&gt; (with the help of multple linear regression using sklearn; almost straight)&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Also:&lt;/strong&gt; From all the above plots, it really clear if going with broadening&lt;em&gt;max&lt;/em&gt;width=300cm-1 in wavespace, it will take alot more time than fft in all aspects.&lt;/p&gt;
&lt;p&gt;But upon replacing &lt;code class="language-text"&gt;np.convolve&lt;/code&gt; with &lt;code class="language-text"&gt;scipy.signal.oaconvolve&lt;/code&gt;, we were able to achieve &lt;code class="language-text"&gt;2 to 30&lt;/code&gt; times performance boost. So it will be interesting to re run benchmarks with the latest piece of code and see which method performs better. Also some benchmarks will be added to ASV benchmark too to see how its performance changes over time.&lt;/p&gt;
&lt;p&gt;Also profiler was modified to a tree like a stucture using &lt;code class="language-text"&gt;OrderedDict&lt;/code&gt; and &lt;code class="language-text"&gt;YAML&lt;/code&gt; has been used to print the profiler in a proper structued way using &lt;strong&gt;Spectrum.print_perf_profiler()&lt;/strong&gt; or &lt;strong&gt;SpectrumFactory.print_perf_profiler()&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;s = calc_spectrum(1900, 2300,         # cm-1
molecule='CO',
isotope='1,2,3',
pressure=1.01325,   # bar
Tvib=1000,          # K
Trot=300,           # K
mole_fraction=0.1,
verbose=3,
)
s.print_perf_profile()&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Gives the following output:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;&amp;gt;&amp;gt;&amp;gt; spectrum_calculation:
&amp;gt;&amp;gt;&amp;gt;   applied_linestrength_cutoff: 0.0024361610412597656
&amp;gt;&amp;gt;&amp;gt;   calc_emission_integral: 0.006468772888183594
&amp;gt;&amp;gt;&amp;gt;   calc_hwhm: 0.006415128707885742
&amp;gt;&amp;gt;&amp;gt;   calc_line_broadening:
&amp;gt;&amp;gt;&amp;gt;     DLM_Distribute_lines: 0.0003898143768310547
&amp;gt;&amp;gt;&amp;gt;     DLM_Initialized_vectors: 9.775161743164062e-06
&amp;gt;&amp;gt;&amp;gt;     DLM_closest_matching_line: 0.0005028247833251953
&amp;gt;&amp;gt;&amp;gt;     DLM_convolve: 0.029767990112304688
&amp;gt;&amp;gt;&amp;gt;     precompute_DLM_lineshapes: 0.013132810592651367
&amp;gt;&amp;gt;&amp;gt;     value: 0.07619166374206543
&amp;gt;&amp;gt;&amp;gt;   calc_lineshift: 0.00074005126953125
&amp;gt;&amp;gt;&amp;gt;   calc_noneq_population:
&amp;gt;&amp;gt;&amp;gt;     part_function: 0.03405046463012695
&amp;gt;&amp;gt;&amp;gt;     population: 0.005669832229614258
&amp;gt;&amp;gt;&amp;gt;     value: 0.03983640670776367
&amp;gt;&amp;gt;&amp;gt;   calc_other_spectral_quan: 0.002928495407104492
&amp;gt;&amp;gt;&amp;gt;   calc_weight_trans: 0.008247852325439453
&amp;gt;&amp;gt;&amp;gt;   check_line_databank: 0.0002810955047607422
&amp;gt;&amp;gt;&amp;gt;   check_non_eq_param: 0.04109525680541992
&amp;gt;&amp;gt;&amp;gt;   fetch_energy_5: 0.014983654022216797
&amp;gt;&amp;gt;&amp;gt;   generate_spectrum_obj: 0.00032138824462890625
&amp;gt;&amp;gt;&amp;gt;   generate_wavenumber_arrays: 0.0010433197021484375
&amp;gt;&amp;gt;&amp;gt;   reinitialize:
&amp;gt;&amp;gt;&amp;gt;     copy_database: 2.1457672119140625e-06
&amp;gt;&amp;gt;&amp;gt;     memory_usage_warning: 0.0018389225006103516
&amp;gt;&amp;gt;&amp;gt;     reset_population: 2.6226043701171875e-05
&amp;gt;&amp;gt;&amp;gt;     value: 0.001964569091796875
&amp;gt;&amp;gt;&amp;gt;   scaled_non_eq_linestrength:
&amp;gt;&amp;gt;&amp;gt;     corrected_population_se: 0.002747774124145508
&amp;gt;&amp;gt;&amp;gt;     map_part_func: 0.0010590553283691406
&amp;gt;&amp;gt;&amp;gt;     value: 0.0038983821868896484
&amp;gt;&amp;gt;&amp;gt;   value: 0.1904621124267578&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So at the end a productive week! Looking forward to conclude GSoC with a worthy ending :)&lt;/p&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2021/08/20210801_1424_anandxkumar/</guid><pubDate>Sun, 01 Aug 2021 13:24:32 GMT</pubDate></item><item><title>Chapter 3: Midnight Sun</title><link>http://openastronomy.org/Universe_OA/posts/2021/07/20210719_1645_anandxkumar/</link><dc:creator>anandxkumar</dc:creator><description>&lt;p&gt;Phase 1 is over :) ! We are half way through the journey. Great learning experience so far. Let’s find out what I accomplished during the previous 2 weeks (since I believe you have been following me from the beginning ;)&lt;/p&gt;
&lt;p&gt;Getting straight to the point, most of the time was spent on fixing bugs of the Profiler class and other Pull requests regarding documentation and gallery example. A new gallery example was added to demonstrate the working of &lt;code class="language-text"&gt;SpecDatabase&lt;/code&gt; and &lt;code class="language-text"&gt;init_database&lt;/code&gt; to help user to store all Spectrums in the form of a &lt;code class="language-text"&gt;.spec&lt;/code&gt; file and all input parameters in a &lt;code class="language-text"&gt;csv&lt;/code&gt; file under a folder. The same folder can be used to retrieve all Spectrums thus saving a lot of time and also no need to recompute all spectrums, so quite a handy feature. Radis has &lt;code class="language-text"&gt;plot_cond&lt;/code&gt; function to plot a 2D heat map based on the parameters in csv file for all spectrums. Creates some good looking and informative plots :) &lt;br&gt;-&amp;gt; &lt;a href="https://radis.readthedocs.io/en/latest/auto_examples/plot_SpecDatabase.html#sphx-glr-auto-examples-plot-specdatabase-py"&gt;Gallery Example&lt;/a&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Back to the analysis part; for LDM we expected:&lt;br&gt;&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;time(LDM_fft) ~ c2*N_lines + c3*(N_G*N_L + 1)*N_v*log(N_v) (where N_v =  Spectral Points)
time(LDM_voigt) ~ c2*N_lines + c3'*(N_G*N_L + 1)*N_truncation*log(N_truncation) (where N_truncation = broadening width / wstep)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For Legacy method I was able to prove that Calculation Time is independent of Spectral Range if we keep the N&lt;em&gt;lines and wstep constant but same is not for LDM voigt.&lt;br&gt;
A straight up comparison between Legacy and LDM voigt for NO  keeping N&lt;/em&gt;lines and wstep constant and varying the Spectral range:
&lt;a href="https://public.tableau.com/app/profile/anand.kumar4841/viz/LDMvsLegacyforSpectralRangeN_linesconstantandVoigtbroadening/Sheet1"&gt;Link&lt;/a&gt;&lt;br&gt;
Here also for None optimization we are getting constant time for different spectral range but a linear dependency for LDM Voigt which will fail the assumption of&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;t_LDM_voigt ~ c2*N_lines + c3'*(N_G*N_L + 1)*N_truncation  *log(N_truncation  )
but rather t_LDM_voigt ~ c2*N_lines + c3*(N_G*N_L + 1)*N_v*log(N_v)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;A New Discovery&lt;/h3&gt;
&lt;p&gt;On generating spectrum for millions of lines, one unique observation was seen. The bottleneck step was no longer taking the most time. Max time was spent upon an unknown process. Upon deep analysis it was found a part of code was using &lt;code class="language-text"&gt;sys.getsizeof()&lt;/code&gt; to get the size of dataframe, and when the dataframe consisited of &lt;code class="language-text"&gt;object&lt;/code&gt; type columns with millions of lines, most of the time was spent on this step only.&lt;/p&gt;
&lt;p&gt;&lt;span class="gatsby-resp-image-wrapper" style="display: block; margin-left: auto; margin-right: auto;"&gt;
&lt;a class="gatsby-resp-image-link" href="https://anandkumar-blog.netlify.app/static/95eda74e349d883f4a1fcc85291a91cc/6af66/ldm.png" rel="noopener" style="display: block;" target="_blank"&gt;
&lt;span class="gatsby-resp-image-background-image" style="display: block;"&gt;&lt;/span&gt;
&lt;img alt="complexity.jpg" class="gatsby-resp-image-image" src="https://anandkumar-blog.netlify.app/static/95eda74e349d883f4a1fcc85291a91cc/f058b/ldm.png" style="width: 100%; height: 100%; margin: 0; vertical-align: middle;" title="complexity.jpg"&gt;
&lt;/a&gt;
&lt;/span&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;We replaced it with &lt;code class="language-text"&gt;memory_usage(deep=False)&lt;/code&gt; with a different threshold which made computation almost &lt;strong&gt;2x&lt;/strong&gt; faster.&lt;/p&gt;
&lt;p&gt;&lt;span class="gatsby-resp-image-wrapper" style="display: block; margin-left: auto; margin-right: auto;"&gt;
&lt;a class="gatsby-resp-image-link" href="https://anandkumar-blog.netlify.app/static/28b1ad4d276fa9921520808bc6360002/87488/ba.png" rel="noopener" style="display: block;" target="_blank"&gt;
&lt;span class="gatsby-resp-image-background-image" style="display: block;"&gt;&lt;/span&gt;
&lt;img alt="complexity.jpg" class="gatsby-resp-image-image" src="https://anandkumar-blog.netlify.app/static/28b1ad4d276fa9921520808bc6360002/f058b/ba.png" style="width: 100%; height: 100%; margin: 0; vertical-align: middle;" title="complexity.jpg"&gt;
&lt;/a&gt;
&lt;/span&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;So phase 1 is over,and phase 2 is going to begin which will mainly focus on optimizing the the existing LDM method with appropriate truncation and other possible areas!&lt;/p&gt;
&lt;p&gt;See you on the other side of the sea ;)&lt;/p&gt;
&lt;p&gt;&lt;span class="gatsby-resp-image-wrapper" style="display: block; margin-left: auto; margin-right: auto;"&gt;
&lt;a class="gatsby-resp-image-link" href="https://anandkumar-blog.netlify.app/static/6c695ad1951b1c737cc12c701ffce0e4/2551b/other.jpg" rel="noopener" style="display: block;" target="_blank"&gt;
&lt;span class="gatsby-resp-image-background-image" style="display: block;"&gt;&lt;/span&gt;
&lt;img alt="complexity.jpg" class="gatsby-resp-image-image" src="https://anandkumar-blog.netlify.app/static/6c695ad1951b1c737cc12c701ffce0e4/828fb/other.jpg" style="width: 100%; height: 100%; margin: 0; vertical-align: middle;" title="complexity.jpg"&gt;
&lt;/a&gt;
&lt;/span&gt;&lt;br&gt;&lt;/p&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2021/07/20210719_1645_anandxkumar/</guid><pubDate>Mon, 19 Jul 2021 15:45:32 GMT</pubDate></item><item><title>Chapter 2: Survey Corps</title><link>http://openastronomy.org/Universe_OA/posts/2021/07/20210705_2340_anandxkumar/</link><dc:creator>anandxkumar</dc:creator><description>&lt;p&gt;So its been around 4 weeks into the coding period, a lot of insights and progress so far!&lt;/p&gt;
&lt;h3&gt;Profiler Class&lt;/h3&gt;
&lt;p&gt;The good news is that the Profiler class has been successfully implemented in the develop branch and will be available to users by version &lt;code class="language-text"&gt;0.9.30&lt;/code&gt; .&lt;br&gt;
&lt;!-- TEASER_END --&gt;
Link : &lt;a href="https://github.com/radis/radis/pull/286"&gt;Profiler PR&lt;/a&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Below is a simple example how all steps are printed based on the verbose level:&lt;br&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;wmin = 2000
wmax = 3300
wstep = 0.01
T = 3000.0 #K
p = 0.1 #bar
broadening_max_width=10

sf = SpectrumFactory(wavenum_min=wmin, wavenum_max=wmax,
pressure=p,
wstep=wstep,
broadening_max_width=broadening_max_width,
molecule="CO",
cutoff=0, # 1e-27,
verbose=3,
)
sf.load_databank('HITEMP-CO')
s = sf.eq_spectrum(T)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Output:&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;... Scaling equilibrium linestrength
... 0.01s - Scaled equilibrium linestrength
... 0.00s - Calculated lineshift
... 0.00s - Calculate broadening HWHM
... Calculating line broadening (60869 lines: expect ~ 6.09s on 1 CPU)
...... 0.16s - Precomputed DLM lineshapes (30)
...... 0.00s - Initialized vectors
...... 0.00s - Get closest matching line &amp;amp; fraction
...... 0.02s - Distribute lines over DLM
...... 1.95s - Convolve and sum on spectral range
... 2.14s - Calculated line broadening
... 0.01s - Calculated other spectral quantities
... 2.21s - Spectrum calculated (before object generation)
... 0.01s - Generated Spectrum object
2.22s - Spectrum calculated&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Also we can access these steps and the time taken by them using &lt;code class="language-text"&gt;Spectrum.get_conditions()['profiler']&lt;/code&gt;. Also there is a parameter &lt;code class="language-text"&gt;SpectrumFactory.profiler.relative_time_percentage&lt;/code&gt; that stores the percentage of time taken by each steps at a particular verbose level, helpful seeing the most expensive steps in Spectrum calculation.&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Legacy Method Complexity&lt;/h3&gt;
&lt;p&gt;Several Spectrums were benchmarked against various parameters to see it’s correlation and derive its complexity. We used Profiler class with &lt;a href="https://radis.readthedocs.io/en/latest/source/radis.lbl.loader.html#radis.lbl.loader.DatabankLoader.init_database"&gt;init_database()&lt;/a&gt; which stores all parameters of Spectrum along the Profiler in a &lt;code class="language-text"&gt;csv&lt;/code&gt; generated file; all spectrum info got added into the csv file  which could be used to do create visualizations to analyze the data. We used &lt;code class="language-text"&gt;Xexplorer&lt;/code&gt; library and &lt;code class="language-text"&gt;Tableau&lt;/code&gt;(a visual analytics platform) to create visualizations. A &lt;a href="https://github.com/anandxkumar/Benchmark_Visualization_GSoC_2021"&gt;github repository&lt;/a&gt; was created to store the Visualization along the CSV data file of each benchmark.&lt;/p&gt;
&lt;p&gt;Following are the inference of the benchmarks for Legacy Method:&lt;/p&gt;
&lt;b&gt;
•  Calculation Time ∝ Number of lines&lt;br&gt;
•  Calculation Time ∝ Broadening max width&lt;br&gt;
•  Calculation Time ∝ 1/wstep&lt;br&gt;
•  Calculation Time not dependent on Spectral Range&lt;br&gt;
&lt;/b&gt;&lt;br&gt;
&lt;p&gt;So complexity of Legacy method can be derived as: &lt;br&gt;
&lt;strong&gt;&lt;code class="language-text"&gt;complexity = constant * Number of lines * Broadening Max Width / Wstep&lt;/code&gt;&lt;/strong&gt; &lt;br&gt;&lt;/p&gt;
&lt;h3&gt;LDM Method Complexity&lt;/h3&gt;
&lt;p&gt;Similar technique was used to benchmark LDM method. Now LDM uses 2 types of broadening method that are &lt;code class="language-text"&gt;voigt&lt;/code&gt; and &lt;code class="language-text"&gt;fft&lt;/code&gt;. &lt;code class="language-text"&gt;voigt&lt;/code&gt; uses truncation for calculating spectrum  in wavenmber space where as &lt;code class="language-text"&gt;fft&lt;/code&gt; calculates spectrum on entire spectral range in fourier space. So benchmarks were done on both methods to compare their performance against various parameters.&lt;/p&gt;
&lt;p&gt;Spectrum were benchmarked against parameters like Spectral Range, Wstep, Spectral Points, Number of Lines and Broadening Max Width. Following are the inferences.&lt;/p&gt;
&lt;p&gt;For &lt;code class="language-text"&gt;fft&lt;/code&gt;:&lt;br&gt;
&lt;b&gt;
• Calculation Time ∝ Spectral Points&lt;br&gt;
• Calculation Time ∝ Number of Lines&lt;br&gt;
&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;For &lt;code class="language-text"&gt;voigt&lt;/code&gt;:&lt;br&gt;
&lt;b&gt;
• Calculation Time ∝ Spectral Points&lt;br&gt;
• Calculation Time ∝ Number of Lines&lt;br&gt;
• Calculation Time ∝ Broadening Max Width&lt;br&gt;
&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;For LDM we are expecting the following complexity:&lt;br&gt;
&lt;strong&gt;&lt;code class="language-text"&gt;t_LDM_fft ~ c2*N_lines + c3*(N_G*N_L + 1)*N_v*log(N_v)&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
&lt;strong&gt;&lt;code class="language-text"&gt;t_LDM_voigt ~ c2*N_lines + c3'*(N_G*N_L + 1)*N_truncation*log(N_truncation)&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt; So the goal for the next 2 weeks will be to get the complexity of both &lt;code class="language-text"&gt;voigt&lt;/code&gt; and &lt;code class="language-text"&gt;fft&lt;/code&gt; method and see places for improving both methods and quite possibily create a &lt;code class="language-text"&gt;Hybrid&lt;/code&gt; method taking the best of both worlds. &lt;/p&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2021/07/20210705_2340_anandxkumar/</guid><pubDate>Mon, 05 Jul 2021 22:40:32 GMT</pubDate></item><item><title>Chapter 1: First Flight</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210621_2240_anandxkumar/</link><dc:creator>anandxkumar</dc:creator><description>&lt;p&gt;Hey! Missed me? I’m back with another blog, the first related to the Coding Period. Got some progress and interesting observation to share!&lt;/p&gt;
&lt;h3&gt;Ready -&amp;gt; Set -&amp;gt; Code -&amp;gt; Analyze&lt;/h3&gt;
&lt;p&gt;The first thing I did in the coding period, was analyse the problem and get a feasible approach to resolve it.&lt;br&gt;&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; Find the complexity of the Legacy and LDM method.&lt;br&gt;
&lt;strong&gt;Solution:&lt;/strong&gt; Run some benchmarks and find the bottleneck step.&lt;br&gt;&lt;/p&gt;
&lt;p&gt;First I chose the &lt;strong&gt;Legacy&lt;/strong&gt; method because if its simpler architecture. I ran some benchmarks varying the &lt;code class="language-text"&gt;spectral range&lt;/code&gt; of &lt;code class="language-text"&gt;OH&lt;/code&gt; and &lt;code class="language-text"&gt;CO2&lt;/code&gt; molecule to get similar number of lines. I kept parameters like &lt;code class="language-text"&gt;pressure&lt;/code&gt;, &lt;code class="language-text"&gt;temperature&lt;/code&gt;, &lt;code class="language-text"&gt;broadening_max_width&lt;/code&gt;, &lt;code class="language-text"&gt;wstep&lt;/code&gt;, etc constant to see the dependence of Legacy method on &lt;strong&gt;Spectral range&lt;/strong&gt;. &lt;br&gt;&lt;/p&gt;
&lt;p&gt;In order to get similar number of lines, I created a function which will take the &lt;strong&gt;Spectrum Factory&lt;/strong&gt; &lt;code class="language-text"&gt;dataframe&lt;/code&gt; and select the target number of lines. But the issue with Pandas dataframe is that when modify the dataframe there are chances that the metadata will get lost and we will no longer be able to do Spectrum calculation. To avoid this we have to drop the right number of lines with &lt;code class="language-text"&gt;inplace=True&lt;/code&gt;. So we will need to fix the number of lines and then we can proceed ahead with the benchmarking. Every parameter is the same except the Spectral Range.  Full code &lt;a href="https://gist.github.com/anandxkumar/cbe12f47170e1d71a82f4b246bd01dcc"&gt;here&lt;/a&gt;.&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Earlier we assumed that the complexity of Legacy method is: &lt;br&gt;
&lt;strong&gt;&lt;code class="language-text"&gt;Voigt Broadening = Broadening_max_width * spectral_range/math.pow(wstep,2) * N&lt;/code&gt;&lt;/strong&gt; &lt;br&gt;&lt;/p&gt;
&lt;p&gt;Thus I was expecting to have different calculation time for both benchmarks. But to my surprise the computational times were almost equivalent! I re-ran each benchmarks &lt;strong&gt;100 times&lt;/strong&gt; just to be sure and more precise about it. Following were the observations:&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Number of lines - &lt;b&gt;{‘OH’: 28143, ‘CO’: 26778}&lt;/b&gt;&lt;/li&gt;
&lt;li&gt;Total Calculation time(Avg) -  &lt;b&gt;{‘OH’: 4.4087, ‘CO’: 3.8404000000000003}&lt;/b&gt;&lt;/li&gt;
&lt;li&gt;Total Voigt_Broadening TIME(Avg) - &lt;b&gt;{‘OH’: 3.1428814244270327, ‘CO’: 3.081623389720917}&lt;/b&gt;&lt;/li&gt;
&lt;li&gt;spectral_range - &lt;b&gt;{‘OH’: 38010, ‘CO’: 8010}&lt;/b&gt;&lt;/li&gt;
&lt;li&gt;Legacy_Scale - &lt;b&gt;{‘OH’: 4x10^14, ‘CO’: 8x10^13}&lt;/b&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are some inference we can make from the above observation:&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A)&lt;/strong&gt; The bottleneck step(Voigt Broadening) loosely depends on &lt;code class="language-text"&gt;Spectral Range&lt;/code&gt;.&lt;br&gt;
&lt;strong&gt;B)&lt;/strong&gt; The complexity of Voigt Broadening needs to be modified because there is a difference of order of &lt;strong&gt;~10&lt;/strong&gt; in the Legacy Scaled value of OH and CO2.&lt;br&gt;&lt;/p&gt;
&lt;p align="center"&gt;
&lt;span class="gatsby-resp-image-wrapper" style="display: block; margin-left: auto; margin-right: auto;"&gt;
&lt;a class="gatsby-resp-image-link" href="https://anandkumar-blog.netlify.app/static/d9b32b4e96e6cd9a91016a49ad940239/0b533/Blog2.png" rel="noopener" style="display: block;" target="_blank"&gt;
&lt;span class="gatsby-resp-image-background-image" style="padding-bottom: 100%; display: block;"&gt;&lt;/span&gt;
&lt;img alt="Blog2" class="gatsby-resp-image-image" src="https://anandkumar-blog.netlify.app/static/d9b32b4e96e6cd9a91016a49ad940239/0b533/Blog2.png" style="width: 100%; height: 100%; margin: 0; vertical-align: middle;" title="Blog2"&gt;
&lt;/a&gt;
&lt;/span&gt;&lt;br&gt;
&lt;b&gt;Credits - Me :p&lt;/b&gt;&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;So in order to do some analysis, we first need data of different steps in the broadening phase and conditions of various Spectrum which brings me to the &lt;strong&gt;Code&lt;/strong&gt; part in &lt;strong&gt;Coding Period.&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Profiler Class&lt;/h3&gt;
&lt;p&gt;The aim of this class is to replace all the print statements by a common &lt;code class="language-text"&gt;start&lt;/code&gt;, &lt;code class="language-text"&gt;stop&lt;/code&gt;, &lt;code class="language-text"&gt;_print&lt;/code&gt; method. Earlier each step computational time was done using &lt;code class="language-text"&gt;time()&lt;/code&gt; library. Now the whole codebase is being refactored with the Profiler class that will do all the work based on the &lt;code class="language-text"&gt;verbose&lt;/code&gt; level. In addition to this the biggest benefit is that each step will be stored in a dictionary with its computational time that will help me gather data to find which step is in actual bottleneck and further which part of the function is the most expensive time wise. A simple example is below:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Before:&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;if __debug__:
t0 = time()
..........
..........
if __debug__:
t1 = time()
.........
.........
if __debug__:
if self.verbose &amp;gt;= 3:
printg("... Initialized vectors in {0:.1f}s".format(t1 - t0))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;After:&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;self.profiler.start(
key="init_vectors", verbose=3, details="Initialized vectors"
)
.........
.........
self.profiler.stop("init_vectors")&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So using a common key we can make it happen. This will be stored in the conditons of &lt;code class="language-text"&gt;Spectrum&lt;/code&gt; object in the &lt;code class="language-text"&gt;'profiler'&lt;/code&gt; key. All these Spectrums and their conditions can be exported using a &lt;a href="https://radis.readthedocs.io/en/latest/spectrum/spectrum.html#spectrum-database"&gt;SpecDatabase&lt;/a&gt;. This will create a csv file comprising of all the parameters of all Spectrums which will be useful in getting some insights.
-&amp;gt; &lt;a href="https://github.com/radis/radis/pull/286"&gt;PR LINK&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Digging in whiting_jit&lt;/h3&gt;
&lt;p&gt;Based on several benchmarks, it is estimated that around &lt;strong&gt;70-80%&lt;/strong&gt; time is spent on calculating the broadening. The broadening part has the following hierarchy:&lt;br&gt;
&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;_calc_broadening()
-&amp;gt; _calc_lineshape()
-&amp;gt; _voigt_broadening()
-&amp;gt; _voigt_lineshape()
-&amp;gt; whiting_jit()&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;On close inspection we observed that &lt;strong&gt;80-90%&lt;/strong&gt; time is spent on &lt;code class="language-text"&gt;whiting_jit&lt;/code&gt; process. Going further down in &lt;code class="language-text"&gt;whiting_jit&lt;/code&gt;, &lt;strong&gt;60-80%&lt;/strong&gt; time is spent on &lt;strong&gt;lineshape calculation.&lt;/strong&gt; Below is the formula:&lt;br&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;lineshape = (
(1 - wl_wv) * exp(-2.772 * w_wv_2)
+ wl_wv * 1 / (1 + 4 * w_wv_2)
# ... 2nd order correction
+ 0.016 * (1 - wl_wv) * wl_wv * (exp(-0.4 * w_wv_225) - 10 / (10 + w_wv_225))
)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The whole process can be divided into 4 parts:&lt;br&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;    part_1 =   (1 - wl_wv) * exp(-2.772 * w_wv_2)

part_2 =    wl_wv * 1 / (1 + 4 * w_wv_2)

# ... 2nd order correction
part_3 =  0.016 * (1 - wl_wv) * wl_wv * exp(-0.4 * w_wv_225)

part_4 =  - 10 / (10 + w_wv_225)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The complexity of each part comes out: &lt;br&gt;
&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;    o1 = broadening__max_width * n_lines / wstep

O(part_1) = n_lines * o1
O(part_2) = n_lines * 4 * o1
O(part_3) = (n_lines)**2 * o1
O(part_4) = o1 &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Running several benchmark showed us that &lt;strong&gt;part_3&lt;/strong&gt; takes the most time out of all steps. So clearly we can see that the complexity of Legacy method is not dependent on
Spectral Range but rather &lt;code class="language-text"&gt;Number of Calculated Lines&lt;/code&gt;,&lt;code class="language-text"&gt;broadening__max_width&lt;/code&gt; and &lt;code class="language-text"&gt;wstep&lt;/code&gt;. It may seem that the complexity of Legacy method is:&lt;br&gt;&lt;/p&gt;
&lt;p align="center"&gt;&lt;b&gt; n_lines^2 * broadening__max_width * n_lines / wstep&lt;/b&gt;&lt;/p&gt; &lt;br&gt;
&lt;p&gt;But inorder to prove this we need more benchmarks and evidence to verify this and it may involve normalization of all steps in lineshape calculation!&lt;br&gt; &lt;/p&gt;
&lt;p&gt;So the goal for the next 2 weeks is clear:&lt;br&gt;
&lt;b&gt;i)&lt;/b&gt; Refactor the entire codebase with Profiler.&lt;br&gt;
&lt;b&gt;ii)&lt;/b&gt; Find the complexity of &lt;strong&gt;Legacy Method&lt;/strong&gt; with the help of more benchmark and analysis.&lt;br&gt;
&lt;b&gt;iii)&lt;/b&gt; Do the same for &lt;strong&gt;LDM Method&lt;/strong&gt;!&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Ok I guess time’s up! See you after 2 weeks :)&lt;/p&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210621_2240_anandxkumar/</guid><pubDate>Mon, 21 Jun 2021 21:40:32 GMT</pubDate></item><item><title>Chapter 0: Prologue</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210606_2240_anandxkumar/</link><dc:creator>anandxkumar</dc:creator><description>&lt;p&gt;Hi There and Namaste! This is going to be the second blog and first blog related to GSoC where I will be sharing my experience Community Bonding Period Experience with &lt;b&gt;Radis&lt;/b&gt;. Before moving ahead lets learn about GSoC and my perspective about it.&lt;/p&gt;
&lt;h3&gt;Google Summer of Code&lt;/h3&gt;
&lt;p&gt;GSoC or the way I like to say it &lt;strong&gt;(Great Summer Opportunity to Code ;)&lt;/strong&gt; is a program conducted and funded by Google to promote college students around the world to engage with Open Source Community and contribute to the organization for a tenure of 3 months. In the process, code is created and released for the world to see and use. But the main aim of GSoC is to promote students to stick to the organizations and help to grow the Open Source Community. This is a great initiative by Google that brings thousands of students every year and help them get an opportunity to peek into the world of open source development, learn new skills and also get compensated for the work, quite generously.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;I remember during second year of my college, it was around end of March and my roommate was applying for GSoC and I was like what is this program? There I got to know about it but since the deadline was near I was afraid of doing all the stuffs in a week of time, so I didn’t apply for it. Fast forwarding to next year, I was prepared enough this time and I feel priviledged to be a part of GSoC as part of OpenAstronomy. &lt;/p&gt;
&lt;h3&gt;My GSoC Project&lt;/h3&gt;
&lt;p&gt;I’m part of &lt;b&gt;&lt;a href="https://github.com/radis/radis"&gt;Radis&lt;/a&gt;&lt;/b&gt; organization which is a sub-org of &lt;a href="https://github.com/OpenAstronomy"&gt;OpenAstronomy&lt;/a&gt;. Radis is a fast line-by-line code used to synthesize high-resolution infrared molecular
spectra and a post-processing library to analyze spectral lines. It can synthesize absorption
and emission spectrum for multiple molecular species at both equilibrium and
non-equilibrium conditions.&lt;br&gt;
Radis computes every spectral line (absorption/emission) from the molecule considering
the effect of parameters like Temperature, Pressure. Due to these parameters, we don’t get
a discrete line but rather a shape with a width. This is called line broadening and for any spectral synthesis code, this is the bottleneck step. Ok let us C what my GSoC project is all about! &lt;br&gt;&lt;/p&gt;
&lt;p&gt;Radis has 2 methods to calculate the lineshape of lines.&lt;br&gt;
● Legacy Method&lt;br&gt;
● DLM Method&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The goal of this project is to derive an equation comprising all parameters that affect the
performance for calculating Voigt broadening by running several benchmarks for different
parameters involved in the calculation of lineshapes to check their significance in
computation time. Then we need to find the critical value for the derived equation (&lt;code class="language-text"&gt;Rc&lt;/code&gt;)
which will tell us which optimization technique to select based on the computed &lt;code class="language-text"&gt;R&lt;/code&gt; value in
&lt;b&gt;calc_spectrum()&lt;/b&gt;. An &lt;code class="language-text"&gt;optimization = "auto"&lt;/code&gt; will be added that will choose the best method based on the parameters provided.&lt;/p&gt;
&lt;h3&gt;Community Bonding Period&lt;/h3&gt;
&lt;p&gt;The first phase of GSoC is the &lt;b&gt;Community Bonding Period&lt;/b&gt; which is a 3 weeks long period. Its main aim is allow the student to get familiar with the community and the codebase. It serves as a warm-up period before the coding period. The first thing I did was that I went though the original Radis &lt;a href="https://www.sciencedirect.com/science/article/abs/pii/S0022407318305867?via%3Dihub"&gt;paper&lt;/a&gt; and also the DLM implementation &lt;a href="https://ui.adsabs.harvard.edu/abs/2021JQSRT.26107476V/abstract"&gt;paper&lt;/a&gt; because our project objective is based on these 2 implementations. It helped me understand the main purpose of RADIS, its architecture and the science behind different steps of both equilibrium and non-equilibrium spectrum, though I have to accept these papers are way too technical for me :p (Complex Spectroscopy related formulas).&lt;br&gt; I believed inorder to get myself ready for the coding period, I shall focus on solving some related issues to make me more familiar with the codebase.&lt;br&gt;&lt;/p&gt;
&lt;p&gt;In order to compute any spectrum we need to determine several parameters like: minimum-maximum wavenumber, molecule, Temperature of gas, mole fraction, wstep, etc.&lt;br&gt;
&lt;code class="language-text"&gt;wstep&lt;/code&gt; determines the wavenumber grid’s resolution. Smaller the value, higher the resolution and vice-versa. By default radis uses &lt;code class="language-text"&gt;wstep=0.01&lt;/code&gt;. You can manually set the wstep value in &lt;b&gt;calc_spectrum()&lt;/b&gt; and &lt;strong&gt;SpectrumFactory&lt;/strong&gt;. To get more accurate result you can further reduce the value, and to increase the performance you can increase the value.&lt;/p&gt;
&lt;p&gt;Based on wstep, it will determine the number of gridpoints per linewidth. To make sure that there are enough gridpoints, Radis will raise an &lt;strong&gt;Accuracy Warning&lt;/strong&gt;. If number of gridpoints are less than &lt;code class="language-text"&gt;GRIDPOINTS_PER_LINEWIDTH_WARN_THRESHOLD&lt;/code&gt; and raises an &lt;strong&gt;Accuracy Error&lt;/strong&gt; if number of gridpoints are less than &lt;code class="language-text"&gt;GRIDPOINTS_PER_LINEWIDTH_ERROR_THRESHOLD&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;So inorder to select the optimum value of &lt;code class="language-text"&gt;wstep&lt;/code&gt; I had to refactor the codebase such that we could compute the minimum FWHM (&lt;code class="language-text"&gt;min_width&lt;/code&gt;) value after calculating the HWHM of all lines and and set &lt;code class="language-text"&gt;wstep = min_width / GRIDPOINTS_PER_LINEWIDTH_WARN_THRESHOLD&lt;/code&gt;. All &lt;code class="language-text"&gt;wstep&lt;/code&gt; dependent parameters had to be refactored to make sure they are not being called before the calculating &lt;code class="language-text"&gt;min_width&lt;/code&gt;. At the end this feature was successfully merged in the develop branch of Radis and now users can use &lt;code class="language-text"&gt;wstep = "auto"&lt;/code&gt; to automatically get the optimal value of &lt;code class="language-text"&gt;wstep&lt;/code&gt;. This feature will be available from version &lt;b&gt;0.9.30&lt;/b&gt;. Here is the &lt;a href="https://github.com/radis/radis/pull/271"&gt;link&lt;/a&gt; of the merged PR.&lt;/p&gt;
&lt;p&gt;In short, the Community Bonding Period has been great and I have learned alot about Radis during this time. In the next 2 weeks I will be focussing on building a benchmarking framework and run various benchmarks for both Legacy and DLM method and determine the most influential paramters for performance.&lt;/p&gt;
&lt;p&gt;I’m very excited for the upcoming months. I know that this summer is going to be a life long experience and I’m really looking forward to do amazing things for the community and want to thank Google, OpenAstronomy, Radis and my mentors &lt;a href="https://github.com/erwanp"&gt;Erwan Pannier&lt;/a&gt;, &lt;a href="https://github.com/dcmvdbekerom"&gt;Dirk van den Bekerom&lt;/a&gt; and &lt;a href="https://github.com/pkj-m"&gt;Pankaj Mishra&lt;/a&gt; for this opportunity.
I’m ready for this amazing adventure.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;b&gt;LETS DO THIS&lt;/b&gt;&lt;br&gt;
&lt;img src="https://anandkumar-blog.netlify.app/2b4e6a4b663f4bc49d559484b8dd37b1/Start.gif"&gt;&lt;br&gt;
ps: Am a huge Spiderman Fan :p
&lt;/p&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210606_2240_anandxkumar/</guid><pubDate>Sun, 06 Jun 2021 21:40:32 GMT</pubDate></item><item><title>print(" Hello World!!! ")</title><link>http://openastronomy.org/Universe_OA/posts/2021/05/20210518_2240_anandxkumar/</link><dc:creator>anandxkumar</dc:creator><description>&lt;p&gt;Hey everyone &lt;b&gt;Anand Kumar&lt;/b&gt; this side. This is going to be a series of blogs where I will cover my Summer Journey with &lt;b&gt;Radis&lt;/b&gt; organization as a part of Google Summer of Code. Welcome to my first blog where I will be introducing myself coz that is kind of necessary :p. I’m a Junior from National
Institute of Technology, Hamirpur, India currently pursuing my BTech in Computer Science and Engineering.&lt;br&gt;&lt;/p&gt;
&lt;p&gt;I am a geek. I love life, computers and everything in between!&lt;br&gt;
&lt;!-- TEASER_END --&gt;
I have been coding since my school days and soon realized that man this thing is so cool!
I am an A.I. enthusiast and have made various projects related to Data Analysis, Machine Learning, Deep Learning
and Web Development. Also I have completed a Data Analytics Internship at Pikkal &amp;amp; Co, Singapore and a
Deep Learning Internship at Mavoix Solutions Pvt Ltd, Bangalore.&lt;br&gt;
Currently I’m a Student Developer at OpenAstronomy organization as a part of Google Summer of Code 2021. &lt;br&gt;&lt;/p&gt;
&lt;p&gt;Wanna know a less known fact, I’m a huge hardcore video gamer! If I’m not coding, I will probably be killing some time
on my laptop or my Playstation console ;)&lt;/p&gt;
&lt;p&gt;Wanna know more about me and my work? Below are some links, do check out;)&lt;br&gt;
&lt;a href="https://www.linkedin.com/in/anand-kumar-83896717a/"&gt;LinkedIn&lt;/a&gt; | &lt;a href="https://github.com/anandxkumar"&gt;github&lt;/a&gt;
| &lt;a href="https://anandkumar.netlify.app/"&gt;Portfolio&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Also one huge shout out to the guys at &lt;b&gt;GatsbyJS&lt;/b&gt; for providing such an amazing blogging template(keep it simple and clean, they say!).
The biggest advantage of this template is that every blog is written in &lt;b&gt;Markdown&lt;/b&gt;. So its gives alot of flexibility and functionality
to the user to edit their texts. Plus their templates codebase is easy to understand so anyone can just clone and get started!&lt;/p&gt;
&lt;p&gt;Anyways I guess this should wrap up this blog. See you in the next one where I will be starting my GSoC journey and discuss my project ;)&lt;br&gt;
Till then take care and ba-bye :)&lt;/p&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2021/05/20210518_2240_anandxkumar/</guid><pubDate>Tue, 18 May 2021 21:40:32 GMT</pubDate></item></channel></rss>