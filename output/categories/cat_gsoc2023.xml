<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Universe OpenAstronomy (Posts about gsoc2023)</title><link>http://openastronomy.org/Universe_OA/</link><description></description><atom:link href="http://openastronomy.org/Universe_OA/categories/cat_gsoc2023.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Fri, 12 Jan 2024 00:59:45 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>GSoC Week 11-13</title><link>http://openastronomy.org/Universe_OA/posts/2023/08/20230831_2038_pupperemeritus/</link><dc:creator>pupper emeritus</dc:creator><description>&lt;p&gt;I am reaching the final stages of my project well ahead of the schedule.&lt;/p&gt;

&lt;p&gt;Both the Crosspectrum and Powerspectrum classes, the Lomb Scargle Fourier Transform and Basic Documentation have been completed. Tests are almost done too&lt;/p&gt;
&lt;!-- TEASER_END --&gt;

&lt;p&gt;I have double checked with Matteo that the results are consistent using multiple examples and they are close to expected results.&lt;/p&gt;

&lt;p&gt;The following are the documentation notebooks on how to use the classes.&lt;/p&gt;


&lt;div class="ltag_gist-liquid-tag"&gt;

&lt;/div&gt;



&lt;div class="ltag_gist-liquid-tag"&gt;

&lt;/div&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2023/08/20230831_2038_pupperemeritus/</guid><pubDate>Thu, 31 Aug 2023 19:38:33 GMT</pubDate></item><item><title>Benchmark Tests</title><link>http://openastronomy.org/Universe_OA/posts/2023/08/20230825_0000_1someshverma/</link><dc:creator>Somesh Verma</dc:creator><description>&lt;p&gt;I have finished the refactoring the code for vaex and also writtten test cases to compare the spectrum calculated using pandas with the dataframe calculated using vaex dataframe . Also, Various spectroscopic quantities as absorbance , emissitivity is also compared for the both the dataframams.
Also, there was many issues that was raised by the maintainers and I have resolved almost all of these , and commented on the other issues to discuss the problem and discuss some possible solution .Issues raised by the maintainers was mainly related to make changes more matainable and easy to understand and simple programming logic is preferred inplace of using some complex code without explaining that in detail.
Also , the issue was to ensure a light test suite , that is test cases which takes less resources and time . Initialy , I didn’t focused on this thing and focused on testing the code and changes more elaborately by writing the test cases that cover many areas of code .&lt;/p&gt;
&lt;!-- TEASER_END --&gt;

&lt;p&gt;But, later as told by maintainer I have refacatored the changes and made the changes more light and test cases more light .It helped to reduce the time required test the new commit as excecution time of the test cases were reduced.
After, all this another thing was to add benchmark test to compare the memory use by vaex and pandas and also compare the execution time used by both these engines.&lt;/p&gt;

&lt;p&gt;Benchmark Test added to compare time taken by code is :&lt;/p&gt;
&lt;div class="language-plaintext highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;def compare_vaex_pandas_time():
"""
Compares the time performance of pandas and Vaex and generates a plot. This scripts takes several minutes to run.
This results shoud shown that vaex and pandas provide similar performances in term if speed.
Returns
-------
None.
"""
time_list, timeC_list, lines_list = [], [], []
time_list_va, timeC_list_va, lines_list_va = [], [], []
wmin = 1000
steps = 5
wmax_arr = np.geomspace(10, 1000, steps)

initial_engine = config[
"DATAFRAME_ENGINE"
]  # To make sure dataframe engine not changed after running this test
pb = ProgressBar(N=2 * steps)
for i, engine in enumerate(["vaex", "pandas"]):
config["DATAFRAME_ENGINE"] = engine
for j, w_range in enumerate(wmax_arr):
t0 = time.time()
s = calc_spectrum(
wmin,
wmin + w_range,  # cm-1
molecule="H2O",
isotope="1,2,3",
pressure=1.01325,  # bar
Tgas=1000,
mole_fraction=0.1,
databank="hitemp",  # or 'hitemp'
wstep="auto",
cutoff=1e-28,
verbose=0,
)
t1 = time.time()
if engine == "vaex":
timeC_list_va.append(s.conditions["calculation_time"])
lines_list_va.append(s.conditions["lines_calculated"])
time_list_va.append(t1 - t0)
# lines_list_va.append(s.conditions['lines_calculated']+s.conditions['lines_cutoff'])
else:
timeC_list.append(s.conditions["calculation_time"])
lines_list.append(s.conditions["lines_calculated"])
time_list.append(t1 - t0)
# lines_list.append(s.conditions['lines_calculated']+s.conditions['lines_cutoff'])
pb.update(i * steps + (j + 1))
plt.figure()
plt.plot(lines_list, time_list, "k", label="pandas total")
plt.plot(lines_list, timeC_list, "k--", label="pandas computation")
plt.plot(lines_list_va, time_list_va, "r", label="vaex total")
plt.plot(lines_list_va, timeC_list_va, "r--", label="vaex computation")
plt.ylabel("Time [s]")
plt.xlabel("Number of lines")
plt.legend()

config["DATAFRAME_ENGINE"] = initial_engine
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img alt="Vaex Comparison Time" src="https://1someshverma.github.io/images/timeComparison.png"&gt;&lt;/p&gt;

&lt;p&gt;while Graph for Memory use and code are :
&lt;img alt="Vaex Comparison" src="https://1someshverma.github.io/images/vaexcomparison.png"&gt;&lt;/p&gt;

&lt;div class="language-plaintext highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;# Compare the memory performance of Pandas and Vaex
def compare_pandas_vs_vaex_memory():
"""
Compare memory usage of `engine="vaex"` and `engine="pandas"` in calc_spectrum.
Expected behavior is "vaex" using much less memory. This function takes tens of seconds to run.
Returns
-------
None.
"""

import tracemalloc

initial_engine = config[
"DATAFRAME_ENGINE"
]  # To make sure dataframe engine not changed after running this test
for engine in ["pandas", "vaex"]:
config["DATAFRAME_ENGINE"] = engine
tracemalloc.start()
s = calc_spectrum(
1000,
1500,  # cm-1
molecule="H2O",
isotope="1,2,3",
pressure=1.01325,  # bar
Tgas=1000,  # K
mole_fraction=0.1,
wstep="auto",
databank="hitemp",  # or 'hitemp', 'geisa', 'exomol'
verbose=0,
)
snapshot = tracemalloc.take_snapshot()
memory = tracemalloc.get_traced_memory()
tracemalloc.stop()

# Some raw outputs
print("\n******** Engine = {} ***********".format(engine))
print(
"Peak, current = {:.1e}, {:.1e} for {:} lines calculated".format(
*memory, s.conditions["lines_calculated"]
)
)

# More sophisticated
print("*** List of biggest objects ***")
top_stats = snapshot.statistics("lineno")
for rank, stat in enumerate(top_stats[:3]):
print("#{}".format(rank + 1))
print(stat)

# Clear for next engine in the loop
tracemalloc.clear_traces()

config["DATAFRAME_ENGINE"] = initial_engine

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2023/08/20230825_0000_1someshverma/</guid><pubDate>Thu, 24 Aug 2023 23:00:00 GMT</pubDate></item><item><title>Final GSoC Report</title><link>http://openastronomy.org/Universe_OA/posts/2023/08/20230822_0000_labeeb-7z/</link><dc:creator>Labib Asari</dc:creator><description>&lt;p&gt;I will be discussing the goals of my GSoC project, how I spent my time and what I learned during this period. I will also be discussing the future of my project and what I plan to do next.&lt;/p&gt;

&lt;h2 id="goals-of-my-gsoc"&gt;Goals of my GSoC&lt;/h2&gt;
&lt;!-- TEASER_END --&gt;

&lt;p&gt;The &lt;a href="https://openastronomy.org/gsoc/gsoc2023/#/projects?project=gnuastro_library_in_python"&gt;original&lt;/a&gt; Google Summer of Code project this was year was to :&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Redisgn the &lt;code class="language-plaintext highlighter-rouge"&gt;error handling&lt;/code&gt; inside Gnuastro C library.&lt;/li&gt;
&lt;li&gt;Adding wrappers for Gnuastro library functions in &lt;code class="language-plaintext highlighter-rouge"&gt;pyGnuastro&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Prior to GSoC, my experience mostly consisted of Deep Learning and Computer Vision. I had a good high-level understanding of how GPUs were leveraged for the compute intensive tasks in various libraries and frameworks in these domains. I had started exploring the lower-level abstractions over GPUs using the CUDA framework.&lt;/p&gt;

&lt;p&gt;In the early weeks of February, I delivered a &lt;a href="https://docs.google.com/presentation/d/1texW2MQJqjdbtPCuLULqXf8-1GuIrub4bJh_b_EffS4/edit?usp=sharing"&gt;presentation&lt;/a&gt; to the Gnuastro development team. The point of this presentation was a proposal outlining the integration of GPU support into Gnuastro — an idea borrowed from the Machine Learning world but with huge advancement potential in the feild of Astronomy. Both of these domains process huge amounts of data. Both of these domains are characterized by the processing of substantial volumes of data.&lt;/p&gt;

&lt;p&gt;My mentor &lt;a href="https://akhlaghi.org/"&gt;Mohammad Akhlaghi&lt;/a&gt; was very supportive of this idea and gave me the go ahead to start working on it.&lt;/p&gt;

&lt;p&gt;And so, we had a 3rd goal for this GSoC project :&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Adding GPU support to Gnuastro.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id="work-done-throughout-the-gsoc"&gt;Work Done Throughout the GSoC&lt;/h3&gt;

&lt;h4 id="error-handling"&gt;Error Handling&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Need&lt;/strong&gt; : All of Gnuastro’s library functions  performed error handling using &lt;code class="language-plaintext highlighter-rouge"&gt;error(EXIT_FAILURE, ....)&lt;/code&gt;; thus exiting the program whenever an error was encountered with a detailed error message. This wasn’t a problem for the Gnuastro programs however for other callers like pyGnuastro, this is problematic as it exits from the entire Python environment.&lt;/p&gt;

&lt;p&gt;The new error handling mechanism defines a module &lt;code class="language-plaintext highlighter-rouge"&gt;error.h&lt;/code&gt;  new data structure &lt;code class="language-plaintext highlighter-rouge"&gt;gal_error_t&lt;/code&gt;. The exact contents of this structure have gone through multiple iterations but the final one is :&lt;/p&gt;

&lt;p&gt;&lt;img alt="gal_error_t" src="https://labeeb-7z.github.io/Blogs/img/posts/final/gal_error.png"&gt;&lt;/p&gt;

&lt;p&gt;The user should define a &lt;code class="language-plaintext highlighter-rouge"&gt;gal_error_t&lt;/code&gt; before the function call and pass it as an argument to the function(every function in Gnuastro will have an extra argument now).&lt;/p&gt;

&lt;p&gt;During the function execution, if any error occurs, it will populate the &lt;code class="language-plaintext highlighter-rouge"&gt;gal_error_t&lt;/code&gt; with the error message and the error code. The user can then check the error code and the error message to determine what went wrong.&lt;/p&gt;

&lt;p&gt;&lt;img alt="new_error_handling" src="https://labeeb-7z.github.io/Blogs/img/posts/final/error_handling.png"&gt;&lt;/p&gt;

&lt;p&gt;Corresponding functions are added in &lt;code class="language-plaintext highlighter-rouge"&gt;error.h&lt;/code&gt; for writing and managing the structure. Some methods are also provided for Python interface.&lt;/p&gt;

&lt;p&gt;After the module was finished, Mohammad implemented the new error mechanism inside the &lt;code class="language-plaintext highlighter-rouge"&gt;cosmology.c&lt;/code&gt; module, and then I used it to update the corresponding cosmology module in pyGnuastro. This solved the main the problem of python environment exiting on any error, instead errors were being reported inside the python shell.&lt;/p&gt;

&lt;p&gt;&lt;img alt="python_error" src="https://labeeb-7z.github.io/Blogs/img/posts/final/py-error.png"&gt;&lt;/p&gt;

&lt;p&gt;This completed setting-up the low level infrastructure for the new error handling mechanism. This can be now used by other modules of Gnuastro to update what happens when an error occurs. Implementing the high level error function calls, deciding the exact error type and defining what message should be shown, would be best done by the original authors of the modules.&lt;/p&gt;

&lt;p&gt;The new error handling mechanism currently lives at the &lt;a href="https://gitlab.com/makhlaghi/gnuastro-dev/-/tree/error"&gt;Gnuastro repository&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id="pygnuastro"&gt;pyGnuastro&lt;/h4&gt;

&lt;p&gt;Apart from implementing the new error handling mechanism in existing modules of pyGnuastro, I worked on 2 major things&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Implemented speclines module in pyGnuastro&lt;/strong&gt; : this is a simple module without any complex data structures. I tried this first when I was learning about the C-Python API. It gave me a good grasp of how and what’s going on in the existing pyGnuastro implementation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GAL_DATA_T for Python&lt;/strong&gt; :  The core data structure of Gnuastro - gal_data_t is a C struct. Any external data is represented using this structure. It was crucuial to had a similar structure in Python. Previously Jash had worked on loading and saving fits file made use of the Numpy-C API to to convert the raw data inside the gal_data_t to a Numpy array. This was an extremely clever and efficient idea, however it skipped all the other details inside gal_data_t. We had to find a way to represent the entire gal_data_t in Python. The normal way to create a new data structure in Python would be to create a new class. However, the wrappers are written in C language and we don’t get access to the Python interpreter. I took some more inspiration from Numpy on how they &lt;a href="https://numpy.org/doc/stable/reference/c-api/index.html"&gt;created a new Python&lt;/a&gt; - their core data structure : &lt;code class="language-plaintext highlighter-rouge"&gt;numpy.ndarray&lt;/code&gt; - using the C-Python API. I then discovered the API allows us to &lt;a href="https://docs.python.org/3/extending/newtypes_tutorial.html"&gt;define custom objects&lt;/a&gt; which may be used a data type for the Python interpreter. I learnt and used them to have a corresponding &lt;code class="language-plaintext highlighter-rouge"&gt;pygnuastro.data&lt;/code&gt; for pyGnuastro. It basically acted as a new data type in python similar to &lt;code class="language-plaintext highlighter-rouge"&gt;numpy.ndarray&lt;/code&gt;, had other details of gal_data_t.After this we had details of gal_data_t in python but we were missing on Jash’s idea of utilizing Numpy in pyGnuastro. I spent some time to make sure we can still utilize numpy’s speed inside pyGnuastro, The C-Python API is versatile and it allows having complex objects as sub-objects to other objects. Eventually we had the array(raw data) being represented as a &lt;code class="language-plaintext highlighter-rouge"&gt;numpy.ndarray&lt;/code&gt;! This meant we had both the speed of numpy and the details of gal_data_t in pyGnuastro’s &lt;code class="language-plaintext highlighter-rouge"&gt;pygnuastro.data&lt;/code&gt;. This was a major milestone in pyGnuastro.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img alt="pyGnuastro.data" src="https://labeeb-7z.github.io/Blogs/img/posts/final/python-type.jpg"&gt;&lt;/p&gt;

&lt;h4 id="gpus-in-gnuastro"&gt;GPUs in Gnuastro&lt;/h4&gt;

&lt;p&gt;Gnuastro is an astronomical data analysis and manipulation library. Astronomical data is usually very large in size, and thus computationally intensive. If the operations performed on this data are parallelizable, then GPUs can significantly speed up the processing.&lt;/p&gt;

&lt;p&gt;I started my work on GPUs right after Mohammad approved my initial idea. Here’s a summary/story of all the work done for GPU support :&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Learning about build systems&lt;/strong&gt; : After GPU support idea was accepted, my mentor suggested we should first setup the build system so CUDA modules can be integrated smoothly in the future. Gnuastro uses &lt;a href="https://en.wikipedia.org/wiki/GNU_Autotools"&gt;Autotools&lt;/a&gt; for its build system. I started by learning about &lt;a href="https://www.gnu.org/software/autoconf/"&gt;autoconf&lt;/a&gt;, &lt;a href="https://www.gnu.org/software/automake/"&gt;automake&lt;/a&gt; and &lt;a href="https://www.gnu.org/software/libtool/"&gt;libtool&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Linking Gnuastro with CUDA runtime&lt;/strong&gt; : CUDA SDK provides a &lt;a href="https://nvidia.github.io/cuda-python/module/cudart.html"&gt;runtime library - &lt;code class="language-plaintext highlighter-rouge"&gt;cudart&lt;/code&gt;&lt;/a&gt; which the necessay component to initiate communication with the GPU drivers. The runtime library is distributed as both a static and shared object file. This made things easier as we could link the runtime library statically with the Gnuastro library, making &lt;code class="language-plaintext highlighter-rouge"&gt;cudart&lt;/code&gt; part of Gnuastro. I modified the configure script to link the runtime library statically with Gnuastro. This was also the time I learnt extensively about how low level system libraries are built, linked and distributed.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Struggling with Libtool&lt;/strong&gt; : I then tried to implement some simple matrix functions in CUDA and integrate them with Gnuastro.  CUDA source code is compiled by &lt;code class="language-plaintext highlighter-rouge"&gt;nvcc&lt;/code&gt; compiler. However during linking, libtool assumes that all source files are compiled by &lt;code class="language-plaintext highlighter-rouge"&gt;gcc&lt;/code&gt;. It ignored all the CUDA source files. After writing dedicated rules for CUDA source compilation in the Makefile, the CUDA source was getting compiled, but not being linked to the Gnuastro. Libtool only links files having a corresponding libtool object(.lo files) and they’re created by libtool for each source file handled by it(which in our case were gcc compiled files).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;AutoMake developers rescuing us&lt;/strong&gt; : After trying and struggling with libtool for a few days, my mentor suggested that I contact the AutoMake developers to seek some help. I &lt;a href="https://lists.gnu.org/archive/html/automake/2023-03/msg00036.html"&gt;mailed&lt;/a&gt; them a &lt;a href="https://github.com/labeeb-7z/cuda-gnu/tree/main/shared-library"&gt;small demonstration&lt;/a&gt; of what I was trying to do and waited for there response. After a few days, I received a reply from them. The fix was actually simple, automake had special variables(&lt;code class="language-plaintext highlighter-rouge"&gt;LD_ADD&lt;/code&gt;) which directly communicates with the GNU linker (ld) and I just had to add CUDA object files to this variable. It worked and we finally had a working CUDA module in Gnuastro which used GPU for execution!&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;It was around 1st week of April now, I made my final proposal submission and had fingers crossed for getting selected in GSoC.&lt;/p&gt;

&lt;p&gt;As mentioned in the GSoC proposal, we had to first focus on the Error handling and Python wrappers, so I started working on these two goals (I was also indeed selected for GSoC in the meantime!).&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Convolutions on GPU&lt;/strong&gt; : After getting back to working with GPUs in around June-July, I started with implementing the convolution function in CUDA. Convolution is a direct operation as well as a subroutine to other operations in Gnuastro.
The results of CUDA convolution were remarkable. We got upto 400x speed up on convolution operation! My mentor then suggested me since the speedup is very significant, I should prioritise getting more of GPU work done.
Read more about Convolution on GPU in my blog &lt;a href="https://labeeb-7z.github.io/Blogs/2023/07/03/GPUs-and-Convolution.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Adapting OpenCL&lt;/strong&gt; : CUDA is a proprietary framework by Nvidia. It only works on Nvidia GPUs. We wanted to make Gnuastro GPU support available to all users, irrespective of the GPU they have. This is where OpenCL comes in. OpenCL is an open standard for parallel programming of heterogeneous systems. It is supported by all major GPU vendors. I started learning about OpenCL and how it works at a low level. I also started learning about the OpenCL C99 programming standard. Read more about starting with OpenCL in my blog &lt;a href="https://labeeb-7z.github.io/Blogs/2023/07/28/Towards-OpenCL.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Integrating OpenCL&lt;/strong&gt; : OpenCL was initially hard to learn, but I managed to integrate that with Gnuastro right before my GSoC’s official timeline was about to end! I have a pretty detailed blog on the the entire integration process &lt;a href="https://labeeb-7z.github.io/Blogs/2023/08/12/Integrating-OpenCL.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Same code on CPU and GPU&lt;/strong&gt; : After we had success with OpenCL, my mentor recommended we should try executing the &lt;code class="language-plaintext highlighter-rouge"&gt;exact&lt;/code&gt; same code on CPU and GPU - to show the concept of executing same instructions both processors and seeing the speed-up on GPUs. This was never done in the field of Astronomy so it’d have been a great demonstration. This was quite challenging as GPUs are programmed with different frameworks and have some extra components in code for management. Usually in Machine Learning frameworks, the GPU and CPU modules are generally written seperately(Infact Tensorflow used to have different package altogether for GPU until 2.0)
However the good part is, most of the GPU frameworks are derived from C/C++ language and have  . I spent my last week of GSoC trying to implement the core logic in a Macro which will be shared by both OpenCL kernels and C library and had success, this can be accessed here.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Future&lt;/strong&gt; : The future of this project is very bright. I have set up the bare-bone GPU integration already, I’ll continue to add GPU modules building upon it.. We have a working OpenCL integration. We have a working CUDA integration. We have a working CPU-GPU code sharing. I mentioned certain challenges we are currently facing in my &lt;a href="https://labeeb-7z.github.io/Blogs/2023/08/12/Integrating-OpenCL.html"&gt;opencl_integration&lt;/a&gt; blog. I’ll continue to figure out a solution for them and adding support for further modules on GPU.&lt;/p&gt;

&lt;h3 id="acknowledgements"&gt;Acknowledgements&lt;/h3&gt;

&lt;p&gt;GSoC has been a great learning experience for me. I’m extremely grateful to everyone who was part of this journey.&lt;/p&gt;

&lt;p&gt;I would like to thank my mentor &lt;a href="https://akhlaghi.org/"&gt;Mohammad Akhlaghi&lt;/a&gt; for his constant support and guidance throughout the project. He has been very patient right from the beginning, beleived in me when I did not have a clear idea on how I’d approach all the goals. He allowed me work on my pace, explore and learn things as needed and has always pulled me out of the rabbit hole whenever I got stuck. Everytime I join a meeting with him, I learn something new. I’m very grateful to him for giving me this opportunity to work on this project.&lt;/p&gt;

&lt;p&gt;I am Graciously thankful to Jash Shah for introducing me to the Gnuastro development team and walking me through the existing work on error handling and pyGnuastro. It provided me a huge boost was extremely valuable. He’s always been attentive to my small queries and has supported me through multiple challenges. In general, Im very grateful to have him as a mentor and freind.&lt;/p&gt;

&lt;p&gt;I would also like to thank the Gnuastro development team for their support and feedback throughout the project. Its been such a wonderful time working with them. I have learnt a ton from attending Pedram’s work on adding Sql to Gnuastro, Fathma’s work on Tiff files and Curl library, Faezeh’s work on implementing Convolutional Neural Networks in Gnuastro.
They’ve always been crucial in providing feedback and suggestions on my work. I’m very grateful to them for their support.
I am genuinely grateful for the opportunity to collaborate with such a talented and committed group, and I look forward to work and grow with them in the future.&lt;/p&gt;

&lt;p&gt;I would also like to thank the Google Summer of Code team for taking the wonderful initiative and giving me this opportunity to work on this project.&lt;/p&gt;</description><category>gnuastro</category><guid>http://openastronomy.org/Universe_OA/posts/2023/08/20230822_0000_labeeb-7z/</guid><pubDate>Mon, 21 Aug 2023 23:00:00 GMT</pubDate></item><item><title>GSoC Week 9-10</title><link>http://openastronomy.org/Universe_OA/posts/2023/08/20230813_1023_pupperemeritus/</link><dc:creator>pupper emeritus</dc:creator><description>&lt;p&gt;Big progress on the front of algorithm working. Turns out there wasn't that much of a problem in the algorithm. I just had to subtract the mean from the data before taking the fourier transform. The Lomb Scargle seems to work on data that has mean subtracted from it. Furthermore they dont seem to work that well or at all in full spectrum. &lt;br&gt;
The LSFT also is highly sensitive to the time intervals that are input to it. I have found some more clues as to how to make it even better. I will expound upon this further within 2 weeks since my final exams are going on and I have limited time.&lt;/p&gt;

&lt;!-- TEASER_END --&gt;
&lt;p&gt;Minimum Working Code Example&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight js-code-highlight"&gt;
&lt;pre class="highlight plaintext"&gt;&lt;code&gt;def data_func(time, freq=1.2324235252):
return  2 * np.sin(2 * np.pi * time * freq)

t0 = 0
t1 = 100
dt = 0.1

np.random.seed(43)
time_uniform = np.arange(t0, t1, dt)
time_nonuniform = np.sort(np.random.uniform(t0, t1, time_uniform.size))

npft = np.fft.fft(data_func(time_uniform))
npfreqs = np.fft.fftfreq(npft.size, dt)
npft = npft[npfreqs&amp;gt;=0]
npfreqs = npfreqs[npfreqs&amp;gt;=0]
lsfreqs = np.linspace(np.min(npfreqs), np.max(npfreqs), npfreqs.size * 8)
lsfreqs = lsfreqs[lsfreqs&amp;gt;=0]
np.random.seed(43)
lsft_slow_arr = lsft_slow(data_func(time_nonuniform), time_nonuniform, lsfreqs,sign=-1, fullspec=False)
lsft_fast_arr = lsft_fast(data_func(time_nonuniform), time_nonuniform, lsfreqs,sign=-1, fullspec=False,oversampling=10)
plt.plot(time_nonuniform,lsft_slow_inv(lsft_slow_arr,freqs=time_nonuniform, t=lsfreqs).real ,alpha=0.5,label="Slow")
plt.plot(time_nonuniform,lsft_fast_inv(lsft_fast_arr,freqs=time_nonuniform, t=lsfreqs).real,alpha=0.5,label="Fast")
plt.plot(time_nonuniform,data_func(time_nonuniform),label="Original Data")
plt.legend()
plt.xlim(0,10)
plt.ylim(-3,3)
&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;



&lt;p&gt;&lt;a class="article-body-image-wrapper" href="https://res.cloudinary.com/practicaldev/image/fetch/s--v8B0QIbQ--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/g10l11a0u36xzhtkq5n5.png"&gt;&lt;img alt="Image description" height="418" src="https://res.cloudinary.com/practicaldev/image/fetch/s--v8B0QIbQ--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/g10l11a0u36xzhtkq5n5.png" width="555"&gt;&lt;/a&gt;&lt;/p&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2023/08/20230813_1023_pupperemeritus/</guid><pubDate>Sun, 13 Aug 2023 09:23:47 GMT</pubDate></item><item><title>Docs and Marty and the Moving Around of Code and Tests</title><link>http://openastronomy.org/Universe_OA/posts/2023/08/20230812_0533_exitflynn/</link><dc:creator>exitflynn</dc:creator><description>&lt;p&gt;Since the last post, I worked on the moving-the-code task. Me, Nabil and Alasdair got on a call to discuss what should go where and I ended up creating a &lt;code&gt;scraper_utils.py&lt;/code&gt; file to complement the &lt;code&gt;scraper.py&lt;/code&gt; file. The other options were moving the functions to &lt;code&gt;.util.net&lt;/code&gt; or inside the &lt;code&gt;scraper.py&lt;/code&gt; file but outside the &lt;code&gt;Scraper&lt;/code&gt; class. After I moved the tests as well, I wrote new tests, increased test coverage and renamed some functions which were previously named in the JavaScript-style CamelCase.
I also added and extended doc-strings to some functions which could use some updating and made fixes as asked in Code-Reviews.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;</description><category>SunPy</category><guid>http://openastronomy.org/Universe_OA/posts/2023/08/20230812_0533_exitflynn/</guid><pubDate>Sat, 12 Aug 2023 04:33:30 GMT</pubDate></item><item><title>Integrating OpenCL with Gnuastro</title><link>http://openastronomy.org/Universe_OA/posts/2023/08/20230812_0000_labeeb-7z/</link><dc:creator>Labib Asari</dc:creator><description>&lt;h3 id="background"&gt;Background&lt;/h3&gt;

&lt;p&gt;In the last post, I discussed what is OpenCL and why we chose to integrate it with Gnuastro. In this post, I’ll be discussing the actual implementation and the challenges I faced.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;

&lt;h3 id="programming-in-opencl"&gt;Programming in OpenCL&lt;/h3&gt;

&lt;p&gt;The OpenCL 3.0 standard has done a great job of simplifying the programming model. The OpenCL 3.0 API is a header-only library that provides a modern, object-oriented interface to the OpenCL runtime. It is designed to be easy to use and provides a abstraction of the OpenCL runtime, making it easier to write portable code across different OpenCL implementations. We still have to communicate with the driver (unlike CUDA) at a low level, but this becomes a mandatory step when we want to run our code on different hardware (CUDA always expects an NVIDIA device).&lt;/p&gt;

&lt;p&gt;Here’s a general overview of steps to be followed when writing an using OpenCL :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Check for available Platforms&lt;/strong&gt; : A platform is a collection of OpenCL devices. A platform can be a CPU, GPU, or an FPGA (Remember OpenCL can work with any platform!). This is done specifically to identify which OpenCL implementation will be used during runtime. We can query the system for available platforms using the &lt;code class="language-plaintext highlighter-rouge"&gt;clGetPlatformIDs&lt;/code&gt; function. This function returns a list of platforms available on the system.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Check for available devices&lt;/strong&gt; : A device is a physical device that can execute OpenCL kernels. A device can be a CPU, GPU, or an FPGA. We can query the system for available devices using the &lt;code class="language-plaintext highlighter-rouge"&gt;clGetDeviceIDs&lt;/code&gt; function. This function returns a list of devices available on the system.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Create a context&lt;/strong&gt; : A context is a container for all the OpenCL objects. It is used to manage the memory, command queues, and other OpenCL objects. It is created by passing a list of devices to the constructor. Since OpenCL can work with multiple devices, we can create a context with multiple devices. This is useful when we want to run our code on multiple devices at the same time.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Create a command queue&lt;/strong&gt; : A command queue is used to queue up commands for the device to execute. The command queue is used to give commands to the device. The device executes the commands in the order they are received. The commands can be kernel execution, memory transfer, or any other OpenCL command. We can also create multiple command queues. This is useful when we want to run to multiple commands. Command queues in OpenCL are asynchronous by default. This means that the commands are queued up and the control is returned to the host. The host can then continue with other tasks. We can also create a synchronous command queue. This means that the commands are queued up and the control is returned to the host only when the commands are executed.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Load the Kernel&lt;/strong&gt; : A kernel is a function that is executed on the device. It is written as per the &lt;code class="language-plaintext highlighter-rouge"&gt;C99 standard&lt;/code&gt;. We can load the kernel from a file or we can write the kernel inline. To maintain portablitiy, OpenCL kernels are generally compiled at runtime using &lt;code class="language-plaintext highlighter-rouge"&gt;clBuildProgram&lt;/code&gt;. We can also compile the kernel offline. This is useful when we want to compile the kernel for a specific device.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Copy Data to device memory&lt;/strong&gt; : All the data used in kernel, must be on the device memory. So we have to copy the data from the host to the device memory. We can do this using the &lt;code class="language-plaintext highlighter-rouge"&gt;clCreateBuffer&lt;/code&gt; function. This function creates a buffer on the device memory. We can then copy the data from the host to the device using the &lt;code class="language-plaintext highlighter-rouge"&gt;clEnqueueWriteBuffer&lt;/code&gt; function. This function copies the data from the host to the device.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Launch the kernel&lt;/strong&gt; : We can launch the kernel by passing the kernel object to the command queue. We have to set the arguments for the kernel seperately, using the &lt;code class="language-plaintext highlighter-rouge"&gt;clSetKernelArg&lt;/code&gt; function. We can also set the global and local work size. The global work size is the total number of work items that will be executed. The local work size is the number of work items that will be executed in a work group. The global work size should be a multiple of the local work size. If the global work size is not a multiple of the local work size, then the global work size is rounded up to the next multiple of the local work size.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Read the output&lt;/strong&gt; : We can read the output from the device using the &lt;code class="language-plaintext highlighter-rouge"&gt;clEnqueueReadBuffer&lt;/code&gt; function. This function copies the data from the device to the host.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id="implementation"&gt;Implementation&lt;/h3&gt;

&lt;p&gt;Among all the steps mentioned above, everything up till loading the kernel is common to all the programs that’ll be using OpenCL. So we defined a &lt;code class="language-plaintext highlighter-rouge"&gt;gpu_utils&lt;/code&gt; module which is responsible for querying for the available platforms and devices, creating the context and command queue, loading and compiling the kernel. The only external data it requires is the path to the kernel file. This is provided as an input.
It also provides utility functions to copy specific data types to and from device memory.&lt;/p&gt;

&lt;p&gt;There’ll be 2 types of OpenCL program in Gnuastro :&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Programs using OpenCL to speed-up existing operations inside Gnuastro.&lt;/li&gt;
&lt;li&gt;User defined OpenCL kernels, responsible for performing a custom task.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id="programs-using-opencl-to-speed-up-existing-operations-inside-gnuastro"&gt;Programs using OpenCL to speed-up existing operations inside Gnuastro&lt;/h4&gt;

&lt;p&gt;These programs will be using OpenCL to speed-up existing operations inside Gnuastro. For example, we can use OpenCL to speed-up the &lt;code class="language-plaintext highlighter-rouge"&gt;astconvolve&lt;/code&gt; operation by passing an extra &lt;code class="language-plaintext highlighter-rouge"&gt;--gpu&lt;/code&gt;. For these programs, the OpenCL kernels will be part of the Gnuastro Library.&lt;/p&gt;

&lt;p&gt;The general flow of the program then becomes :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The user passes the input data for a specific operation, and also choses the local and global work size.&lt;/li&gt;
&lt;li&gt;The program then initializes the device using &lt;code class="language-plaintext highlighter-rouge"&gt;gpu_utils&lt;/code&gt; module by providing the kernel file from the library, which does everything and returns a &lt;code class="language-plaintext highlighter-rouge"&gt;cl_kernel&lt;/code&gt; (which is essentially the compiled kernel).&lt;/li&gt;
&lt;li&gt;Data transfer from CPU to device (GPU) is done using the functions provided by &lt;code class="language-plaintext highlighter-rouge"&gt;gpu_utils&lt;/code&gt; module.&lt;/li&gt;
&lt;li&gt;The kernel is launched using with the provided global and local work size.&lt;/li&gt;
&lt;li&gt;Data is copied back to CPU memory and returned to the user.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id="user-defined-opencl-kernels-responsible-for-performing-a-custom-task"&gt;User defined OpenCL kernels, responsible for performing a custom task&lt;/h4&gt;

&lt;p&gt;These programs will be using OpenCL to perform a custom task. For example, we can use OpenCL to perform a custom convolution operation by passing a custom kernel. For these programs, the OpenCL kernels will be provided by the user. The exact design details yet to be determined for this.&lt;/p&gt;

&lt;h3 id="results"&gt;Results&lt;/h3&gt;
&lt;p&gt;Input image is 10,000 x 20,000 random image with normal distribution.
Kernel is 7 x 7 standard convolution kernel.
CPU : Intel(R) Core(TM) i5-9300HF CPU @ 2.40GHz
GPU : NVIDIA GeForce GTX 1650&lt;/p&gt;

&lt;p&gt;Convolution using existing convolution in Gnuastro :&lt;/p&gt;

&lt;p&gt;&lt;img alt="Convolution using existing convolution in Gnuastro" src="https://labeeb-7z.github.io/Blogs/img/posts/opencl-imp/conv_cpu.png"&gt;&lt;/p&gt;

&lt;p&gt;Convolution on OpenCL :&lt;/p&gt;

&lt;p&gt;&lt;img alt="Convolution on OpenCL" src="https://labeeb-7z.github.io/Blogs/img/posts/opencl-imp/conv_gpu.png"&gt;&lt;/p&gt;

&lt;p&gt;Result&lt;/p&gt;

&lt;p&gt;&lt;img alt="Result" src="https://labeeb-7z.github.io/Blogs/img/posts/opencl-imp/res.png"&gt;&lt;/p&gt;

&lt;p&gt;The speed up for convolution operation is specifically ranges from 300-500x, but for the entire operation its around 3-5x due to the overhead of copying data to and from the device. Overcoming this is a big and important challenge!&lt;/p&gt;

&lt;h3 id="challenges"&gt;Challenges&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;No &lt;code class="language-plaintext highlighter-rouge"&gt;GAL_DATA_T&lt;/code&gt; inside OpenCL kernel!&lt;/strong&gt; : Inside OpenCL, &lt;code class="language-plaintext highlighter-rouge"&gt;cl_mem&lt;/code&gt; is the primary object used to represent memory objects such as buffers and images. It is used to allocate memory on the device. Regardless of where the data is coming from on device (arrays, structs, etc), it’s all converted into a &lt;code class="language-plaintext highlighter-rouge"&gt;cl_mem&lt;/code&gt; object when copied to the device.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However inside Gnuastro, the core data structure is &lt;code class="language-plaintext highlighter-rouge"&gt;gal_data_t&lt;/code&gt; which is essentially just a C struct.&lt;/p&gt;

&lt;p&gt;Why is this a problem? Well the raw data of the input image/table is not contained inside the &lt;code class="language-plaintext highlighter-rouge"&gt;gal_data_t&lt;/code&gt;. It merely consists a pointer to that data! So wehn we copy the &lt;code class="language-plaintext highlighter-rouge"&gt;gal_data_t&lt;/code&gt; to device, the raw data(which is huge) is not copied. (It lives on the CPU memory, and hence cant use CPU pointers on GPU memory).&lt;/p&gt;

&lt;p&gt;What about copying the raw data seperately on the GPU memory, and then replacing the pointer inside &lt;code class="language-plaintext highlighter-rouge"&gt;gal_data_t&lt;/code&gt; with a pointer which has the address on the GPU memory? Well, this is not possible either. Why? See, when we are on CPU, we’ve a good &lt;code class="language-plaintext highlighter-rouge"&gt;gal_data_t&lt;/code&gt; struct which is a single big object with ‘sub-objects’(one of which is the pointer). But on GPU, we’ve a &lt;code class="language-plaintext highlighter-rouge"&gt;cl_mem&lt;/code&gt; which is an object, but unlike structs, it cant have sub-objects!&lt;/p&gt;

&lt;p&gt;How do we solve this? Currently all the required pointers inside &lt;code class="language-plaintext highlighter-rouge"&gt;gal_data_t&lt;/code&gt; are passed as seperate arguments to the kernel. After a careful study of the internal implementation of the &lt;code class="language-plaintext highlighter-rouge"&gt;cl_mem&lt;/code&gt; object, we’ll see if we can directly pass the &lt;code class="language-plaintext highlighter-rouge"&gt;gal_data_t&lt;/code&gt; to the kernel.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Data Transfer Overhead&lt;/strong&gt; : As mentioned multiple times, for using GPUs, we must copy data to and from the GPU memory. Astronomical datasets are huge, and copying them for each operation is a big overhead! Infact the data transfer overhead is so huge, that the actual operation is much faster than the data transfer. Adding more to that, its not just faster, its much much faster! So much so that around 95% of the time is spent in copying data to and from the GPU memory. It reduces performance by ~100x! It can’t continue this way!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One solution we’ve figured is, when the External data is loaded for the first time in the program, we load it on the GPU memory instead of the CPU memory. This way, for each subsequent operation, we dont have to copy the data from CPU to GPU memory. After all the operations are done, we’ll copy the result back to CPU memory and save it to the disk. This will avoid almost all the Data Transfer overhead.&lt;/p&gt;

&lt;p&gt;This is about the same approach used by Machine Learning Libraries such as Tensorflow. Basically during initialization, it occupies all the GPU memory it can, and keeps it occupied. All the operations, their results and the subsequent operations are done on the GPU memory itself.&lt;/p&gt;</description><category>gnuastro</category><guid>http://openastronomy.org/Universe_OA/posts/2023/08/20230812_0000_labeeb-7z/</guid><pubDate>Fri, 11 Aug 2023 23:00:00 GMT</pubDate></item><item><title>Progress on Kurucz and NIST databases</title><link>http://openastronomy.org/Universe_OA/posts/2023/08/20230811_1723_menasrac/</link><dc:creator>Racim MENASRIA</dc:creator><description>&lt;p&gt;Since the last article, I received a lot of feedback and comments about the Kurucz PR.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/952/1*iyB8Ya_dKD5OIkQ8gbcovg.png"&gt;&lt;/figure&gt;&lt;p&gt;Here is and example of a Fe_I spectrum I can obtain with these conditions.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*alxgHx0L0Bg54hyUcp8h0Q.png"&gt;&lt;/figure&gt;&lt;h4&gt;The main remarks where that :&lt;/h4&gt;&lt;p&gt;I needed to adjust the code to make it more general and user friendly. I introduced a specie argument to SpectrumFactory and calc_spectrum to replace atom and molecule and gather them under a same name.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;I made sure to respect the Radis structure by mooving files where I needed to and adding a new Partfunc class for Kurucz. &lt;br&gt;Then I added a few tests and removed old tests that were not needed any longer.&lt;/p&gt;
&lt;p&gt;I also cleaned my PR : removed all the unused methods from the Kurucz API,added references, moved hardcoded arrays to proper files.&lt;/p&gt;
&lt;p&gt;We asked the Exojax team for more help about the broadening parameters. For the moment, there are some approximations and placeholders about the airbrd (air broadening which is required in the Radis format) by computing it thanks to the Kurucz parameters.&lt;br&gt;A simplified version of the broadening allows to plot spectra for now but there are still values to adjust for the various species.&lt;/p&gt;
&lt;p&gt;I also started to work on the NIST database by fixing a parsers developed last year. Though I can plot NIST spectra for some wavelength, there still are issues particularly about the FWHM to deal with.&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=e955d61c1591" width="1"&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2023/08/20230811_1723_menasrac/</guid><pubDate>Fri, 11 Aug 2023 16:23:24 GMT</pubDate></item><item><title>Writing Test Cases</title><link>http://openastronomy.org/Universe_OA/posts/2023/08/20230801_0000_1someshverma/</link><dc:creator>Somesh Verma</dc:creator><description>&lt;p&gt;For testing specturm produce using vaex and pandas for non-equilibrium calculations are same , the code similar to equilibrium calculations is used&lt;/p&gt;

&lt;div class="language-plaintext highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;from radis import calc_spectrum
&lt;!-- TEASER_END --&gt;

import time
t0=time.time()

s, factory_s = calc_spectrum(1800, 1820,         # cm-1
molecule='CO',
isotope='1',
pressure=1.01325,   # bar
Tgas=700,           # K
Tvib=710,
Trot=710,
mole_fraction=0.1,
wstep='auto',
path_length=1,      # cm
databank='hitemp',  # or 'hitemp', 'geisa', 'exomol'
optimization=None,
engine='vaex',
verbose=3,
return_factory=True,
)

s.apply_slit(0.5, 'nm')       # simulate an experimental slit

t1=time.time()
print('Time taken : '+str(t1 - t0))

t0=time.time()

s1, factory_s1 = calc_spectrum(1800, 1820,         # cm-1
molecule='CO',
isotope='1',
pressure=1.01325,   # bar
Tgas=700,           # K
Tvib=710,
Trot=710,
mole_fraction=0.1,
wstep='auto',
path_length=1,      # cm
databank='hitemp',  # or 'hitemp', 'geisa', 'exomol'
engine='pandas',
verbose=3,
return_factory=True,
)

s.apply_slit(0.5, 'nm')       # simulate an experimental slit

t1=time.time()
print(s.get("absorbance"))
s.plot('radiance_noslit')
print('Time taken : '+str(t1 - t0))

import numpy as np
print(np.allclose(s.get("absorbance"), s1.get("absorbance")))

for column in factory_s1.df1.columns:
assert np.all(factory_s1.df1[column] == factory_s.df1[column].to_numpy())
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I will add more test cases .&lt;/p&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2023/08/20230801_0000_1someshverma/</guid><pubDate>Mon, 31 Jul 2023 23:00:00 GMT</pubDate></item><item><title>GSoC Week 5-8</title><link>http://openastronomy.org/Universe_OA/posts/2023/07/20230730_0735_pupperemeritus/</link><dc:creator>pupper emeritus</dc:creator><description>&lt;h3&gt;


&lt;!-- TEASER_END --&gt;
Brief
&lt;/h3&gt;

&lt;p&gt;I have worked on creating unit tests for the Lomb Scargle Cross Spectrum class, cross verifying the algorithm by comparing with the papers and fixed typos in docstrings. Apologies for the delay. Had my exams.&lt;/p&gt;

&lt;h3&gt;


Details
&lt;/h3&gt;

&lt;p&gt;I have noticed a few faults in both the fast and the slow algorithms. I have gone back to the drawing board and tried to address those issues by following the papers as closely as possible. All the changes are visible in the following draft pull request.&lt;br&gt;
&lt;a href="https://github.com/StingraySoftware/stingray/pull/737"&gt;https://github.com/StingraySoftware/stingray/pull/737&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;After the fixing, the fast and the slow algorithm have started giving very similar outputs. I am starting to suspect that the time lags might be broken in the algorithms themselves. It is starting to get a little suspicious when different methods are giving very similar results and they are still not what that is expected. Last time around the fast and slow algorithms have given different results. After cross verifying with the papers, The results from both have fast and slow algorithms converged.&lt;/p&gt;

&lt;p&gt;To keep the project sailing along while I wait for confirmation that this is an issue with the implementation or the algorithm , I have decided to work on writing unit tests for the various classes and methods. Furthermore I also worked on fixing the docstrings.&lt;/p&gt;


&lt;div class="ltag_gist-liquid-tag"&gt;

&lt;/div&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2023/07/20230730_0735_pupperemeritus/</guid><pubDate>Sun, 30 Jul 2023 06:35:19 GMT</pubDate></item><item><title>Adapting Kurucz to SpectrumFactory and what is next ?</title><link>http://openastronomy.org/Universe_OA/posts/2023/07/20230729_2343_menasrac/</link><dc:creator>Racim MENASRIA</dc:creator><description>&lt;h4&gt;&lt;strong&gt;Adapting Kurucz to SpectrumFactory and what is next ?&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;After my first pull request I received some feedback.&lt;br&gt;Optional and major changes were requested. The most important changes were that my code should &lt;strong&gt;better integrate the existing Radis code&lt;/strong&gt;. Indeed, though I added a new database with Kurucz, its API remained distinct which is something which will make Radis progress toward a common API.&lt;/p&gt;
&lt;p&gt;Another key remark was that my code didn’t take into account the &lt;strong&gt;Broadening effects&lt;/strong&gt; that modify the lineshapes.&lt;br&gt;This is why I had a Team meeeting with my mentors to discuss the physics behind the code. It helped me a lot to understand what was expected.&lt;/p&gt;
&lt;p&gt;After this I worked on adding broadening and merging the new AdB Kurucz with SpectrumFactory. In order to do so, I worked on an example which allows to plot a spectrum using the Kurucz atomic data and &lt;strong&gt;SpectrumFactory.&lt;/strong&gt; My first attempt was to use one of the existing Radis formats for databanks named &lt;strong&gt;hdf5-radisdb&lt;/strong&gt; since I worked with hdf5 files in my Class.&lt;br&gt;This attempt happened to be too difficult because the formats were made for molecules and too many columns of my dataframe were different from the expected columns.&lt;br&gt;This is why I eventually decided to add &lt;strong&gt;a new format named “kurucz” &lt;/strong&gt;to the load_databank method which allows to load the kurucz data with the proper form.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;Then, I worked on the &lt;strong&gt;eq_spectrum&lt;/strong&gt; method to adjust it to this newformat.&lt;br&gt;I added some methods and adapted methods from Exojax to handle linestrength computation, broadening,convolution,pressure layers and create a Spectrum Object. It took me a lot of efforts and I modified many files as Broadening.py, Base.by,Factory.py or loader.py.&lt;br&gt;However, the results of the Spectrum I obtained were not convincing and some parameters and units didn’t fit properly.&lt;/p&gt;
&lt;p&gt;Moreover, by the time I wrote this spectrocopy code, I fell behind in my project, that’s why we organized a long meeting with one of my supervisors in order to take stock, we adjusted the objectives of the project.&lt;/p&gt;
&lt;h4&gt;We gave up the last one about adding the CIA database and agreed on the following timeline :&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;finishing with SpectrumFactory for Kurucz ASAP&lt;/li&gt;&lt;li&gt;Moving to NIST&lt;/li&gt;&lt;li&gt;Then working on the DatabaseManager Class architecture and adapting to AdB and MdB manager subclasses&lt;/li&gt;&lt;li&gt;Moving to the TheoreTS ( it will require to reach people in Reims to fix the db that I still cannot access).&lt;/li&gt;&lt;li&gt;Working on developing an example during the last week to show what applications the atomic spectra physics brings.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;We also noticed that I had written my Spectrum Factory example from the beginning rather than using the existing radis methods which is why I lost time and it was unaccurate. However, the meeting brought me the right guidelines and working on this code allowed me to getting a better understanding of the architecture and adapting the example to the existing structure of the code should be easier now. We also discussed about a few existing codes which could be a could starting point for adding NIST to Kurucz.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;The next weeeks will take me a lot of time and effort to complete the objectives but in the end, I am happy that we had this meeting because it unblocked me when I was kinda stuck with Kurucz for a while.&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=d3453292daf1" width="1"&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2023/07/20230729_2343_menasrac/</guid><pubDate>Sat, 29 Jul 2023 22:43:53 GMT</pubDate></item></channel></rss>