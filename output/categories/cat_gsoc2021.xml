<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Universe OpenAstronomy (Posts about gsoc2021)</title><link>http://openastronomy.org/Universe_OA/</link><description></description><atom:link href="http://openastronomy.org/Universe_OA/categories/cat_gsoc2021.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Sat, 07 Aug 2021 05:02:36 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>GSoC update!</title><link>http://openastronomy.org/Universe_OA/posts/2021/08/20210803_0200_rashmiraj137/</link><dc:creator>Raj Rashmi</dc:creator><description>&lt;div&gt;&lt;p&gt;GSoC started four months ago and it is not just about knowing more about the open-source that made the experience great! My mentors made it way cooler than I thought it would be. I was writing my Master thesis, for the last three months and surely, it has been a super productive summer for me! The best part is I get to do things at my own pace. My project particularly hasn‚Äôt been very easy to implement. I need to bridge a Machine Learning algorithm in the existing codebase. The fun part is venturing with different notebooks and figuring out with intuition, what could be efficient in terms of computational time, efficiency, cost etc. But as of now, the struggle has been to define the problem as exactly to achieve the result. But I will keep working on finding a solution with my mentor Daniela, and trust that struggle will bring some positive construction in Stingray.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/0*2GcUC2cgvKapk8Lj"&gt;&lt;/figure&gt;&lt;p&gt;The current data fit for the evaluation of likelihood happens using scipy.optimize.minimize function. However, there exists numerous ways to do this. SciPy optimize provides functions for minimizing (or maximizing) objective functions, possibly subject to constraints. It includes solvers for nonlinear problems (with support for both local and global optimization algorithms), linear programming, constrained and nonlinear least-squares, root finding, and curve fitting. The problem with the current minimization algorithm is that it converges at local minimum instead of global, i.e. it is not very robust. Recently, Machine Learning has evident development in such optimization tools. The strategy for ahead is that I will work on finding alternatives that potentially accelerate the code, makes it¬†robust.&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=2d16a70cc267" width="1"&gt;
&lt;!-- TEASER_END --&gt;&lt;/div&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2021/08/20210803_0200_rashmiraj137/</guid><pubDate>Tue, 03 Aug 2021 01:00:58 GMT</pubDate></item><item><title>GSoC - 3</title><link>http://openastronomy.org/Universe_OA/posts/2021/08/20210802_0300_gagan-aryan/</link><dc:creator>Gagan Aryan</dc:creator><description>&lt;div&gt;&lt;p&gt;Hello and welcome to the first blog of GSoC phase-2. Ever faced a time when there was way too much on the plate and you find it really hard to catch up on all the work? That is pretty much how the previous two weeks were for me. With the start of the oncampus internship drive, I was finding it really hard to give manage the project. Somehow I was able to make some progress but I am yet to complete the task.&lt;/p&gt;
&lt;p&gt;I am basically trying to parse the &lt;code&gt;.bz2&lt;/code&gt; files of &lt;code&gt;HITEMP&lt;/code&gt; databases into HDF5 files in a Vaex friendly format. Currently &lt;code&gt;.bz2&lt;/code&gt; files are parsed into HDF5 files with the help of high level pandas functions. But as we already know pandas can be very memory consuming. So, I am trying to write to HDF5 files with &lt;code&gt;h5py&lt;/code&gt; library and produce HDF5 files that are vaex friendly (column based).&lt;/p&gt;
&lt;p&gt;In order to do this, I am first converting &lt;code&gt;bz2&lt;/code&gt; files to &lt;code&gt;.csv&lt;/code&gt; upon download -&amp;gt; mapping the datatypes of each of the columns -&amp;gt; writing to a HDF5 file with &lt;code&gt;h5py&lt;/code&gt;. I am currently stuck at mapping the datatypes and also trying to make optimizations with respect to the chunksize.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;This blog is just a quick update on the things that are happening currently. Once the writing to a HDF5 files is completed, look out for a detailed tutorial on the same on towardsdatascience :)&lt;/p&gt;&lt;/div&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2021/08/20210802_0300_gagan-aryan/</guid><pubDate>Mon, 02 Aug 2021 02:00:06 GMT</pubDate></item><item><title>About my Google Summer of Code Project: Part 3</title><link>http://openastronomy.org/Universe_OA/posts/2021/08/20210801_1853_adwaitbhope/</link><dc:creator>Adwait Bhope</dc:creator><description>&lt;div&gt;&lt;p&gt;First and foremost, I celebrate the merging of the PR that brings reproject to NDCube! It defines a base-level functionality or MVP if you want to call it that, along with some relevant documentation. We also mark the release of ndcube‚Äôs 2.0 RC1. This is an important milestone since ndcube 2.0 brings significant changes, owing to the implementation of the new high-level WCS¬†API.&lt;/p&gt;
&lt;p&gt;Our next plan of action was to extend the method to use other algorithms that reproject supports. Interpolation (the one that the above PR implements) supports multi-dimensional cubes but ‚Äúadaptive‚Äù and ‚Äúexact‚Äù algorithms do not. For the time being, they only work on 2D cubes containing celestial axes. So that‚Äôs what I‚Äôve implemented them for in a new PR, which is currently under review and should hopefully get merged¬†soon.&lt;/p&gt;
&lt;p&gt;The only problem for this PR was identifying celestial axes. We‚Äôve taken a shortcut to solve this quickly and avoid creating a blocker, but a better implementation is¬†due.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;The NDCubeSequence PR that I talked about in the last blog post hit a few unexpected edge cases which are under¬†work.&lt;/p&gt;
&lt;p&gt;We‚Äôre nearing the end of GSoC‚Äôs official timeline and while that is saddening, the good thing is that open source doesn‚Äôt need a GSoC timeline for contributing. I do hope that I‚Äôll be able to tie up any loose ends before the end date, but I suppose that does not matter in the community‚Äôs bigger picture. Functional additions, bug fixes, and performance improvements are always going to be coming in for reproject, and I plan to maintain at least that bit of code (or more) in the¬†future.&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=f6354389b27f" width="1"&gt;&lt;/div&gt;</description><category>SunPy</category><guid>http://openastronomy.org/Universe_OA/posts/2021/08/20210801_1853_adwaitbhope/</guid><pubDate>Sun, 01 Aug 2021 17:53:41 GMT</pubDate></item><item><title>Chapter 4: The Other Side</title><link>http://openastronomy.org/Universe_OA/posts/2021/08/20210801_1424_anandxkumar/</link><dc:creator>anandxkumar</dc:creator><description>&lt;div&gt;&lt;p&gt;A new month has started and I have started to see the light at the end of the tunnel. Good Morning and welcome back. Phase 2 has been rolling and let us look at the new findings.&lt;/p&gt;
&lt;p&gt;Earlier the complexity of Legacy method was determined. The complexity of LDM Voigt and LDM FFT was to be determined using similar approach. Upon executing several benchmarks based on Number of lines, Spectum range, wstep, broadening max width. Previously it was thought the complexity was: &lt;br&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;time(LDM_fft) ~ c2*Nlines + c3*(N_G*N_L + 1)*N_v*log(N_v) (where N_v =  Spectral Points)
&lt;!-- TEASER_END --&gt;
time(LDM_voigt) ~ c2*Nlines + c3'*(N_G*N_L + 1)*N_truncation*log(N_truncation) (where N_truncation = broadening width / wstep)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;But in actual Re running all benchmarks for &lt;strong&gt;LDM&amp;gt;Voigt&lt;/strong&gt; and &lt;strong&gt;LDM&amp;gt;FFT&lt;/strong&gt; with a &lt;code class="language-text"&gt;broadening max width = 300 cm-1&lt;/code&gt;. All benchmarks and visualizations can be found &lt;a href="https://anandxkumar.github.io/Benchmark_Visualization_GSoC_2021/"&gt;here&lt;/a&gt; we were able to conclude the followings:&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;FFT:&lt;/strong&gt;&lt;br&gt;
‚Ä¢  Complexity doesn‚Äôt depend on Nlines but rather wL x wG ; check this benchmark: &lt;a href="https://public.tableau.com/app/profile/anand.kumar4841/viz/LDMLinesvsCalculationTimeUpdatedCO2/Sheet1"&gt;link&lt;/a&gt;, it certainly looks like Complexity ‚àù Nlines but its actually dependent on wL and wG, and gives same result on (wL x wG+ 1) x Spectral&lt;em&gt;Points x Log(Spectral&lt;/em&gt;Points).&lt;br&gt;
‚Ä¢  Upon implementing multiple linear regression for &lt;strong&gt;c1 x Nlines + c2 x (wL x wG+ 1)*Spectral&lt;em&gt;Points x Log(Spectral&lt;/em&gt;Points)&lt;/strong&gt; gives &lt;code class="language-text"&gt;c1=2.65e-07&lt;/code&gt;, &lt;code class="language-text"&gt;c2=4.48256e-08&lt;/code&gt; but their &lt;code class="language-text"&gt;p value = 0.648 and 0.00001&lt;/code&gt;, and &lt;code class="language-text"&gt;p&amp;gt;0.05&lt;/code&gt; are insignificant, thus Nlines is insignificant for determining the complexity.&lt;br&gt;
‚Ä¢  Since FFT is independent of broadening max width; benchmark: &lt;a href="https://public.tableau.com/app/profile/anand.kumar4841/viz/LDMVoigtandFFTBMW_NEW/Sheet1"&gt;link&lt;/a&gt;, so on comparing it Spectral point gives us same same time. Thus Spectral Point =  (wavenum max - wavenum max)/wstep instead of (wavenum maxcalc - wavenum min calc)/wstep&lt;br&gt;
‚Ä¢  &lt;strong&gt;Overall complexity =  4.48256897e-08 x (wL x wG+ 1) x Spectral&lt;em&gt;Points(without BMW) x Log(Spectral&lt;/em&gt;Points(without BMW))&lt;/strong&gt;  &lt;a href="https://anandxkumar.github.io/Benchmark_Visualization_GSoC_2021/LDM/Complexity_FFT_Final/Complexity_FFT_Final.html"&gt;link&lt;/a&gt; (with the help of multple linear regression using sklearn; is almost accurate)&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Voigt:&lt;/strong&gt;&lt;br&gt;
‚Ä¢  Similar to 1st point of FFT.&lt;br&gt;
‚Ä¢  Upon doing multiple linear regression for &lt;strong&gt;c1 x N lines + c2 x (wL x wG + 1) x SpectralPoints x BMW xLog(SpectralPoints x (BMW) )&lt;/strong&gt; gives &lt;code class="language-text"&gt;c1=-1.9392e-06, c2=1.28256e-09&lt;/code&gt; but their &lt;code class="language-text"&gt;p value = 0.848 and 0.00001&lt;/code&gt;, and &lt;code class="language-text"&gt;p&amp;gt;0.05&lt;/code&gt; are insignificant, thus N&lt;em&gt;lines is insignificant for determining the complexity.&lt;br&gt;
‚Ä¢  Calculation time is dependent `Broadening&lt;/em&gt;Max&lt;em&gt;width`, but upon inspections with Spectral Points, we have the exact same plot. So complexity is dependent only on Spectral Points but with broadening&lt;/em&gt;max&lt;em&gt;width i.e. wavenum&lt;/em&gt;calc, which causes the increase in computational time on increasing broadening&lt;em&gt;max&lt;/em&gt;width.&lt;br&gt;
‚Ä¢  &lt;strong&gt;Overall complexity = 5.26795e-07 * (wL x wG+ 1)*Spectral Points x Log(Spectral Points)&lt;/strong&gt; &lt;a href="https://anandxkumar.github.io/Benchmark_Visualization_GSoC_2021/LDM/Complexity_Voigt_Final/Complexity_Voigt_Final.html"&gt;link&lt;/a&gt; (with the help of multple linear regression using sklearn; almost straight)&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Also:&lt;/strong&gt; From all the above plots, it really clear if going with broadening&lt;em&gt;max&lt;/em&gt;width=300cm-1 in wavespace, it will take alot more time than fft in all aspects.&lt;/p&gt;
&lt;p&gt;But upon replacing &lt;code class="language-text"&gt;np.convolve&lt;/code&gt; with &lt;code class="language-text"&gt;scipy.signal.oaconvolve&lt;/code&gt;, we were able to achieve &lt;code class="language-text"&gt;2 to 30&lt;/code&gt; times performance boost. So it will be interesting to re run benchmarks with the latest piece of code and see which method performs better. Also some benchmarks will be added to ASV benchmark too to see how its performance changes over time.&lt;/p&gt;
&lt;p&gt;Also profiler was modified to a tree like a stucture using &lt;code class="language-text"&gt;OrderedDict&lt;/code&gt; and &lt;code class="language-text"&gt;YAML&lt;/code&gt; has been used to print the profiler in a proper structued way using &lt;strong&gt;Spectrum.print_perf_profiler()&lt;/strong&gt; or &lt;strong&gt;SpectrumFactory.print_perf_profiler()&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;s = calc_spectrum(1900, 2300,         # cm-1
molecule='CO',
isotope='1,2,3',
pressure=1.01325,   # bar
Tvib=1000,          # K
Trot=300,           # K
mole_fraction=0.1,
verbose=3,
)
s.print_perf_profile()&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Gives the following output:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;&amp;gt;&amp;gt;&amp;gt; spectrum_calculation:
&amp;gt;&amp;gt;&amp;gt;   applied_linestrength_cutoff: 0.0024361610412597656
&amp;gt;&amp;gt;&amp;gt;   calc_emission_integral: 0.006468772888183594
&amp;gt;&amp;gt;&amp;gt;   calc_hwhm: 0.006415128707885742
&amp;gt;&amp;gt;&amp;gt;   calc_line_broadening:
&amp;gt;&amp;gt;&amp;gt;     DLM_Distribute_lines: 0.0003898143768310547
&amp;gt;&amp;gt;&amp;gt;     DLM_Initialized_vectors: 9.775161743164062e-06
&amp;gt;&amp;gt;&amp;gt;     DLM_closest_matching_line: 0.0005028247833251953
&amp;gt;&amp;gt;&amp;gt;     DLM_convolve: 0.029767990112304688
&amp;gt;&amp;gt;&amp;gt;     precompute_DLM_lineshapes: 0.013132810592651367
&amp;gt;&amp;gt;&amp;gt;     value: 0.07619166374206543
&amp;gt;&amp;gt;&amp;gt;   calc_lineshift: 0.00074005126953125
&amp;gt;&amp;gt;&amp;gt;   calc_noneq_population:
&amp;gt;&amp;gt;&amp;gt;     part_function: 0.03405046463012695
&amp;gt;&amp;gt;&amp;gt;     population: 0.005669832229614258
&amp;gt;&amp;gt;&amp;gt;     value: 0.03983640670776367
&amp;gt;&amp;gt;&amp;gt;   calc_other_spectral_quan: 0.002928495407104492
&amp;gt;&amp;gt;&amp;gt;   calc_weight_trans: 0.008247852325439453
&amp;gt;&amp;gt;&amp;gt;   check_line_databank: 0.0002810955047607422
&amp;gt;&amp;gt;&amp;gt;   check_non_eq_param: 0.04109525680541992
&amp;gt;&amp;gt;&amp;gt;   fetch_energy_5: 0.014983654022216797
&amp;gt;&amp;gt;&amp;gt;   generate_spectrum_obj: 0.00032138824462890625
&amp;gt;&amp;gt;&amp;gt;   generate_wavenumber_arrays: 0.0010433197021484375
&amp;gt;&amp;gt;&amp;gt;   reinitialize:
&amp;gt;&amp;gt;&amp;gt;     copy_database: 2.1457672119140625e-06
&amp;gt;&amp;gt;&amp;gt;     memory_usage_warning: 0.0018389225006103516
&amp;gt;&amp;gt;&amp;gt;     reset_population: 2.6226043701171875e-05
&amp;gt;&amp;gt;&amp;gt;     value: 0.001964569091796875
&amp;gt;&amp;gt;&amp;gt;   scaled_non_eq_linestrength:
&amp;gt;&amp;gt;&amp;gt;     corrected_population_se: 0.002747774124145508
&amp;gt;&amp;gt;&amp;gt;     map_part_func: 0.0010590553283691406
&amp;gt;&amp;gt;&amp;gt;     value: 0.0038983821868896484
&amp;gt;&amp;gt;&amp;gt;   value: 0.1904621124267578&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So at the end a productive week! Looking forward to conclude GSoC with a worthy ending :)&lt;/p&gt;&lt;/div&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2021/08/20210801_1424_anandxkumar/</guid><pubDate>Sun, 01 Aug 2021 13:24:32 GMT</pubDate></item><item><title>GSoC Post 3</title><link>http://openastronomy.org/Universe_OA/posts/2021/08/20210801_1244_ndanzanello/</link><dc:creator>ndanzanello</dc:creator><description>&lt;div&gt;&lt;p&gt;Hi! In my last post I mentioned that we would start calculating the distortions contained in the image. But we followed a different path! As the linear part was ready, we first worked on making some plots (scatter plots with side histograms of the difference in pixel scale of the celestial coordinates measured with the WCS we find and the celestial coordinates given as input) and drawing some quads to visualize it. This part was done using LaTeX and TikZ, a wonderful tool to produce graphics!&lt;/p&gt;


&lt;!-- TEASER_END --&gt;

&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter size-large"&gt;&lt;img alt="" class="wp-image-73" src="https://ndanzanello.files.wordpress.com/2021/08/image.png?w=342"&gt;&lt;figcaption&gt;Example of a quad drawn using TikZ. The black points are the stars of the catalog.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;



&lt;p&gt;After that, we started to evaluate our results, and some changes were made: in statistics, for example, instead of getting direct the median, we use sigma clipping (a technique that removes outliers), allowing a better result. We also compared our results with one well established software: Astrometry.net. We‚Äôre getting pretty good results, but our running time was way bigger than the Astrometry.net one. So, we started working on that, and we have some ways to decrease our running time, such as making an only geo-hash search on the kdtree before the search containing the magnitude hashes. This reduces the dimentionality, which degrades the performance the higher it is. Other solution is to divide the celestial catalog in tiles, decreasing the number of total quads that we have to evaluate. Also, we can reduce the number of stars that we use to make quads. With these approaches, our running time got way better! &lt;img alt="üôÇ" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" style="height: 1em;"&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;/div&gt;</description><category>gnuastro</category><guid>http://openastronomy.org/Universe_OA/posts/2021/08/20210801_1244_ndanzanello/</guid><pubDate>Sun, 01 Aug 2021 11:44:53 GMT</pubDate></item><item><title>Balance</title><link>http://openastronomy.org/Universe_OA/posts/2021/07/20210730_1925_jeffreypaul15/</link><dc:creator>Jeffrey Paul</dc:creator><description>&lt;div&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/400/1*6vTTZ8HC33kEpYk0Wr8gxg.jpeg"&gt;&lt;figcaption&gt;‚ÄúCosmic Balance‚Äù‚Ää‚Äî‚Ääcompletely unrelated type of balance to what I go on to talk to talk about in this post. Although, outer space looks all fancy and it goes well with the theme of OpenAstronomy.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;From all my previous posts, it is quite evident that I obsess over small and insignificant details, mainly directed towards how my life is going to be in the future and this entire concept of ‚Äúhappiness‚Äù. However, the past few months of working with Sunpy has brought about this odd sense of calm. The entire feeling of stress being an emotion goes out the window. Maybe this is what doing something you love, maybe it‚Äôs the people‚Ää‚Äî‚Ääor maybe it‚Äôs just extremely good timing combined with coincidence.&lt;/p&gt;
&lt;p&gt;Ah, I suppose these questions don‚Äôt have simple answers. Regardless, whatever I‚Äôm doing with right now has restored that balance that I was longing¬†for.&lt;/p&gt;
&lt;p&gt;Coming to what has been happening with sunkit-pyvista. To summarize, I spent an entire week fixing things that I caused due to over-confidence¬†:)&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;Aside from that, I think I‚Äôm nearing the end of this project. A few bits of functionality has to be added in but for the most part, I think it‚Äôs all in¬†there.&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Saving and loading entire scenes are now possible with the work that we‚Äôve done. This took quite a bit of time and it was pretty interesting to see how we could extend functionality to such massive¬†extents.&lt;/li&gt;&lt;li&gt;We uncovered a few hidden issues that may have occurred because of me. Ones such as ill-defined tests because I may have been slightly over-confident with how I write¬†code.&lt;/li&gt;&lt;li&gt;Quite a bit of time was spent resolving these issues, but I can definitely conclude that it was well worth the effort and I certainly learnt my¬†lesson.&lt;/li&gt;&lt;li&gt;Figure tests are now a thing, we drew some parallels with Pyvista‚Äôs code and structured our own figure testing methodology which makes it easier for us to visually identify any mishaps in our plots. After all, we‚Äôre creating a library for data visualization. It‚Äôd be sad if our code tests pass and we‚Äôre under the assumption that everything is working fine (yet another dig at myself for not writing efficient tests).&lt;/li&gt;&lt;li&gt;I can safely say that Sunkit-Pyvista is quite balanced and usable now, or at least I hope¬†so.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Getting back to how this might be the last few PRs of this project under the whole ‚ÄúGSoC timeline‚Äù, this makes me both sad and happy. It‚Äôs saddening to see that something I worked towards for over 6 months has kind of come to an end. Happy because I‚Äôve gotten to work with some of the best developers and I genuinely enjoyed every bit of it. This offsets the balance in my life, but I think we may have a solution to¬†this?&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=239840d26318" width="1"&gt;&lt;/div&gt;</description><category>SunPy</category><guid>http://openastronomy.org/Universe_OA/posts/2021/07/20210730_1925_jeffreypaul15/</guid><pubDate>Fri, 30 Jul 2021 18:25:35 GMT</pubDate></item><item><title>astropy@GSoC Blog Post #5, Week 6&amp;7</title><link>http://openastronomy.org/Universe_OA/posts/2021/07/20210721_2000_suyog7130/</link><dc:creator>Suyog Garg</dc:creator><description>&lt;div&gt;&lt;p&gt;Hi,&lt;/p&gt;
&lt;p&gt;How are you?&lt;/p&gt;
&lt;p&gt;My dear mentors and I have decided to have the MRT (Machine Readable Table) format writing first. The same CDS code as been used now will be used, just the template of the written table will be in the MRT format.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;Points to be noted regarding this and the immediate things that have been and will done are as follows:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul style="text-align: left;"&gt;&lt;li&gt;Leave out writing all the optional CDS ReadMe fields as of now. These can be dealt with individual PRs later.&lt;/li&gt;&lt;li&gt;Some tests fail because &lt;span style="font-family: courier;"&gt;start_line = None&lt;/span&gt; doesn't work. It has been introduced once again within &lt;span style="font-family: courier;"&gt;CdsData.write&lt;/span&gt; function in addition to been defined in the main &lt;span style="font-family: courier;"&gt;Cds&lt;/span&gt; class. The test failure occurs because CdsData now inherits from &lt;span style="font-family: courier;"&gt;FixedWidthData&lt;/span&gt; which itself inherits &lt;span style="font-family: courier;"&gt;basic.BasicReader&lt;/span&gt; instead of BaseReader. I should make sure that all tests pass properly.&lt;/li&gt;&lt;li&gt;Have a template for MRT tables and write them first. &lt;b&gt;Title&lt;/b&gt;, &lt;b&gt;Authors&lt;/b&gt;, &lt;b&gt;Date&lt;/b&gt;, &lt;b&gt;Caption&lt;/b&gt; and &lt;b&gt;Notes&lt;/b&gt; sections, i.e. all sections except the Byte-By-Byte and the Data itself, will be left blank in the template, with warning for the user to put them in manually afterwards.&lt;/li&gt;&lt;li&gt;Documentation for the CDS/MRT format writer.&lt;/li&gt;&lt;li&gt;At present issue a warning note for tables with two or more mix-in columns (&lt;span style="font-family: courier;"&gt;SkyCoord&lt;/span&gt; cols primarily). If ways to correctly work out such situations is thought of, add that feature in a separate PR.&lt;/li&gt;&lt;li&gt;Work with a copy of the original table, so that¬† the copy is modified and not the original table, when component coordinate columns are written. The modified copy of the table is written to a file, while the user retains access to the columns of the original table.&lt;/li&gt;&lt;li&gt;Need to have features to recognise non Spherical coordinates, like the Cartesian coordinates, and either skip them or write them as Single column string values. Add test for such other coordinates. Also for cases when coordinates are in a &lt;span style="font-family: courier;"&gt;SkyCoord&lt;/span&gt; object but the frame is not Spherical.&lt;/li&gt;&lt;li&gt;Have two other templates, one for CDS in which the user fills values of optional fields manually later and another in which filling optional fields can be done from within Astropy, via a &lt;span style="font-family: courier;"&gt;cdsdict&lt;/span&gt;. In separate PRs. Here too write only the required fields in the ReadMe first, like &lt;b&gt;Abstract&lt;/b&gt;.&lt;/li&gt;&lt;li&gt;Have features for Time columns later within the original PR or much later.&lt;/li&gt;&lt;li&gt;Simplify how column format is obtained for float columns. The current manner of string formatting is too complicated. &lt;span style="font-family: courier;"&gt;col.width&lt;/span&gt; value can be directly used in some cases. The &lt;span style="font-family: courier;"&gt;Outputter&lt;/span&gt; class will also know the column format since it writes out the table.&lt;/li&gt;&lt;li&gt;Other minor/major edits and modifications as suggested by others.&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;With this PR for the MRT format table writing getting eventually merged to Astropy, the main goal of my astropy@GSoC project will be completed. The support for other extra features essentially serves as appendages to the primary task been done by this PR.&lt;/div&gt;&lt;div&gt;Let's see how it goes.&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Oh! On another note, a few days back I received the GSoC First Evaluations payment! üòÅ&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Adious!&lt;/div&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;</description><category>Astropy</category><guid>http://openastronomy.org/Universe_OA/posts/2021/07/20210721_2000_suyog7130/</guid><pubDate>Wed, 21 Jul 2021 19:00:00 GMT</pubDate></item><item><title>Halfway into GSoC</title><link>http://openastronomy.org/Universe_OA/posts/2021/07/20210719_1916_dhruv9vats/</link><dc:creator>Dhruv Vats</dc:creator><description>&lt;div&gt;&lt;p&gt;This introductory adventure to Open Source is already at its midpoint, and while the learnings have been great and the experiences meaningful, I‚Äôm sure many of my fellow participants feel that a program like this should have an extended duration, and I am no exception. Such an extended timeline could provide many benefits, such as the ability to work on more complex and sophisticated projects, more time to collaborate and improve, to name a¬†few.&lt;/p&gt;
&lt;h5&gt;First Evaluation&lt;/h5&gt;&lt;p&gt;Another noteworthy thing concerning GSoC that happened in the last week was that the results of the first evaluation were declared, and while most cleared it, some didn‚Äôt. Although there is little to no need to question their abilities, sometimes life just doesn‚Äôt go as planned; it seems easy to say that that‚Äôs what the real test is, nevertheless it can quickly become something tricky to cope¬†with.&lt;/p&gt;
&lt;h5&gt;What next?&lt;/h5&gt;&lt;p&gt;While most of the ‚Äúproposed‚Äù work has been done, I will now be preparing some tutorials for the newly added functionality and tools, in an attempt to reduce the barrier to experimentation, use, and possible adoption of these new techniques into the workflow of its¬†users.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;So while this could take a good amount of time if quality is needed, there will surely some be time to play around with other things, but what exactly will end up happening will be answered by¬†time.&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=b6f9ec014333" width="1"&gt;&lt;/div&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2021/07/20210719_1916_dhruv9vats/</guid><pubDate>Mon, 19 Jul 2021 18:16:00 GMT</pubDate></item><item><title>Chapter 3: Midnight Sun</title><link>http://openastronomy.org/Universe_OA/posts/2021/07/20210719_1645_anandxkumar/</link><dc:creator>anandxkumar</dc:creator><description>&lt;div&gt;&lt;p&gt;Phase 1 is over :) ! We are half way through the journey. Great learning experience so far. Let‚Äôs find out what I accomplished during the previous 2 weeks (since I believe you have been following me from the beginning ;)&lt;/p&gt;
&lt;p&gt;Getting straight to the point, most of the time was spent on fixing bugs of the Profiler class and other Pull requests regarding documentation and gallery example. A new gallery example was added to demonstrate the working of &lt;code class="language-text"&gt;SpecDatabase&lt;/code&gt; and &lt;code class="language-text"&gt;init_database&lt;/code&gt; to help user to store all Spectrums in the form of a &lt;code class="language-text"&gt;.spec&lt;/code&gt; file and all input parameters in a &lt;code class="language-text"&gt;csv&lt;/code&gt; file under a folder. The same folder can be used to retrieve all Spectrums thus saving a lot of time and also no need to recompute all spectrums, so quite a handy feature. Radis has &lt;code class="language-text"&gt;plot_cond&lt;/code&gt; function to plot a 2D heat map based on the parameters in csv file for all spectrums. Creates some good looking and informative plots :) &lt;br&gt;-&amp;gt; &lt;a href="https://radis.readthedocs.io/en/latest/auto_examples/plot_SpecDatabase.html#sphx-glr-auto-examples-plot-specdatabase-py"&gt;Gallery Example&lt;/a&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Back to the analysis part; for LDM we expected:&lt;br&gt;&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;time(LDM_fft) ~ c2*N_lines + c3*(N_G*N_L + 1)*N_v*log(N_v) (where N_v =  Spectral Points)
time(LDM_voigt) ~ c2*N_lines + c3'*(N_G*N_L + 1)*N_truncation*log(N_truncation) (where N_truncation = broadening width / wstep)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For Legacy method I was able to prove that Calculation Time is independent of Spectral Range if we keep the N&lt;em&gt;lines and wstep constant but same is not for LDM voigt.&lt;br&gt;
A straight up comparison between Legacy and LDM voigt for NO  keeping N&lt;/em&gt;lines and wstep constant and varying the Spectral range:
&lt;a href="https://public.tableau.com/app/profile/anand.kumar4841/viz/LDMvsLegacyforSpectralRangeN_linesconstantandVoigtbroadening/Sheet1"&gt;Link&lt;/a&gt;&lt;br&gt;
Here also for None optimization we are getting constant time for different spectral range but a linear dependency for LDM Voigt which will fail the assumption of&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;t_LDM_voigt ~ c2*N_lines + c3'*(N_G*N_L + 1)*N_truncation  *log(N_truncation  )
but rather t_LDM_voigt ~ c2*N_lines + c3*(N_G*N_L + 1)*N_v*log(N_v)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;A New Discovery&lt;/h3&gt;
&lt;p&gt;On generating spectrum for millions of lines, one unique observation was seen. The bottleneck step was no longer taking the most time. Max time was spent upon an unknown process. Upon deep analysis it was found a part of code was using &lt;code class="language-text"&gt;sys.getsizeof()&lt;/code&gt; to get the size of dataframe, and when the dataframe consisited of &lt;code class="language-text"&gt;object&lt;/code&gt; type columns with millions of lines, most of the time was spent on this step only.&lt;/p&gt;
&lt;p&gt;&lt;span class="gatsby-resp-image-wrapper" style="display: block; margin-left: auto; margin-right: auto;"&gt;
&lt;a class="gatsby-resp-image-link" href="https://anandkumar-blog.netlify.app/static/95eda74e349d883f4a1fcc85291a91cc/6af66/ldm.png" rel="noopener" style="display: block;" target="_blank"&gt;
&lt;span class="gatsby-resp-image-background-image" style="display: block;"&gt;&lt;/span&gt;
&lt;img alt="complexity.jpg" class="gatsby-resp-image-image" src="https://anandkumar-blog.netlify.app/static/95eda74e349d883f4a1fcc85291a91cc/f058b/ldm.png" style="width: 100%; height: 100%; margin: 0; vertical-align: middle;" title="complexity.jpg"&gt;
&lt;/a&gt;
&lt;/span&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;We replaced it with &lt;code class="language-text"&gt;memory_usage(deep=False)&lt;/code&gt; with a different threshold which made computation almost &lt;strong&gt;2x&lt;/strong&gt; faster.&lt;/p&gt;
&lt;p&gt;&lt;span class="gatsby-resp-image-wrapper" style="display: block; margin-left: auto; margin-right: auto;"&gt;
&lt;a class="gatsby-resp-image-link" href="https://anandkumar-blog.netlify.app/static/28b1ad4d276fa9921520808bc6360002/87488/ba.png" rel="noopener" style="display: block;" target="_blank"&gt;
&lt;span class="gatsby-resp-image-background-image" style="display: block;"&gt;&lt;/span&gt;
&lt;img alt="complexity.jpg" class="gatsby-resp-image-image" src="https://anandkumar-blog.netlify.app/static/28b1ad4d276fa9921520808bc6360002/f058b/ba.png" style="width: 100%; height: 100%; margin: 0; vertical-align: middle;" title="complexity.jpg"&gt;
&lt;/a&gt;
&lt;/span&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;So phase 1 is over,and phase 2 is going to begin which will mainly focus on optimizing the the existing LDM method with appropriate truncation and other possible areas!&lt;/p&gt;
&lt;p&gt;See you on the other side of the sea ;)&lt;/p&gt;
&lt;p&gt;&lt;span class="gatsby-resp-image-wrapper" style="display: block; margin-left: auto; margin-right: auto;"&gt;
&lt;a class="gatsby-resp-image-link" href="https://anandkumar-blog.netlify.app/static/6c695ad1951b1c737cc12c701ffce0e4/2551b/other.jpg" rel="noopener" style="display: block;" target="_blank"&gt;
&lt;span class="gatsby-resp-image-background-image" style="display: block;"&gt;&lt;/span&gt;
&lt;img alt="complexity.jpg" class="gatsby-resp-image-image" src="https://anandkumar-blog.netlify.app/static/6c695ad1951b1c737cc12c701ffce0e4/828fb/other.jpg" style="width: 100%; height: 100%; margin: 0; vertical-align: middle;" title="complexity.jpg"&gt;
&lt;/a&gt;
&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;/div&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2021/07/20210719_1645_anandxkumar/</guid><pubDate>Mon, 19 Jul 2021 15:45:32 GMT</pubDate></item><item><title>GSoC - 2</title><link>http://openastronomy.org/Universe_OA/posts/2021/07/20210719_0300_gagan-aryan/</link><dc:creator>Gagan Aryan</dc:creator><description>&lt;div&gt;&lt;p&gt;Welcome back !! So we are done with our first phase of the project and are shifting into the second one. I will be keeping this blog short since most of the details of the refactor have already been written in my previous post.&lt;/p&gt;
&lt;p&gt;By the time I was writing my previous post I had a pretty decent idea of how I would be doing each of the refactors. We had already decided that we may not have to implement all of them because Vaex might render a few of those changes redundant.&lt;/p&gt;
&lt;p&gt;I started out by writing a proof-of-concept to remove the column where partition function was added. Only the case of equilibrium molecules was handled here. The idea was to make use of pandas‚Äô dictionary efficiently and remove the column. With the proof-of-concept we could conclude that not only did this approach reduce memory, but it also reduced CPU pressure by around 2x. For the lines of &lt;code&gt;HITEMP-CH4&lt;/code&gt; molecules for the waverange 2000-3000 previously the dataframe occupied 1.2 GB but with this method we could compress that to around 100 MB. &lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;Apart from this I wrote down another notebook that demostrated that we can radically improve memory usage by crunching the datatypes of the columns of &lt;code&gt;HITRAN/HITEMP&lt;/code&gt; molecules. The notebook just contains elementary operations to arrive at the right datatype for each of the column. We haven‚Äôt implemented this into the codebase yet because we still haven‚Äôt figured out what we will be doing with the missing lines. A problem I had already mentioned in my first post. &lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;I was somehow able to sneak my way into successfully completing GSoC phase one with feedback that has pumped me to do even better. I am looking forward to the second phase and hope to deliver.&lt;/p&gt;
&lt;div class="footnotes"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;&lt;a href="https://github.com/radis/radis-benchmark/blob/master/manual_benchmarks/test_Qgas.ipynb"&gt;Proof-of-Concept for Qgas&lt;/a&gt; &lt;a class="footnote-backref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fnref:1"&gt;‚Ü©Ô∏é&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;&lt;a href="https://github.com/radis/radis-benchmark/pull/11"&gt;Proof-of-Concept for Datatype Crunching - WIP&lt;/a&gt; &lt;a class="footnote-backref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fnref:2"&gt;‚Ü©Ô∏é&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2021/07/20210719_0300_gagan-aryan/</guid><pubDate>Mon, 19 Jul 2021 02:00:06 GMT</pubDate></item></channel></rss>