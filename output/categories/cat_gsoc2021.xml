<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Universe OpenAstronomy (Posts about gsoc2021)</title><link>http://openastronomy.org/Universe_OA/</link><description></description><atom:link href="http://openastronomy.org/Universe_OA/categories/cat_gsoc2021.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Sun, 15 Aug 2021 05:08:19 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>astropy@GSoC Blog Post #6.5 - Week 10, Final Evaluations</title><link>http://openastronomy.org/Universe_OA/posts/2021/08/20210814_2236_suyog7130/</link><dc:creator>Suyog Garg</dc:creator><description>&lt;div&gt;&lt;p&gt;¬†You know, 7's my lucky number.&lt;/p&gt;
&lt;p&gt;And Happy Independence Day!&lt;/p&gt;
&lt;!-- TEASER_END --&gt;&lt;/div&gt;</description><category>Astropy</category><guid>http://openastronomy.org/Universe_OA/posts/2021/08/20210814_2236_suyog7130/</guid><pubDate>Sat, 14 Aug 2021 21:36:00 GMT</pubDate></item><item><title>A Glimpse into my GSoC project</title><link>http://openastronomy.org/Universe_OA/posts/2021/08/20210812_2028_dhruv9vats/</link><dc:creator>Dhruv Vats</dc:creator><description>&lt;div&gt;&lt;p&gt;While all my blog posts till now were kind of abstract, here I will try to show some of the technical details of the project without making it too bloated. So as a one-line description, I had to study, implement and integrate a spectral estimation technique, namely the &lt;em&gt;Multitaper Periodogram¬≥&lt;/em&gt; (and its derivatives¬π), which are used to analyze astronomical time¬†series.&lt;/p&gt;
&lt;h5&gt;Why spectral representations?&lt;/h5&gt;&lt;p&gt;Before getting into the how of spectral analysis and its estimation, a brief sidenote on the why. Why do we even bother to study the spectral properties of a time series? It turns out, some of the determining characteristics or defining parameters associated with a certain time series are better &lt;em&gt;brought out&lt;/em&gt; in their spectral representations (frequency domain representations).&lt;/p&gt;
&lt;p&gt;As an example, the power spectral density is a common tool to try and unearth the periodic element(s) in a time series. Such spectral analysis techniques, at their core, are enabled by the Fourier Transform, and if you‚Äôd like to gain a better intuitive understanding of it, do check out &lt;a href="https://www.youtube.com/watch?v=spUNpyF58BY"&gt;this awesome video&lt;/a&gt; by &lt;a href="https://www.youtube.com/c/GrantSanderson"&gt;Grant Sanderson&lt;/a&gt; on &lt;a href="https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw"&gt;3Blue1Brown&lt;/a&gt;.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;h5&gt;Multi what?&lt;/h5&gt;&lt;p&gt;Spectral analysis isn‚Äôt without its fair share of practical complications, in fact, far from it. But there have been quite a few very effective techniques to mitigate¬†them.&lt;/p&gt;
&lt;p&gt;One of them is &lt;em&gt;tapering (&lt;/em&gt;multiplying the time series with a bell-shaped function), effective in reducing spectral leakage. Tapering a time series as a way of obtaining a spectral estimator with acceptable bias properties is an important concept. The loss of information (contained at the extremes of the time series) inherent in tapering can often be avoided either by prewhitening or by using Welch‚Äôs overlapped segment averaging.&lt;/p&gt;
&lt;p&gt;The multitaper periodogram is another approach to recover information lost due to tapering. This approach was introduced by Thomson (1982)¬≥ and involves the use of multiple orthogonal tapers, having approximately uncorrelated spectral densities.&lt;/p&gt;
&lt;p&gt;In the multitaper method, the data is windowed or tapered, but this method differs from the traditional methods in the tapers used, which are the most band-limited functions amongst those defined on a finite time domain, and also, these tapers are orthogonal, enabling us to average the &lt;em&gt;eigenspectrum&lt;/em&gt; (spectrum estimates from individual tapers) from more than one tapers to obtain a superior estimate in terms of noise. The resulting spectrum has low leakage, low variance, and retains information contained in the beginning and end of the time¬†series.&lt;/p&gt;
&lt;p&gt;The tapers used are the discrete prolate spheroidal sequences (DPSS), or, the Slepians (Slepian¬†1978)‚Å¥.&lt;/p&gt;
&lt;h5&gt;A look at the DPSS¬†tapers&lt;/h5&gt;&lt;p&gt;Let‚Äôs consider a time series sampled from an autoregressive process of order 4, AR(4), which has been frequently exemplified in literature¬π in similar contexts.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/657/1*h-k9Xm1CgerMQVIeKbfyOw.png"&gt;&lt;figcaption&gt;A time series sampled from an autoregressive process of order¬†4.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;A good way to gain an intuitive understanding of the properties of the DPSS tapers, and how they affect the time series, is to visualize the effect. Given here are the time and frequency domain representations of the tapers and the tapered time¬†series.&lt;/p&gt;
&lt;p&gt;The first 8 tapers and the corresponding tapered time¬†series‚Åµ.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/645/1*SEG0OEstBuAgP5OqgnS4zA.png"&gt;&lt;/figure&gt;&lt;p&gt;This showcases the product of a windowing function and a time series quite well. Next let‚Äôs have a look at their spectral representations‚Åµ, more specifically, their power spectrum densities (PSD).&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/680/1*nvqbaD08gDf1zdh9P2ljqQ.png"&gt;&lt;figcaption&gt;For this example, we took the normalized half-bandwidth product to be equal to 4 (NW = 4), resulting in 8 tapers being used. The spectral concentration in the band [-W, W] can then be seen from the plots. (Here N, the number of data points, is 1024, hence, W = 4/N = 0.003906)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;There is a significant increase in the bias of the PSD estimates as the spectral concentration of the tapers worsens. To prevent these estimates with greater biases from affecting the final averaged estimate but still use the variance reductions they bring, we weigh the different estimates according to their spectral concentration (percentage of energy concentrated in the desired frequency band).&lt;/p&gt;
&lt;p&gt;This can be kicked up a notch by using what is called adaptive weighing, which adaptively (duh?!) combines the different estimates, calculating the weights using an iterative process.&lt;/p&gt;
&lt;h5&gt;A brief summary of the Multitaper spectral estimation&lt;/h5&gt;&lt;a href="https://medium.com/media/3f23eb5c2c41460b8793fbc2e6fbc04d/href"&gt;https://medium.com/media/3f23eb5c2c41460b8793fbc2e6fbc04d/href&lt;/a&gt;&lt;p&gt;This summary, by no means, is an exhaustive explanation of the multitapering concept. Further exploration of the topic is highly encouraged. Use the references as the starting¬†point.&lt;/p&gt;
&lt;h5&gt;The Final¬†Result&lt;/h5&gt;&lt;p&gt;Using all the techniques outlined here, let's see how well can this multitaper periodogram estimate the true spectrum of this auto-regressive process. Also added is the classical periodogram (also sometimes referred to as a na√Øve spectrum estimator because of its basic estimation process) for comparison.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/707/1*7eYiabftab1cYuMpBC4uqA.png"&gt;&lt;figcaption&gt;Here the multitaper estimate uses the adaptive weighting technique and the first 7 DPSS¬†tapers&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;All this functionality is now implemented in &lt;a href="https://github.com/StingraySoftware/stingray"&gt;Stingray&lt;/a&gt;¬≤&lt;/p&gt;
&lt;h5&gt;References&lt;/h5&gt;&lt;p&gt;[1]: Springford, Aaron, Gwendolyn M. Eadie, and David J. Thomson. 2020. ‚ÄúImproving the Lomb‚ÄìScargle Periodogram with the Thomson Multitaper.‚Äù The Astronomical Journal (American Astronomical Society) 159: 205. doi:10.3847/1538‚Äì3881/ab7fa1.&lt;/p&gt;
&lt;p&gt;[2]: Huppenkothen, Daniela, Matteo Bachetti, Abigail L. Stevens, Simone Migliari, Paul Balm, Omar Hammad, Usman Mahmood Khan, et al. 2019. ‚ÄúStingray: A Modern Python Library for Spectral Timing.‚Äù The Astrophysical Journal (American Astronomical Society) 881: 39. doi:10.3847/1538‚Äì4357/ab258d.&lt;/p&gt;
&lt;p&gt;[3]: Thomson, D. J. 1982. ‚ÄúSpectrum Estimation and Harmonic Analysis.‚Äù IEEE Proceedings 70: 1055‚Äì1096. &lt;a href="https://ui.adsabs.harvard.edu/abs/1982IEEEP..70.1055T."&gt;https://ui.adsabs.harvard.edu/abs/1982IEEEP..70.1055T.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4]: Slepian, D. 1978. ‚ÄúProlate Spheroidal Wave Functions, Fourier Analysis, and Uncertainty-V: The Discrete Case.‚Äù Bell System Technical Journal (Institute of Electrical and Electronics Engineers (IEEE)) 57: 1371‚Äì1430. doi:10.1002/j.1538‚Äì7305.1978.tb02104.x&lt;/p&gt;
&lt;p&gt;[5]: D.B. Percival and A.T. Walden, Spectral Analysis for Physical Applications: Multitaper and Conventional Univariate Techniques. Cambridge, U.K.: Cambridge Univ. Press,¬†1993.&lt;/p&gt;
&lt;p&gt;[6]: Thomson, D. J. 1990. ‚ÄúTime series analysis of Holocene climate data.‚Äù Philosophical Transactions of the Royal Society of London. Series A, Mathematical and Physical Sciences (The Royal Society) 330: 601‚Äì616. doi:10.1098/rsta.1990.0041&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=25c0fe3296dd" width="1"&gt;&lt;/div&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2021/08/20210812_2028_dhruv9vats/</guid><pubDate>Thu, 12 Aug 2021 19:28:01 GMT</pubDate></item><item><title>astropy@GSoC Blog Post #6, Week 8&amp;9</title><link>http://openastronomy.org/Universe_OA/posts/2021/08/20210809_0848_suyog7130/</link><dc:creator>Suyog Garg</dc:creator><description>&lt;div&gt;&lt;b&gt;Heads-up about the Progress of   &lt;a href="https://github.com/astropy/astropy/pull/11897"&gt;#11897&lt;/a&gt;&lt;/b&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;  In summary the situation of the concerned PR a few days back was 4 types of CI   test errors, one bug and possibly a need for modification of part of the code   copied from pycdsreadme. All these have been taken care of as detailed below,   but for the numpy depreciation warnings that keep coming up. I don't think we   can do anything about the latter's persistence as of now. I shall comment more   about it on GitHub as well. &lt;/div&gt;&lt;div&gt;  &lt;ol style="text-align: left;"&gt;    &lt;li&gt;      &lt;i&gt;File not found error&lt;/i&gt;: Moritz's HW, i.e. using       &lt;span style="font-family: courier;"&gt;get_pkg_data_filename&lt;/span&gt; import,       directly took care of this.     &lt;/li&gt;    &lt;li&gt;      &lt;i&gt;Error in coord col decimal places&lt;/i&gt;: The precision of the coordinate       component columns was getting set arbitrarily, which created difference in       the output for 32-bit and 62-bit machines, and possibly between different       operating systems. This has been corrected by having a fixed number of 12       digits after decimal for &lt;b&gt;RAs,&lt;/b&gt;¬†&lt;b&gt;DEs&lt;/b&gt; and the       latitude/longitude columns of Galactic and Ecliptic coords. This error       also relates with the Formats bug.     &lt;/li&gt;    &lt;li&gt;      &lt;span style="font-family: courier;"&gt;&lt;i&gt;SphericalRepresentation&lt;/i&gt;&lt;/span&gt;&lt;i&gt; col error&lt;/i&gt;: Now, this was a bit major issue compared to the two       above, although the solution was only 2 line changes. When the coords cols       were checked for and divided into components, the original SkyCoord col       was deleted right within the loop. This made the iteration index of the       loop to point to i+2 column after deletion, where i is the index of the       original &lt;span style="font-family: courier;"&gt;SkyCoord&lt;/span&gt; col. That is,       effectively skipping the immediate next column after the       &lt;span style="font-family: courier;"&gt;SkyCoord&lt;/span&gt; col, as it would have       receded by one place in the list. Got this fixed by popping the original       &lt;span style="font-family: courier;"&gt;SkyCoord&lt;/span&gt; col after all the       columns in the table have been iterated over. This way all       &lt;span style="font-family: courier;"&gt;object&lt;/span&gt; type columns are       converted to &lt;span style="font-family: courier;"&gt;Column&lt;/span&gt; objects       with &lt;span style="font-family: courier;"&gt;str&lt;/span&gt; values.     &lt;/li&gt;    &lt;li&gt;      &lt;i&gt;~table.tests and &lt;/i&gt;&lt;span style="font-family: courier;"&gt;&lt;i&gt;test_write&lt;/i&gt;&lt;/span&gt;&lt;i&gt; failures&lt;/i&gt;: All these errors were warnings due to depreciation of       numpy specific aliases for different Python types. Most previous tests in       Astropy appear to use these now depreciated numpy types, which raises       warnings during testing our code. I have been able to provide remedy for       majority of these by additionally using       &lt;span style="font-family: courier;"&gt;np.issubdtype(col.dtype, np.integer)&lt;/span&gt;      while checking if the columns has integer values, however, tests with       oldest supported version of all dependencies still fails. See my GitHub       comment for more info.     &lt;/li&gt;  &lt;/ol&gt;  &lt;i&gt;&lt;div&gt;&lt;i&gt;&lt;br&gt;&lt;/i&gt;&lt;/div&gt;The &lt;/i&gt;&lt;span style="font-family: courier;"&gt;&lt;i&gt;formats&lt;/i&gt;&lt;/span&gt;&lt;i&gt; bug&lt;/i&gt;&lt;/div&gt;&lt;div&gt;&lt;i&gt;&lt;br&gt;&lt;/i&gt;&lt;/div&gt;&lt;div&gt;  This was another major problem we had stumbled upon. It took me a while to   skim through various docs and codes to find the optimum fix for this. &lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;  Our initial insight was that the difference between the Byte-By-Byte   description and the data part of the written table, when the   &lt;span style="font-family: courier;"&gt;formats&lt;/span&gt; argument is passed to the   &lt;span style="font-family: courier;"&gt;write&lt;/span&gt; function, related in some   manner to the string formatting part of the code. By first look itself, it was   evident that there isn't any provision in the writer for cases when the   columns already contain a   &lt;span style="font-family: courier;"&gt;format&lt;/span&gt; attribute, which is what is   assigned when &lt;span style="font-family: courier;"&gt;formats&lt;/span&gt; is passed, as   I had written here back then. Creating allowance for this was easy enough,   right away correcting the test outputs. Now, both the Byte-By-Byte and the   table data had the number of decimal digits, or whatever other format for that   matter, we wanted them to have. Apart from the internally created coordinate   component columns, for which the number of digits after decimal was fixed. &lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;  It is when we want to go a step further than this and wanna truncate or   eradicate the string formatting part to obtain the column format, that we   stumble upon a road block. There are two concerns, &lt;/div&gt;&lt;div&gt;  &lt;ul style="text-align: left;"&gt;    &lt;li&gt;      If no &lt;span style="font-family: courier;"&gt;formats&lt;/span&gt; argument is       passed, &lt;span style="font-family: courier;"&gt;col.format&lt;/span&gt; will be set       to &lt;span style="font-family: courier;"&gt;None&lt;/span&gt;.     &lt;/li&gt;    &lt;li&gt;      Even if we already know the column format, say       &lt;span style="font-family: courier;"&gt;.5f&lt;/span&gt;, we still need to evaluate       the maximum size of the value strings of the column in most cases, and do       some formatting to have the format in CDS/MRT recommendation,       &lt;span style="font-family: courier;"&gt;Fx.5&lt;/span&gt;.     &lt;/li&gt;  &lt;/ul&gt;  The column &lt;span style="font-family: courier;"&gt;formats&lt;/span&gt; passed in the   formats argument are set by using the in-build Python function   &lt;span style="font-family: courier;"&gt;format&lt;/span&gt; (&lt;a href="https://docs.python.org/3/library/functions.html#format"&gt;https://docs.python.org/3/library/functions.html#format&lt;/a&gt;). For cases when no formats argument is passed, the default behavior when   writing the table data, for instance in the   &lt;span style="font-family: courier;"&gt;FixedWidth&lt;/span&gt; writer is to set the   column format to &lt;span style="font-family: courier;"&gt;''&lt;/span&gt; which is   equivalent to saying   &lt;span style="font-family: courier;"&gt;val = str(val)&lt;/span&gt;. (&lt;a href="https://docs.astropy.org/en/stable/table/construct_table.html#table-format-string"&gt;https://docs.astropy.org/en/stable/table/construct_table.html#table-format-string&lt;/a&gt;) &lt;span style="font-family: courier;"&gt;FixedWidth&lt;/span&gt; uses the maximum   length of these strings to get the column widths.   &lt;b&gt;So, there the string formatting part of the code is essential if we want to     know the correct format for columns without string values.&lt;/b&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;  However, there may be another solution to this that can be tried in the   long-term. I was curious to know what other writers in Astropy did in such   situations when the column format needs to be given explicitly in the header   of the written table. There aren't extravagantly many such use cases, but the   FITS standard tables do have format keywords in the header as serve the   purpose well. So, looking over the Astropy FITS writer, I found the way in   which it deals with the problem of assigning column formats is by separately   defining all the formats that can be and then using a custom   &lt;span style="font-family: courier;"&gt;Column&lt;/span&gt; class which has some default   format attributes. (See:&lt;br&gt;&lt;a href="https://github.com/astropy/astropy/blob/main/astropy/io/fits/column.py"&gt;https://github.com/astropy/astropy/blob/main/astropy/io/fits/column.py&lt;/a&gt;). ASCII writers also have a custom   &lt;span style="font-family: courier;"&gt;Column&lt;/span&gt; class, but the attributes   that it currently has are exceedingly lacking to be of any use to us now. (&lt;a href="https://github.com/astropy/astropy/blob/79323de928e87827526ed8fce04986a5dd459794/astropy/io/ascii/core.py#L270"&gt;https://github.com/astropy/astropy/blob/79323de928e87827526ed8fce04986a5dd459794/astropy/io/ascii/core.py#L270&lt;/a&gt;) In the long-run, we could take motivation from the FITS writer and make   changes herein.&lt;br&gt;&lt;br&gt;&lt;i&gt;Other updates&lt;/i&gt;&lt;/div&gt;&lt;div&gt;  &lt;i&gt;&lt;br&gt;&lt;/i&gt;  &lt;div&gt;    &lt;div&gt;      I have began to work on the other two branches for Time cols and MRT       metadata resp and would have them done in some time.     &lt;/div&gt;    &lt;div&gt;      On an unrelated note, I found that the       &lt;span style="font-family: courier;"&gt;test_cds_header_from_readme.py&lt;/span&gt;      test file in       &lt;span style="font-family: courier;"&gt;&lt;a href="http://astropy.io/"&gt;astropy.io&lt;/a&gt;.ascii.tests&lt;/span&gt;      contains some CDS reading tests. It was recently modified by the 11593 PR       (&lt;a href="https://github.com/astropy/astropy/pull/11593/files"&gt;https://github.com/astropy/astropy/pull/11593/files&lt;/a&gt;). I imagine that these tests can be incorporated within test_cds.py and       then we won't perhaps have to move CDS/MRT tests to any other test file?     &lt;/div&gt;  &lt;/div&gt;&lt;/div&gt;
&lt;!-- TEASER_END --&gt;&lt;/div&gt;</description><category>Astropy</category><guid>http://openastronomy.org/Universe_OA/posts/2021/08/20210809_0848_suyog7130/</guid><pubDate>Mon, 09 Aug 2021 07:48:00 GMT</pubDate></item><item><title>GSoC update!</title><link>http://openastronomy.org/Universe_OA/posts/2021/08/20210803_0200_rashmiraj137/</link><dc:creator>Raj Rashmi</dc:creator><description>&lt;div&gt;&lt;p&gt;GSoC started four months ago and it is not just about knowing more about the open-source that made the experience great! My mentors made it way cooler than I thought it would be. I was writing my Master thesis, for the last three months and surely, it has been a super productive summer for me! The best part is I get to do things at my own pace. My project particularly hasn‚Äôt been very easy to implement. I need to bridge a Machine Learning algorithm in the existing codebase. The fun part is venturing with different notebooks and figuring out with intuition, what could be efficient in terms of computational time, efficiency, cost etc. But as of now, the struggle has been to define the problem as exactly to achieve the result. But I will keep working on finding a solution with my mentor Daniela, and trust that struggle will bring some positive construction in Stingray.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/0*2GcUC2cgvKapk8Lj"&gt;&lt;/figure&gt;&lt;p&gt;The current data fit for the evaluation of likelihood happens using scipy.optimize.minimize function. However, there exists numerous ways to do this. SciPy optimize provides functions for minimizing (or maximizing) objective functions, possibly subject to constraints. It includes solvers for nonlinear problems (with support for both local and global optimization algorithms), linear programming, constrained and nonlinear least-squares, root finding, and curve fitting. The problem with the current minimization algorithm is that it converges at local minimum instead of global, i.e. it is not very robust. Recently, Machine Learning has evident development in such optimization tools. The strategy for ahead is that I will work on finding alternatives that potentially accelerate the code, makes it¬†robust.&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=2d16a70cc267" width="1"&gt;
&lt;!-- TEASER_END --&gt;&lt;/div&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2021/08/20210803_0200_rashmiraj137/</guid><pubDate>Tue, 03 Aug 2021 01:00:58 GMT</pubDate></item><item><title>GSoC - 3</title><link>http://openastronomy.org/Universe_OA/posts/2021/08/20210802_0300_gagan-aryan/</link><dc:creator>Gagan Aryan</dc:creator><description>&lt;div&gt;&lt;p&gt;Hello and welcome to the first blog of GSoC phase-2. Ever faced a time when there was way too much on the plate and you find it really hard to catch up on all the work? That is pretty much how the previous two weeks were for me. With the start of the oncampus internship drive, I was finding it really hard to give manage the project. Somehow I was able to make some progress but I am yet to complete the task.&lt;/p&gt;
&lt;p&gt;I am basically trying to parse the &lt;code&gt;.bz2&lt;/code&gt; files of &lt;code&gt;HITEMP&lt;/code&gt; databases into HDF5 files in a Vaex friendly format. Currently &lt;code&gt;.bz2&lt;/code&gt; files are parsed into HDF5 files with the help of high level pandas functions. But as we already know pandas can be very memory consuming. So, I am trying to write to HDF5 files with &lt;code&gt;h5py&lt;/code&gt; library and produce HDF5 files that are vaex friendly (column based).&lt;/p&gt;
&lt;p&gt;In order to do this, I am first converting &lt;code&gt;bz2&lt;/code&gt; files to &lt;code&gt;.csv&lt;/code&gt; upon download -&amp;gt; mapping the datatypes of each of the columns -&amp;gt; writing to a HDF5 file with &lt;code&gt;h5py&lt;/code&gt;. I am currently stuck at mapping the datatypes and also trying to make optimizations with respect to the chunksize.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;This blog is just a quick update on the things that are happening currently. Once the writing to a HDF5 files is completed, look out for a detailed tutorial on the same on towardsdatascience :)&lt;/p&gt;&lt;/div&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2021/08/20210802_0300_gagan-aryan/</guid><pubDate>Mon, 02 Aug 2021 02:00:06 GMT</pubDate></item><item><title>About my Google Summer of Code Project: Part 3</title><link>http://openastronomy.org/Universe_OA/posts/2021/08/20210801_1853_adwaitbhope/</link><dc:creator>Adwait Bhope</dc:creator><description>&lt;div&gt;&lt;p&gt;First and foremost, I celebrate the merging of the PR that brings reproject to NDCube! It defines a base-level functionality or MVP if you want to call it that, along with some relevant documentation. We also mark the release of ndcube‚Äôs 2.0 RC1. This is an important milestone since ndcube 2.0 brings significant changes, owing to the implementation of the new high-level WCS¬†API.&lt;/p&gt;
&lt;p&gt;Our next plan of action was to extend the method to use other algorithms that reproject supports. Interpolation (the one that the above PR implements) supports multi-dimensional cubes but ‚Äúadaptive‚Äù and ‚Äúexact‚Äù algorithms do not. For the time being, they only work on 2D cubes containing celestial axes. So that‚Äôs what I‚Äôve implemented them for in a new PR, which is currently under review and should hopefully get merged¬†soon.&lt;/p&gt;
&lt;p&gt;The only problem for this PR was identifying celestial axes. We‚Äôve taken a shortcut to solve this quickly and avoid creating a blocker, but a better implementation is¬†due.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;The NDCubeSequence PR that I talked about in the last blog post hit a few unexpected edge cases which are under¬†work.&lt;/p&gt;
&lt;p&gt;We‚Äôre nearing the end of GSoC‚Äôs official timeline and while that is saddening, the good thing is that open source doesn‚Äôt need a GSoC timeline for contributing. I do hope that I‚Äôll be able to tie up any loose ends before the end date, but I suppose that does not matter in the community‚Äôs bigger picture. Functional additions, bug fixes, and performance improvements are always going to be coming in for reproject, and I plan to maintain at least that bit of code (or more) in the¬†future.&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=f6354389b27f" width="1"&gt;&lt;/div&gt;</description><category>SunPy</category><guid>http://openastronomy.org/Universe_OA/posts/2021/08/20210801_1853_adwaitbhope/</guid><pubDate>Sun, 01 Aug 2021 17:53:41 GMT</pubDate></item><item><title>Chapter 4: The Other Side</title><link>http://openastronomy.org/Universe_OA/posts/2021/08/20210801_1424_anandxkumar/</link><dc:creator>anandxkumar</dc:creator><description>&lt;div&gt;&lt;p&gt;A new month has started and I have started to see the light at the end of the tunnel. Good Morning and welcome back. Phase 2 has been rolling and let us look at the new findings.&lt;/p&gt;
&lt;p&gt;Earlier the complexity of Legacy method was determined. The complexity of LDM Voigt and LDM FFT was to be determined using similar approach. Upon executing several benchmarks based on Number of lines, Spectum range, wstep, broadening max width. Previously it was thought the complexity was: &lt;br&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;time(LDM_fft) ~ c2*Nlines + c3*(N_G*N_L + 1)*N_v*log(N_v) (where N_v =  Spectral Points)
&lt;!-- TEASER_END --&gt;
time(LDM_voigt) ~ c2*Nlines + c3'*(N_G*N_L + 1)*N_truncation*log(N_truncation) (where N_truncation = broadening width / wstep)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;But in actual Re running all benchmarks for &lt;strong&gt;LDM&amp;gt;Voigt&lt;/strong&gt; and &lt;strong&gt;LDM&amp;gt;FFT&lt;/strong&gt; with a &lt;code class="language-text"&gt;broadening max width = 300 cm-1&lt;/code&gt;. All benchmarks and visualizations can be found &lt;a href="https://anandxkumar.github.io/Benchmark_Visualization_GSoC_2021/"&gt;here&lt;/a&gt; we were able to conclude the followings:&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;FFT:&lt;/strong&gt;&lt;br&gt;
‚Ä¢  Complexity doesn‚Äôt depend on Nlines but rather wL x wG ; check this benchmark: &lt;a href="https://public.tableau.com/app/profile/anand.kumar4841/viz/LDMLinesvsCalculationTimeUpdatedCO2/Sheet1"&gt;link&lt;/a&gt;, it certainly looks like Complexity ‚àù Nlines but its actually dependent on wL and wG, and gives same result on (wL x wG+ 1) x Spectral&lt;em&gt;Points x Log(Spectral&lt;/em&gt;Points).&lt;br&gt;
‚Ä¢  Upon implementing multiple linear regression for &lt;strong&gt;c1 x Nlines + c2 x (wL x wG+ 1)*Spectral&lt;em&gt;Points x Log(Spectral&lt;/em&gt;Points)&lt;/strong&gt; gives &lt;code class="language-text"&gt;c1=2.65e-07&lt;/code&gt;, &lt;code class="language-text"&gt;c2=4.48256e-08&lt;/code&gt; but their &lt;code class="language-text"&gt;p value = 0.648 and 0.00001&lt;/code&gt;, and &lt;code class="language-text"&gt;p&amp;gt;0.05&lt;/code&gt; are insignificant, thus Nlines is insignificant for determining the complexity.&lt;br&gt;
‚Ä¢  Since FFT is independent of broadening max width; benchmark: &lt;a href="https://public.tableau.com/app/profile/anand.kumar4841/viz/LDMVoigtandFFTBMW_NEW/Sheet1"&gt;link&lt;/a&gt;, so on comparing it Spectral point gives us same same time. Thus Spectral Point =  (wavenum max - wavenum max)/wstep instead of (wavenum maxcalc - wavenum min calc)/wstep&lt;br&gt;
‚Ä¢  &lt;strong&gt;Overall complexity =  4.48256897e-08 x (wL x wG+ 1) x Spectral&lt;em&gt;Points(without BMW) x Log(Spectral&lt;/em&gt;Points(without BMW))&lt;/strong&gt;  &lt;a href="https://anandxkumar.github.io/Benchmark_Visualization_GSoC_2021/LDM/Complexity_FFT_Final/Complexity_FFT_Final.html"&gt;link&lt;/a&gt; (with the help of multple linear regression using sklearn; is almost accurate)&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Voigt:&lt;/strong&gt;&lt;br&gt;
‚Ä¢  Similar to 1st point of FFT.&lt;br&gt;
‚Ä¢  Upon doing multiple linear regression for &lt;strong&gt;c1 x N lines + c2 x (wL x wG + 1) x SpectralPoints x BMW xLog(SpectralPoints x (BMW) )&lt;/strong&gt; gives &lt;code class="language-text"&gt;c1=-1.9392e-06, c2=1.28256e-09&lt;/code&gt; but their &lt;code class="language-text"&gt;p value = 0.848 and 0.00001&lt;/code&gt;, and &lt;code class="language-text"&gt;p&amp;gt;0.05&lt;/code&gt; are insignificant, thus N&lt;em&gt;lines is insignificant for determining the complexity.&lt;br&gt;
‚Ä¢  Calculation time is dependent `Broadening&lt;/em&gt;Max&lt;em&gt;width`, but upon inspections with Spectral Points, we have the exact same plot. So complexity is dependent only on Spectral Points but with broadening&lt;/em&gt;max&lt;em&gt;width i.e. wavenum&lt;/em&gt;calc, which causes the increase in computational time on increasing broadening&lt;em&gt;max&lt;/em&gt;width.&lt;br&gt;
‚Ä¢  &lt;strong&gt;Overall complexity = 5.26795e-07 * (wL x wG+ 1)*Spectral Points x Log(Spectral Points)&lt;/strong&gt; &lt;a href="https://anandxkumar.github.io/Benchmark_Visualization_GSoC_2021/LDM/Complexity_Voigt_Final/Complexity_Voigt_Final.html"&gt;link&lt;/a&gt; (with the help of multple linear regression using sklearn; almost straight)&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Also:&lt;/strong&gt; From all the above plots, it really clear if going with broadening&lt;em&gt;max&lt;/em&gt;width=300cm-1 in wavespace, it will take alot more time than fft in all aspects.&lt;/p&gt;
&lt;p&gt;But upon replacing &lt;code class="language-text"&gt;np.convolve&lt;/code&gt; with &lt;code class="language-text"&gt;scipy.signal.oaconvolve&lt;/code&gt;, we were able to achieve &lt;code class="language-text"&gt;2 to 30&lt;/code&gt; times performance boost. So it will be interesting to re run benchmarks with the latest piece of code and see which method performs better. Also some benchmarks will be added to ASV benchmark too to see how its performance changes over time.&lt;/p&gt;
&lt;p&gt;Also profiler was modified to a tree like a stucture using &lt;code class="language-text"&gt;OrderedDict&lt;/code&gt; and &lt;code class="language-text"&gt;YAML&lt;/code&gt; has been used to print the profiler in a proper structued way using &lt;strong&gt;Spectrum.print_perf_profiler()&lt;/strong&gt; or &lt;strong&gt;SpectrumFactory.print_perf_profiler()&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;s = calc_spectrum(1900, 2300,         # cm-1
molecule='CO',
isotope='1,2,3',
pressure=1.01325,   # bar
Tvib=1000,          # K
Trot=300,           # K
mole_fraction=0.1,
verbose=3,
)
s.print_perf_profile()&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Gives the following output:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;&amp;gt;&amp;gt;&amp;gt; spectrum_calculation:
&amp;gt;&amp;gt;&amp;gt;   applied_linestrength_cutoff: 0.0024361610412597656
&amp;gt;&amp;gt;&amp;gt;   calc_emission_integral: 0.006468772888183594
&amp;gt;&amp;gt;&amp;gt;   calc_hwhm: 0.006415128707885742
&amp;gt;&amp;gt;&amp;gt;   calc_line_broadening:
&amp;gt;&amp;gt;&amp;gt;     DLM_Distribute_lines: 0.0003898143768310547
&amp;gt;&amp;gt;&amp;gt;     DLM_Initialized_vectors: 9.775161743164062e-06
&amp;gt;&amp;gt;&amp;gt;     DLM_closest_matching_line: 0.0005028247833251953
&amp;gt;&amp;gt;&amp;gt;     DLM_convolve: 0.029767990112304688
&amp;gt;&amp;gt;&amp;gt;     precompute_DLM_lineshapes: 0.013132810592651367
&amp;gt;&amp;gt;&amp;gt;     value: 0.07619166374206543
&amp;gt;&amp;gt;&amp;gt;   calc_lineshift: 0.00074005126953125
&amp;gt;&amp;gt;&amp;gt;   calc_noneq_population:
&amp;gt;&amp;gt;&amp;gt;     part_function: 0.03405046463012695
&amp;gt;&amp;gt;&amp;gt;     population: 0.005669832229614258
&amp;gt;&amp;gt;&amp;gt;     value: 0.03983640670776367
&amp;gt;&amp;gt;&amp;gt;   calc_other_spectral_quan: 0.002928495407104492
&amp;gt;&amp;gt;&amp;gt;   calc_weight_trans: 0.008247852325439453
&amp;gt;&amp;gt;&amp;gt;   check_line_databank: 0.0002810955047607422
&amp;gt;&amp;gt;&amp;gt;   check_non_eq_param: 0.04109525680541992
&amp;gt;&amp;gt;&amp;gt;   fetch_energy_5: 0.014983654022216797
&amp;gt;&amp;gt;&amp;gt;   generate_spectrum_obj: 0.00032138824462890625
&amp;gt;&amp;gt;&amp;gt;   generate_wavenumber_arrays: 0.0010433197021484375
&amp;gt;&amp;gt;&amp;gt;   reinitialize:
&amp;gt;&amp;gt;&amp;gt;     copy_database: 2.1457672119140625e-06
&amp;gt;&amp;gt;&amp;gt;     memory_usage_warning: 0.0018389225006103516
&amp;gt;&amp;gt;&amp;gt;     reset_population: 2.6226043701171875e-05
&amp;gt;&amp;gt;&amp;gt;     value: 0.001964569091796875
&amp;gt;&amp;gt;&amp;gt;   scaled_non_eq_linestrength:
&amp;gt;&amp;gt;&amp;gt;     corrected_population_se: 0.002747774124145508
&amp;gt;&amp;gt;&amp;gt;     map_part_func: 0.0010590553283691406
&amp;gt;&amp;gt;&amp;gt;     value: 0.0038983821868896484
&amp;gt;&amp;gt;&amp;gt;   value: 0.1904621124267578&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So at the end a productive week! Looking forward to conclude GSoC with a worthy ending :)&lt;/p&gt;&lt;/div&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2021/08/20210801_1424_anandxkumar/</guid><pubDate>Sun, 01 Aug 2021 13:24:32 GMT</pubDate></item><item><title>GSoC Post 3</title><link>http://openastronomy.org/Universe_OA/posts/2021/08/20210801_1244_ndanzanello/</link><dc:creator>ndanzanello</dc:creator><description>&lt;div&gt;&lt;p&gt;Hi! In my last post I mentioned that we would start calculating the distortions contained in the image. But we followed a different path! As the linear part was ready, we first worked on making some plots (scatter plots with side histograms of the difference in pixel scale of the celestial coordinates measured with the WCS we find and the celestial coordinates given as input) and drawing some quads to visualize it. This part was done using LaTeX and TikZ, a wonderful tool to produce graphics!&lt;/p&gt;


&lt;!-- TEASER_END --&gt;

&lt;div class="wp-block-image"&gt;&lt;figure class="aligncenter size-large"&gt;&lt;img alt="" class="wp-image-73" src="https://ndanzanello.files.wordpress.com/2021/08/image.png?w=342"&gt;&lt;figcaption&gt;Example of a quad drawn using TikZ. The black points are the stars of the catalog.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;



&lt;p&gt;After that, we started to evaluate our results, and some changes were made: in statistics, for example, instead of getting direct the median, we use sigma clipping (a technique that removes outliers), allowing a better result. We also compared our results with one well established software: Astrometry.net. We‚Äôre getting pretty good results, but our running time was way bigger than the Astrometry.net one. So, we started working on that, and we have some ways to decrease our running time, such as making an only geo-hash search on the kdtree before the search containing the magnitude hashes. This reduces the dimentionality, which degrades the performance the higher it is. Other solution is to divide the celestial catalog in tiles, decreasing the number of total quads that we have to evaluate. Also, we can reduce the number of stars that we use to make quads. With these approaches, our running time got way better! &lt;img alt="üôÇ" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" style="height: 1em;"&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;/div&gt;</description><category>gnuastro</category><guid>http://openastronomy.org/Universe_OA/posts/2021/08/20210801_1244_ndanzanello/</guid><pubDate>Sun, 01 Aug 2021 11:44:53 GMT</pubDate></item><item><title>Balance</title><link>http://openastronomy.org/Universe_OA/posts/2021/07/20210730_1925_jeffreypaul15/</link><dc:creator>Jeffrey Paul</dc:creator><description>&lt;div&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/400/1*6vTTZ8HC33kEpYk0Wr8gxg.jpeg"&gt;&lt;figcaption&gt;‚ÄúCosmic Balance‚Äù‚Ää‚Äî‚Ääcompletely unrelated type of balance to what I go on to talk to talk about in this post. Although, outer space looks all fancy and it goes well with the theme of OpenAstronomy.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;From all my previous posts, it is quite evident that I obsess over small and insignificant details, mainly directed towards how my life is going to be in the future and this entire concept of ‚Äúhappiness‚Äù. However, the past few months of working with Sunpy has brought about this odd sense of calm. The entire feeling of stress being an emotion goes out the window. Maybe this is what doing something you love, maybe it‚Äôs the people‚Ää‚Äî‚Ääor maybe it‚Äôs just extremely good timing combined with coincidence.&lt;/p&gt;
&lt;p&gt;Ah, I suppose these questions don‚Äôt have simple answers. Regardless, whatever I‚Äôm doing with right now has restored that balance that I was longing¬†for.&lt;/p&gt;
&lt;p&gt;Coming to what has been happening with sunkit-pyvista. To summarize, I spent an entire week fixing things that I caused due to over-confidence¬†:)&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;Aside from that, I think I‚Äôm nearing the end of this project. A few bits of functionality has to be added in but for the most part, I think it‚Äôs all in¬†there.&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Saving and loading entire scenes are now possible with the work that we‚Äôve done. This took quite a bit of time and it was pretty interesting to see how we could extend functionality to such massive¬†extents.&lt;/li&gt;&lt;li&gt;We uncovered a few hidden issues that may have occurred because of me. Ones such as ill-defined tests because I may have been slightly over-confident with how I write¬†code.&lt;/li&gt;&lt;li&gt;Quite a bit of time was spent resolving these issues, but I can definitely conclude that it was well worth the effort and I certainly learnt my¬†lesson.&lt;/li&gt;&lt;li&gt;Figure tests are now a thing, we drew some parallels with Pyvista‚Äôs code and structured our own figure testing methodology which makes it easier for us to visually identify any mishaps in our plots. After all, we‚Äôre creating a library for data visualization. It‚Äôd be sad if our code tests pass and we‚Äôre under the assumption that everything is working fine (yet another dig at myself for not writing efficient tests).&lt;/li&gt;&lt;li&gt;I can safely say that Sunkit-Pyvista is quite balanced and usable now, or at least I hope¬†so.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Getting back to how this might be the last few PRs of this project under the whole ‚ÄúGSoC timeline‚Äù, this makes me both sad and happy. It‚Äôs saddening to see that something I worked towards for over 6 months has kind of come to an end. Happy because I‚Äôve gotten to work with some of the best developers and I genuinely enjoyed every bit of it. This offsets the balance in my life, but I think we may have a solution to¬†this?&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=239840d26318" width="1"&gt;&lt;/div&gt;</description><category>SunPy</category><guid>http://openastronomy.org/Universe_OA/posts/2021/07/20210730_1925_jeffreypaul15/</guid><pubDate>Fri, 30 Jul 2021 18:25:35 GMT</pubDate></item><item><title>astropy@GSoC Blog Post #5, Week 6&amp;7</title><link>http://openastronomy.org/Universe_OA/posts/2021/07/20210721_2000_suyog7130/</link><dc:creator>Suyog Garg</dc:creator><description>&lt;div&gt;&lt;p&gt;Hi,&lt;/p&gt;
&lt;p&gt;How are you?&lt;/p&gt;
&lt;p&gt;My dear mentors and I have decided to have the MRT (Machine Readable Table) format writing first. The same CDS code as been used now will be used, just the template of the written table will be in the MRT format.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;Points to be noted regarding this and the immediate things that have been and will done are as follows:&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;ul style="text-align: left;"&gt;&lt;li&gt;Leave out writing all the optional CDS ReadMe fields as of now. These can be dealt with individual PRs later.&lt;/li&gt;&lt;li&gt;Some tests fail because &lt;span style="font-family: courier;"&gt;start_line = None&lt;/span&gt; doesn't work. It has been introduced once again within &lt;span style="font-family: courier;"&gt;CdsData.write&lt;/span&gt; function in addition to been defined in the main &lt;span style="font-family: courier;"&gt;Cds&lt;/span&gt; class. The test failure occurs because CdsData now inherits from &lt;span style="font-family: courier;"&gt;FixedWidthData&lt;/span&gt; which itself inherits &lt;span style="font-family: courier;"&gt;basic.BasicReader&lt;/span&gt; instead of BaseReader. I should make sure that all tests pass properly.&lt;/li&gt;&lt;li&gt;Have a template for MRT tables and write them first. &lt;b&gt;Title&lt;/b&gt;, &lt;b&gt;Authors&lt;/b&gt;, &lt;b&gt;Date&lt;/b&gt;, &lt;b&gt;Caption&lt;/b&gt; and &lt;b&gt;Notes&lt;/b&gt; sections, i.e. all sections except the Byte-By-Byte and the Data itself, will be left blank in the template, with warning for the user to put them in manually afterwards.&lt;/li&gt;&lt;li&gt;Documentation for the CDS/MRT format writer.&lt;/li&gt;&lt;li&gt;At present issue a warning note for tables with two or more mix-in columns (&lt;span style="font-family: courier;"&gt;SkyCoord&lt;/span&gt; cols primarily). If ways to correctly work out such situations is thought of, add that feature in a separate PR.&lt;/li&gt;&lt;li&gt;Work with a copy of the original table, so that¬† the copy is modified and not the original table, when component coordinate columns are written. The modified copy of the table is written to a file, while the user retains access to the columns of the original table.&lt;/li&gt;&lt;li&gt;Need to have features to recognise non Spherical coordinates, like the Cartesian coordinates, and either skip them or write them as Single column string values. Add test for such other coordinates. Also for cases when coordinates are in a &lt;span style="font-family: courier;"&gt;SkyCoord&lt;/span&gt; object but the frame is not Spherical.&lt;/li&gt;&lt;li&gt;Have two other templates, one for CDS in which the user fills values of optional fields manually later and another in which filling optional fields can be done from within Astropy, via a &lt;span style="font-family: courier;"&gt;cdsdict&lt;/span&gt;. In separate PRs. Here too write only the required fields in the ReadMe first, like &lt;b&gt;Abstract&lt;/b&gt;.&lt;/li&gt;&lt;li&gt;Have features for Time columns later within the original PR or much later.&lt;/li&gt;&lt;li&gt;Simplify how column format is obtained for float columns. The current manner of string formatting is too complicated. &lt;span style="font-family: courier;"&gt;col.width&lt;/span&gt; value can be directly used in some cases. The &lt;span style="font-family: courier;"&gt;Outputter&lt;/span&gt; class will also know the column format since it writes out the table.&lt;/li&gt;&lt;li&gt;Other minor/major edits and modifications as suggested by others.&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;With this PR for the MRT format table writing getting eventually merged to Astropy, the main goal of my astropy@GSoC project will be completed. The support for other extra features essentially serves as appendages to the primary task been done by this PR.&lt;/div&gt;&lt;div&gt;Let's see how it goes.&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Oh! On another note, a few days back I received the GSoC First Evaluations payment! üòÅ&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Adious!&lt;/div&gt;&lt;p&gt;&lt;/p&gt;&lt;/div&gt;</description><category>Astropy</category><guid>http://openastronomy.org/Universe_OA/posts/2021/07/20210721_2000_suyog7130/</guid><pubDate>Wed, 21 Jul 2021 19:00:00 GMT</pubDate></item></channel></rss>