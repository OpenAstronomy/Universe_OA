<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Universe OpenAstronomy</title><link>http://openastronomy.org/Universe_OA/</link><description>This is an aggregator of openastronomy people</description><atom:link href="http://openastronomy.org/Universe_OA/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Fri, 05 Jul 2024 01:02:16 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Filling the Temporal Gaps in AGN Light Curve Data</title><link>http://openastronomy.org/Universe_OA/posts/2024/06/20240630_2331_lucasmg18/</link><dc:creator>Lucas Martin Garcia</dc:creator><description>&lt;p&gt; &lt;strong&gt;Introduction to the Challenge&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In our ongoing quest to understand Active Galactic Nuclei (AGNs), handling the discontinuous nature of AGN light curve data remains the main goal. The gaps in observation data, caused by unavoidable operational and environmental constraints, obscure the complete picture of these AGN data. To address several methods are taken into account to approach the temporal data interpolation, combining traditional techniques with advanced machine learning models.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Traditional Interpolation Techniques&lt;/strong&gt;&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;The basic Interpolation Methods include:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Linear Interpolation:&lt;/strong&gt; Useful for filling short gaps where changes between points are expected to be gradual and linear.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Polynomial Interpolation:&lt;/strong&gt; Offers a more flexible approach for non-linear data, providing smoother estimates that can better reflect inherent variabilities in AGN light emissions.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;These techniques are fast and effective for smaller, simpler gaps but often fall short when dealing with larger or more complex interruptions in data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Advanced Machine Learning Techniques&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For more substantial gaps or when high fidelity to complex light curve dynamics is crucial, some machine learning algorithms are:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Recurrent Neural Networks (RNNs):&lt;/strong&gt; These are particularly adept at modeling time-series data, capturing dependencies across time steps to predict missing observations with a high degree of accuracy.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Generative Adversarial Networks (GANs):&lt;/strong&gt; By training GANs on existing data, we can generate new data points that not only fill larger gaps but also maintain statistical consistency with observed data.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Moving Forward&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The integration of these methods has already shown promising results in other fields and applications. As we refine these techniques, we aim not only to improve the quality of data but also to deepen our understanding of the underlying physical processes of AGNs.&lt;/p&gt;
&lt;p&gt;Our journey into the light curves of AGNs is as much about improving our observational tools and techniques as it is about exploring the universe's mysteries. By bridging these data gaps, we hope to bring clarity to the complexities of galaxy evolution and contribute to the broader astronomical community.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The challenge of incomplete data is not unique to astronomy but is a common issue in various scientific domains. Our interdisciplinary approach has obtained already good results in other fields where data integrity impacts the quality of research outcomes.&lt;/p&gt;</description><category>irsa-fornax</category><guid>http://openastronomy.org/Universe_OA/posts/2024/06/20240630_2331_lucasmg18/</guid><pubDate>Sun, 30 Jun 2024 22:31:00 GMT</pubDate></item><item><title>Partition functions</title><link>http://openastronomy.org/Universe_OA/posts/2024/06/20240630_0000_code29563/</link><dc:creator>code29563</dc:creator><description>&lt;p&gt;Support has been added for the species-specific partition functions that are usually included along with the new linelists. In doing so, a new config parameter was added to allow the user to enable RADIS to automatically modify the local database files and update the config file accordingly.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2024/06/20240630_0000_code29563/</guid><pubDate>Sat, 29 Jun 2024 23:00:00 GMT</pubDate></item><item><title>Midterm evaluation are coming near</title><link>http://openastronomy.org/Universe_OA/posts/2024/06/20240628_1734_kartikmandar/</link><dc:creator>Kartik Mandar</dc:creator><description>&lt;p&gt; MidTerm evaluations are near the corner, I guess I am moving well with my project. I am hoping to do certain functionality before Monday and present that to my mentor. I still have one week more after that before the MidTerm evaluations. Let's see how it all goes!!&lt;/p&gt;
&lt;!-- TEASER_END --&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2024/06/20240628_1734_kartikmandar/</guid><pubDate>Fri, 28 Jun 2024 16:34:00 GMT</pubDate></item><item><title>Final layout decided, and a demo</title><link>http://openastronomy.org/Universe_OA/posts/2024/06/20240625_1759_kartikmandar/</link><dc:creator>Kartik Mandar</dc:creator><description>&lt;p&gt; So, the final layout was decided and I discussed with my mentors about the same. They liked it and suggested some changes. &lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhMa67MJ6HJ8F_bfsWuzC7Ro3aL6A-3PB6XfftMKYqVEAxhp6Y8JySOtBydtfw9xJf9lwOuDTPq4uZ0XcdnjIGug776Wuk312LYfB-hEC65hOom8w1zjJwzRNI6W9M-olE2hgKOHexVIEotXea-u7l9g_uuJuRJR1rVQ5_Sp49cqtpOyMP2t7_L32JSFA8p/s3840/B8E1879E-24A0-43AF-A58E-21FCB8A6474F.JPG" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" height="298" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhMa67MJ6HJ8F_bfsWuzC7Ro3aL6A-3PB6XfftMKYqVEAxhp6Y8JySOtBydtfw9xJf9lwOuDTPq4uZ0XcdnjIGug776Wuk312LYfB-hEC65hOom8w1zjJwzRNI6W9M-olE2hgKOHexVIEotXea-u7l9g_uuJuRJR1rVQ5_Sp49cqtpOyMP2t7_L32JSFA8p/w530-h298/B8E1879E-24A0-43AF-A58E-21FCB8A6474F.JPG" width="530"&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;All these loading tabs should be made into one. And the structure should be more coherent. &lt;br&gt;&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;!-- TEASER_END --&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2024/06/20240625_1759_kartikmandar/</guid><pubDate>Tue, 25 Jun 2024 16:59:00 GMT</pubDate></item><item><title>Tackling the Challenges of Active Galactic Nuclei Data with Machine Learning Models</title><link>http://openastronomy.org/Universe_OA/posts/2024/06/20240623_2331_lucasmg18/</link><dc:creator>Lucas Martin Garcia</dc:creator><description>&lt;p&gt; &lt;strong&gt;Understanding the Complexity of AGN Light Curve Data&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Active Galactic Nuclei (AGNs) are among the most luminous and dynamic objects in the universe, characterized by their variable light emissions that provide key insights into the mechanics of galaxy evolution. A fundamental challenge in studying AGNs is the nature of the data collected where the parameters such as time and wavelength are critical. Each observation captures the light curve of an AGN.&lt;/p&gt;
&lt;p&gt;However, this data isn't straightforward. Observations are taken using different instruments, like different stations or satellites, leading to variations in data quality and measurement techniques. More critically, there are inevitable gaps in the data, caused by factors ranging from environmental conditions blocking observations to the simple fact that different tools have different operational time frames and capabilities.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;&lt;strong&gt;The Goal: Enhancing Data Cohesiveness&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The objective of our research project is clear: to enhance the cohesiveness and quality of AGN light curve datasets. This involves not only unifying data across different wavelengths and time periods but also filling in missing data to create a more complete picture of AGN activity. The challenge is non-trivial, as it requires sophisticated approaches to accurately interpolate or simulate missing observations without distorting the underlying physical phenomena.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Advanced Machine Learning Models for Data Enhancement&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To address these challenges, we are exploring several machine learning (ML) models. Deep learning (DL) models, particularly neural networks, are at the forefront of our tools, owing to their ability to model complex patterns and dependencies in large datasets. Recurrent Neural Networks (RNNs) are particularly suited for this task because of their effectiveness in handling sequential data, which is a natural fit for time-series analysis like light curves.&lt;/p&gt;
&lt;p&gt;Moreover, Generative Adversarial Networks (GANs) offer a promising approach to generate new data points synthetically. GANs can be trained to produce data that mimics the statistical properties of existing observations, potentially filling gaps in the light curves with high accuracy. These models learn to simulate new data that could plausibly occur under similar conditions, based on the patterns learned from the data that do exist.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Moving Forward&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Our research is still in the developmental phase, with ongoing efforts to refine the models and enhance their predictive and generative capabilities. By integrating these advanced ML models, we aim to not only improve the data quality of AGN observations but also to provide deeper insights into their dynamic behavior, which remains an enigma in many aspects. This could significantly aid astronomers and astrophysicists in understanding the fundamental processes driving these powerful celestial objects.&lt;/p&gt;
&lt;p&gt;By leveraging the power of machine learning we hope to overcome the significant problems posed by the fragmented and incomplete nature of AGN light curve data. This research not only pushes the boundaries of astronomical data analysis but also contributes to the broader field of applied machine learning in solving real-world problems with high complexity and significant scientific impact.&lt;/p&gt;</description><category>irsa-fornax</category><guid>http://openastronomy.org/Universe_OA/posts/2024/06/20240623_2331_lucasmg18/</guid><pubDate>Sun, 23 Jun 2024 22:31:00 GMT</pubDate></item><item><title>It’s going good.</title><link>http://openastronomy.org/Universe_OA/posts/2024/06/20240623_0821_nucleongodx/</link><dc:creator>Manit Singh</dc:creator><description>&lt;p&gt;It’s going good.&lt;/p&gt;
&lt;p&gt;The past two weeks were dedicated to the implementation of wavelength functionality. Now, wavelength has been successfully implemented for the applicable instruments. Along with that, a how-to guide has been created to assist users in navigating querying over wavelength for different instruments. Additionally, a gallery example for wavelength and detector has been added.&lt;/p&gt;
&lt;h4&gt;What’s the direction of implementation:&lt;/h4&gt;&lt;p&gt;The implementation of wavelength can be divided into two parts:&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Instruments with a “wavelength” column in SOAR&lt;/strong&gt;: These remote sensing instruments are EUI, SOLOHI, and METIS. For these instruments, we query on the basis of wavelength.&lt;/li&gt;&lt;/ol&gt;&lt;pre&gt; instrument = a.Instrument("EUI")&lt;br&gt; time = a.Time("2023-04-03 15:00", "2023-04-03 16:00")&lt;br&gt; level = a.Level(1)&lt;br&gt; wavelength = a.Wavelength(304 * u.AA)&lt;br&gt; res = Fido.search(instrument &amp;amp; time &amp;amp; level &amp;amp; wavelength)&lt;/pre&gt;&lt;p&gt;Result:&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Ty2aSvBet-8KYDf6iE729w.png"&gt;&lt;/figure&gt;&lt;p&gt;A range of wavelength can also be passed, which will be considered wavemin and wavemax for these three instruments.&lt;/p&gt;
&lt;pre&gt;wavelength = a.Wavelength(171 * u.AA, 185 * u.AA)&lt;/pre&gt;&lt;p&gt;Result:&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*oUBJwth5UcL9IegFG5LjaA.png"&gt;&lt;/figure&gt;&lt;p&gt;2. For the instruments PHI and SPICE, we don’t have a “wavelength” column in their instrument table, so we use wavemin and wavemax for querying.&lt;/p&gt;
&lt;p&gt;However, there is a problem with SPICE. Since the range of wavelength is only given for the first spectral window of the data, to ensure the data is not misleading to the user, we do not return any wavelength values.&lt;/p&gt;
&lt;pre&gt; instrument = a.Instrument("PHI")&lt;br&gt; time = a.Time("2023-02-01", "2023-02-02")&lt;br&gt; level = a.Level(2)&lt;br&gt; wavelength = a.Wavelength(6173.065 * u.AA, 6173.501 * u.AA)&lt;br&gt; res = Fido.search(instrument &amp;amp; time &amp;amp; level &amp;amp; wavelength)&lt;/pre&gt;&lt;p&gt;Result:&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*fxkPK8p3KR6xMEv7zIkTXQ.png"&gt;&lt;/figure&gt;&lt;p&gt;Although passing a range of wavelengths is preferred for PHI, if only one value is passed, it will be taken as wavemin, and filtering will be done based on it. Additionally, the corresponding wavemax will be provided in the output table.&lt;/p&gt;
&lt;pre&gt;wavelength = a.Wavelength(6173.065 * u.AA)&lt;/pre&gt;&lt;p&gt;Result:&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*O-pETZd8HZi9qVt9NQGa-w.png"&gt;&lt;/figure&gt;&lt;h4&gt;Challenges that still needs working:&lt;/h4&gt;&lt;p&gt;There is an issue with PHI’s wavelength data as well. The wavelengths returned are sometimes in the order of 6173 and sometimes 617.3, which are essentially just different units of similar wavelength data, but this is not specified in SOAR.&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=160df7122684" width="1"&gt;</description><category>SunPy</category><guid>http://openastronomy.org/Universe_OA/posts/2024/06/20240623_0821_nucleongodx/</guid><pubDate>Sun, 23 Jun 2024 07:21:59 GMT</pubDate></item><item><title>GSoC [Week 02-03] Progress</title><link>http://openastronomy.org/Universe_OA/posts/2024/06/20240623_0719_viciouseagle03/</link><dc:creator>ViciousEagle03</dc:creator><description>&lt;p&gt;This blog post covers all the work done in the second and third week of Google Summer of Code.&lt;/p&gt;
&lt;p&gt;In the past weeks, my focus has been on enhancing the serialization support of NDCube by extending its support for ExtraCoords and GlobalCoords. After establishing serialization support for the fundamental attributes of the NDCube object—such as data and wcs—in the ASDF file format, the next logical step was extending this capability to include &lt;code&gt;ExtraCoords&lt;/code&gt; and &lt;code&gt;GlobalCoords&lt;/code&gt; object.&lt;/p&gt;
&lt;h3 id="globalcoords-and-extracoords"&gt;GlobalCoords and ExtraCoords&lt;/h3&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;ExtraCoords: The ExtraCoords attribute expand NDCube’s capabilities by incorporating additional spatial or spectral coordinates beyond those defined by the primary WCS. It accommodate complex datasets needing extra dimensions or coordinates, such as spectroscopic data with folded axes.&lt;/p&gt;
&lt;p&gt;GlobalCoords: The GlobalCoords attribute are coordinates in an NDCube that provides universal context across the entire dataset and apply uniformly across all axes. It captures metadata such as observation times or global positional coordinates of the NDCube object.&lt;/p&gt;
&lt;h3 id="extending-the-serialization-support"&gt;Extending the Serialization support&lt;/h3&gt;
&lt;p&gt;Now, &lt;code&gt;ExtraCoords&lt;/code&gt; can be initialized by specifying a &lt;code&gt;BaseLowLevelWCS&lt;/code&gt; object and a mapping, or by building it up using one or more lookup tables. To support the latter method, I had to ensure the &lt;code&gt;lookup_tables&lt;/code&gt; were properly preserved during serialization. For this purpose, I designed the schema and wrote the converter class for the following objects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ndcube.extra_coords.extra_coords.ExtraCoords&lt;/li&gt;
&lt;li&gt;ndcube.extra_coords.table_coord.TimeTableCoordinate&lt;/li&gt;
&lt;li&gt;ndcube.extra_coords.table_coord.QuantityTableCoordinate&lt;/li&gt;
&lt;li&gt;ndcube.extra_coords.table_coord.SkyCoordTableCoordinate&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The initialization of &lt;code&gt;GlobalCoords&lt;/code&gt; is more straightforward, which required me to design the schema and write the converter class for the following object:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ndcube.global_coords.GlobalCoords&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Here’s an example of the &lt;code&gt;ExtraCoords&lt;/code&gt; Schema&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre tabindex="0"&gt;&lt;code&gt;%YAML 1.1
---
$schema: "http://stsci.edu/schemas/yaml-schema/draft-01"
id: "asdf://sunpy.org/ndcube/schemas/extra_coords-0.1.0"

title:
Represents the ndcube ExtraCoords object

description:
Represents the ndcube ExtraCoords object

type: object
properties:
wcs:
tag: "tag:stsci.edu:gwcs/wcs-1.*"
mapping:
type: array
lookup_tables:
type: array
items:
type: array
items:
- oneOf:
- type: number
- type: array
- oneOf:
- tag: "tag:sunpy.org:ndcube/extra_coords/table_coord/quantitytablecoordinate-0.*"
- tag: "tag:sunpy.org:ndcube/extra_coords/table_coord/skycoordtablecoordinate-0.*"
- tag: "tag:sunpy.org:ndcube/extra_coords/table_coord/timetablecoordinate-0.*"
dropped_tables:
type: array
ndcube:
tag: "tag:sunpy.org:ndcube/ndcube/ndcube-0.*"

required: [ndcube]
additionalProperties: false
...
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id="whats-new-keeping-up-with-the-coords"&gt;What’s new: Keeping Up with the Coords&lt;/h3&gt;
&lt;p&gt;&lt;img alt="img" src="https://viciouseagle03.github.io/images/ASDF-ser-new-type.png"&gt;&lt;/p&gt;
&lt;p&gt;Now, when we save an NDCube object to an ASDF file, the file successfully stores the &lt;code&gt;ExtraCoords&lt;/code&gt; and &lt;code&gt;GlobalCoords&lt;/code&gt; information, which is preserved when we deserialize and generate a new NDCube object by reading the file. The only thing that is still not supported is initializing an ExtraCoords object using a mapping and an &lt;code&gt;astropy.wcs.WCS&lt;/code&gt; object. This feature will be supported in the future when I implement serialization support for &lt;code&gt;astropy.wcs.WCS&lt;/code&gt; in the asdf-astropy library.&lt;/p&gt;
&lt;h5 id="saving-an-ndcube-with-globalcoords-and-extracoords-attribute"&gt;Saving an NDCube with &lt;code&gt;GlobalCoords&lt;/code&gt; and &lt;code&gt;ExtraCoords&lt;/code&gt; attribute&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;with asdf.open('ndcubeglobal_extra.asdf') as af:
ndcube2 = af.tree['ndcube']
&lt;/code&gt;&lt;/pre&gt;
&lt;h5 id="the-relevant-asdf-file-block"&gt;The relevant ASDF file block&lt;/h5&gt;
&lt;pre&gt;&lt;code&gt;extra_coords: !&amp;lt;tag:sunpy.org:ndcube/extra_coords/extra_coords/extracoords-0.1.0&amp;gt;
dropped_tables: []
lookup_tables:
- - 0
- !&amp;lt;tag:sunpy.org:ndcube/extra_coords/table_coord/timetablecoordinate-0.1.0&amp;gt;
mesh: false
names: [time]
reference_time: !time/time-1.1.0 {base_format: fits, value: '2000-01-01T00:00:00.000'}
table: !time/time-1.1.0
base_format: fits
value: !core/ndarray-1.0.0
byteorder: little
datatype: [ucs4, 23]
shape: [10]
source: 5
- - 0
- !&amp;lt;tag:sunpy.org:ndcube/extra_coords/table_coord/skycoordtablecoordinate-0.1.0&amp;gt;
mesh: false
names: [lon, lat]
table: !&amp;lt;tag:astropy.org:astropy/coordinates/skycoord-1.0.0&amp;gt;
dec: !&amp;lt;tag:astropy.org:astropy/coordinates/latitude-1.0.0&amp;gt;
unit: !unit/unit-1.0.0 deg
value: !core/ndarray-1.0.0
byteorder: little
datatype: float64
shape: [10]
source: 7
frame: icrs
ra: !&amp;lt;tag:astropy.org:astropy/coordinates/longitude-1.0.0&amp;gt;
unit: !unit/unit-1.0.0 deg
value: !core/ndarray-1.0.0
byteorder: little
datatype: float64
shape: [10]
source: 6
wrap_angle: !&amp;lt;tag:astropy.org:astropy/coordinates/angle-1.0.0&amp;gt; {datatype: float64,
unit: !unit/unit-1.0.0 deg, value: 360.0}
representation_type: spherical
- - 1
- !&amp;lt;tag:sunpy.org:ndcube/extra_coords/table_coord/quantitytablecoordinate-0.1.0&amp;gt;
mesh: true
names: [exposure_time]
table:
- !unit/quantity-1.1.0
unit: !unit/unit-1.0.0 s
value: !core/ndarray-1.0.0
byteorder: little
datatype: float64
shape: [10]
source: 8
unit: !unit/unit-1.0.0 s
ndcube: *id001
global_coords: !&amp;lt;tag:sunpy.org:ndcube/global_coords/globalcoords-0.1.0&amp;gt;
internal_coords:
name1:
- custom:physical_type1
- !unit/quantity-1.1.0 {datatype: float64, unit: !unit/unit-1.0.0 m, value: 1.0}
name2:
- custom:physical_type2
- !unit/quantity-1.1.0 {datatype: float64, unit: !unit/unit-1.0.0 s, value: 2.0}
ndcube: *id001
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;During my coding period, my mentors have been incredibly helpful, and huge thanks to &lt;a href="https://github.com/braingram"&gt;@braingram&lt;/a&gt; for explaining and ensuring I understood the entire ASDF serialization process thoroughly. I learned a lot about schema design and JSON schema. I was particularly impressed by the &lt;a href="https://docs.github.com/en/actions/using-workflows"&gt;CI workflows&lt;/a&gt;, when I set up a dedicated schema testing workflow. It showed me how automated testing can catch errors that manual testing might overlook. Although I implemented a straightforward workflow, it go me curious about learning more about CI workflows.&lt;/p&gt;
&lt;p&gt;I am currently adding tests for the new serialization support in NDCube, which I plan to finish by the next coding week.&lt;/p&gt;</description><category>SunPy</category><guid>http://openastronomy.org/Universe_OA/posts/2024/06/20240623_0719_viciouseagle03/</guid><pubDate>Sun, 23 Jun 2024 06:19:36 GMT</pubDate></item><item><title>Stark broadening and the Common API</title><link>http://openastronomy.org/Universe_OA/posts/2024/06/20240623_0000_code29563/</link><dc:creator>code29563</dc:creator><description>&lt;p&gt;Stark broadening for atomic lines has been implemented with different temperature scalings for neutral and ionised radiators.&lt;/p&gt;

&lt;p&gt;I integrated the API for atomic Kurucz linelists into the Common API, and in doing so the issue related to &lt;code class="language-plaintext highlighter-rouge"&gt;truncation&lt;/code&gt; and &lt;code class="language-plaintext highlighter-rouge"&gt;optimization&lt;/code&gt; seems to have been resolved.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;

&lt;p&gt;The main differentiating aspect so far in the Common API for Kurucz is that new versions of linelists and lab lines aren’t available for all species, the result being that the url from which to download the Kurucz linelist isn’t known with certainty before actually attempting to download it, so the possibilities are ranked and attempted in order of preference and the first to return a valid response is used.&lt;/p&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2024/06/20240623_0000_code29563/</guid><pubDate>Sat, 22 Jun 2024 23:00:00 GMT</pubDate></item><item><title>Deeper into OpenCL</title><link>http://openastronomy.org/Universe_OA/posts/2024/06/20240622_2045_deadspheroid/</link><dc:creator>DeadSpheroid</dc:creator><description>&lt;p class="intro"&gt;In this post, I hope to give a high level understanding of OpenCL and its workings&lt;/p&gt;

&lt;h2 id="setting-it-up"&gt;Setting it up&lt;/h2&gt;
&lt;!-- TEASER_END --&gt;
&lt;p align="center" width="100%"&gt;
&lt;img alt="OpenCL ICD" src="https://deadspheroid.github.io/my-blog/assets/img/ocl-icd.png" style="margin-bottom: 0; margin-top: 24px;"&gt;
&lt;/p&gt;
&lt;p&gt;OpenCL is relatively easy to get up and running on your system.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;For users:
All you need, is the OpenCL runtime for your device!
In case of Nvidia, this comes with the Nvidia drivers, while for Intel CPUs, this has to be manually installed by a package manager.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For developers:
You will need the OpenCL library to link against, and the OpenCL headers as well, again easily available in your package manager.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id="heres-a-new-perspective"&gt;Here’s a new perspective&lt;/h2&gt;
&lt;p align="center" width="100%"&gt;
&lt;img alt="OpenCL Platform Model" src="https://deadspheroid.github.io/my-blog/assets/img/ocl-platform.png" style="margin-bottom: 0; margin-top: 24px;"&gt;
&lt;/p&gt;
&lt;p&gt;OpenCL presents a general interface to the developer, no matter what the device or the architecture.&lt;/p&gt;

&lt;p&gt;Firstly, we have the host, which is responsible for all the book-keeping, and task scheduling on the OpenCL device.&lt;/p&gt;

&lt;p&gt;Then, we have our OpenCL device, which is divided into a number of compute units.
Each Compute Unit (CU) is further divided into a number of processing elements.&lt;/p&gt;

&lt;p&gt;But what do these words actually mean?&lt;/p&gt;

&lt;p&gt;Well, a Processing Element(PE) is a single unit, that is responsible for executing a single thread(also called a work item). Think of a single function being executed.&lt;/p&gt;

&lt;p&gt;Each PE has its own private memory, not accessible by anyone, but this PE&lt;/p&gt;

&lt;p&gt;A bunch of processing elements are grouped together to form a compute unit which, at a time, executes a single work group(grouping of many work items).&lt;/p&gt;

&lt;p&gt;The CUs all share a global memory, accessible by anyone&lt;/p&gt;

&lt;p&gt;So for a CPU, the maximum number of CU s is the number of CPU cores!&lt;/p&gt;

&lt;p&gt;But why do you want work groups? Why not have work items only?&lt;/p&gt;

&lt;p&gt;Well, having this grouping of work items, allows for a greater deal of complexity, because we can synchronize across items in a work group, have a local memory only for this work group, and more…&lt;/p&gt;

&lt;h2 id="a-complete-walkthrough"&gt;A complete walkthrough&lt;/h2&gt;
&lt;p align="center" width="100%"&gt;
&lt;img alt="OpenCL Execution Model" src="https://deadspheroid.github.io/my-blog/assets/img/ocl-exec.png" style="margin-bottom: 0; margin-top: 24px;"&gt;
&lt;/p&gt;

&lt;p&gt;Let’s look at a typical workflow for an OpenCL program&lt;/p&gt;

&lt;h3 id="initialisation"&gt;Initialisation&lt;/h3&gt;
&lt;p&gt;First we need to check the currently available OpenCL platforms, which are basically implementations of OpenCL available on your system&lt;/p&gt;

&lt;p&gt;For example, you can have both Intel OpenCL and POCL OpenCL for your i7 CPU.&lt;/p&gt;

&lt;p&gt;Then from these platforms, you need to choose a device to execute on. OpenCL supports CPUs, GPUs, FPGAs, and all sorts of accelerators.&lt;/p&gt;

&lt;h3 id="context"&gt;Context&lt;/h3&gt;
&lt;p&gt;Once you have the platform and device you wish to use, you need to create an OpenCL context, which will handle everything for that particular platform and device.&lt;/p&gt;

&lt;h3 id="command-queue"&gt;Command Queue&lt;/h3&gt;
&lt;p&gt;Then, you have to create a command queue, which, as the name suggests, will store any commands(kernels) you queue for execution, and dispatch them in order(or even out of order if you like!).&lt;/p&gt;

&lt;h3 id="kernel"&gt;Kernel&lt;/h3&gt;
&lt;p&gt;After the command queue, you must compile the kernel source code(the api provides functions to do this), so that it can be executed later.&lt;/p&gt;

&lt;h3 id="memory"&gt;Memory&lt;/h3&gt;
&lt;p&gt;Finally, one of the most important parts of this entire process, is passing the input to the OpenCL device.&lt;/p&gt;

&lt;p&gt;Now, initially the data is stored on your CPU RAM, which is unfortunately inaccessible to your GPU.&lt;/p&gt;

&lt;p&gt;Therefore you need to copy the data to your GPU RAM, using the &lt;code class="language-plaintext highlighter-rouge"&gt;cl_mem&lt;/code&gt; interface that OpenCL provides.&lt;/p&gt;

&lt;p&gt;However, if you know that the device being used is the same CPU, then this copy can be skipped, to save time, using the &lt;code class="language-plaintext highlighter-rouge"&gt;CL_MEM_USE_HOST_PTR&lt;/code&gt; flag while creating a &lt;code class="language-plaintext highlighter-rouge"&gt;cl_mem&lt;/code&gt; object.&lt;/p&gt;

&lt;h3 id="execution"&gt;Execution&lt;/h3&gt;
&lt;p&gt;At the end, you can use the command queue created earlier along with the &lt;code class="language-plaintext highlighter-rouge"&gt;cl_mem&lt;/code&gt; created previously to execute the compiled kernel on the device&lt;/p&gt;

&lt;p&gt;Subsequently don’t forget to copy the output data back to CPU RAM, if the execution was done on GPU.&lt;/p&gt;

&lt;p&gt;However, there’s still a ton of unexplained stuff like, “How do you save the time wasted in copying data to the device?” or “Can you pass any data to the device? Even structs?”&lt;/p&gt;

&lt;p&gt;We’ll explore OpenCL more in subsequent posts.&lt;/p&gt;</description><category>gnuastro</category><guid>http://openastronomy.org/Universe_OA/posts/2024/06/20240622_2045_deadspheroid/</guid><pubDate>Sat, 22 Jun 2024 19:45:00 GMT</pubDate></item><item><title>Making the sidebar intuitive</title><link>http://openastronomy.org/Universe_OA/posts/2024/06/20240621_1329_kartikmandar/</link><dc:creator>Kartik Mandar</dc:creator><description>&lt;p&gt; So over the weeks of coding we transformed from building just a quicklook dashboard to an interactive analysis dashboard. Currently I was making the sidebar intuitive to do all the stuff that we are planning to do from it.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2024/06/20240621_1329_kartikmandar/</guid><pubDate>Fri, 21 Jun 2024 12:29:00 GMT</pubDate></item></channel></rss>