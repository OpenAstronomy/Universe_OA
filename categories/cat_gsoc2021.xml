<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Universe OpenAstronomy (Posts about gsoc2021)</title><link>http://openastronomy.org/Universe_OA/</link><description></description><atom:link href="http://openastronomy.org/Universe_OA/categories/cat_gsoc2021.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Fri, 09 Jul 2021 04:43:00 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Chapter 2: Survey Corps</title><link>http://openastronomy.org/Universe_OA/posts/2021/07/20210705_2340_anandxkumar/</link><dc:creator>anandxkumar</dc:creator><description>&lt;div&gt;&lt;p&gt;So its been around 4 weeks into the coding period, a lot of insights and progress so far!&lt;/p&gt;
&lt;h3&gt;Profiler Class&lt;/h3&gt;
&lt;p&gt;The good news is that the Profiler class has been successfully implemented in the develop branch and will be available to users by version &lt;code class="language-text"&gt;0.9.30&lt;/code&gt; .&lt;br&gt;
&lt;!-- TEASER_END --&gt;
Link : &lt;a href="https://github.com/radis/radis/pull/286"&gt;Profiler PR&lt;/a&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Below is a simple example how all steps are printed based on the verbose level:&lt;br&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;wmin = 2000
wmax = 3300
wstep = 0.01
T = 3000.0 #K
p = 0.1 #bar
broadening_max_width=10

sf = SpectrumFactory(wavenum_min=wmin, wavenum_max=wmax,
pressure=p,
wstep=wstep,
broadening_max_width=broadening_max_width,
molecule="CO",
cutoff=0, # 1e-27,
verbose=3,
)
sf.load_databank('HITEMP-CO')
s = sf.eq_spectrum(T)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Output:&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;... Scaling equilibrium linestrength
... 0.01s - Scaled equilibrium linestrength
... 0.00s - Calculated lineshift
... 0.00s - Calculate broadening HWHM
... Calculating line broadening (60869 lines: expect ~ 6.09s on 1 CPU)
...... 0.16s - Precomputed DLM lineshapes (30)
...... 0.00s - Initialized vectors
...... 0.00s - Get closest matching line &amp;amp; fraction
...... 0.02s - Distribute lines over DLM
...... 1.95s - Convolve and sum on spectral range
... 2.14s - Calculated line broadening
... 0.01s - Calculated other spectral quantities
... 2.21s - Spectrum calculated (before object generation)
... 0.01s - Generated Spectrum object
2.22s - Spectrum calculated&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Also we can access these steps and the time taken by them using &lt;code class="language-text"&gt;Spectrum.get_conditions()['profiler']&lt;/code&gt;. Also there is a parameter &lt;code class="language-text"&gt;SpectrumFactory.profiler.relative_time_percentage&lt;/code&gt; that stores the percentage of time taken by each steps at a particular verbose level, helpful seeing the most expensive steps in Spectrum calculation.&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Legacy Method Complexity&lt;/h3&gt;
&lt;p&gt;Several Spectrums were benchmarked against various parameters to see it‚Äôs correlation and derive its complexity. We used Profiler class with &lt;a href="https://radis.readthedocs.io/en/latest/source/radis.lbl.loader.html#radis.lbl.loader.DatabankLoader.init_database"&gt;init_database()&lt;/a&gt; which stores all parameters of Spectrum along the Profiler in a &lt;code class="language-text"&gt;csv&lt;/code&gt; generated file; all spectrum info got added into the csv file  which could be used to do create visualizations to analyze the data. We used &lt;code class="language-text"&gt;Xexplorer&lt;/code&gt; library and &lt;code class="language-text"&gt;Tableau&lt;/code&gt;(a visual analytics platform) to create visualizations. A &lt;a href="https://github.com/anandxkumar/Benchmark_Visualization_GSoC_2021"&gt;github repository&lt;/a&gt; was created to store the Visualization along the CSV data file of each benchmark.&lt;/p&gt;
&lt;p&gt;Following are the inference of the benchmarks for Legacy Method:&lt;/p&gt;
&lt;b&gt;
‚Ä¢  Calculation Time ‚àù Number of lines&lt;br&gt;
‚Ä¢  Calculation Time ‚àù Broadening max width&lt;br&gt;
‚Ä¢  Calculation Time ‚àù 1/wstep&lt;br&gt;
‚Ä¢  Calculation Time not dependent on Spectral Range&lt;br&gt;
&lt;/b&gt;&lt;br&gt;
&lt;p&gt;So complexity of Legacy method can be derived as: &lt;br&gt;
&lt;strong&gt;&lt;code class="language-text"&gt;complexity = constant * Number of lines * Broadening Max Width / Wstep&lt;/code&gt;&lt;/strong&gt; &lt;br&gt;&lt;/p&gt;
&lt;h3&gt;LDM Method Complexity&lt;/h3&gt;
&lt;p&gt;Similar technique was used to benchmark LDM method. Now LDM uses 2 types of broadening method that are &lt;code class="language-text"&gt;voigt&lt;/code&gt; and &lt;code class="language-text"&gt;fft&lt;/code&gt;. &lt;code class="language-text"&gt;voigt&lt;/code&gt; uses truncation for calculating spectrum  in wavenmber space where as &lt;code class="language-text"&gt;fft&lt;/code&gt; calculates spectrum on entire spectral range in fourier space. So benchmarks were done on both methods to compare their performance against various parameters.&lt;/p&gt;
&lt;p&gt;Spectrum were benchmarked against parameters like Spectral Range, Wstep, Spectral Points, Number of Lines and Broadening Max Width. Following are the inferences.&lt;/p&gt;
&lt;p&gt;For &lt;code class="language-text"&gt;fft&lt;/code&gt;:&lt;br&gt;
&lt;b&gt;
‚Ä¢ Calculation Time ‚àù Spectral Points&lt;br&gt;
‚Ä¢ Calculation Time ‚àù Number of Lines&lt;br&gt;
&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;For &lt;code class="language-text"&gt;voigt&lt;/code&gt;:&lt;br&gt;
&lt;b&gt;
‚Ä¢ Calculation Time ‚àù Spectral Points&lt;br&gt;
‚Ä¢ Calculation Time ‚àù Number of Lines&lt;br&gt;
‚Ä¢ Calculation Time ‚àù Broadening Max Width&lt;br&gt;
&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;For LDM we are expecting the following complexity:&lt;br&gt;
&lt;strong&gt;&lt;code class="language-text"&gt;t_LDM_fft ~ c2*N_lines + c3*(N_G*N_L + 1)*N_v*log(N_v)&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
&lt;strong&gt;&lt;code class="language-text"&gt;t_LDM_voigt ~ c2*N_lines + c3'*(N_G*N_L + 1)*N_truncation*log(N_truncation)&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt; So the goal for the next 2 weeks will be to get the complexity of both &lt;code class="language-text"&gt;voigt&lt;/code&gt; and &lt;code class="language-text"&gt;fft&lt;/code&gt; method and see places for improving both methods and quite possibily create a &lt;code class="language-text"&gt;Hybrid&lt;/code&gt; method taking the best of both worlds. &lt;/p&gt;&lt;/div&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2021/07/20210705_2340_anandxkumar/</guid><pubDate>Mon, 05 Jul 2021 22:40:32 GMT</pubDate></item><item><title>A Month into GSoC</title><link>http://openastronomy.org/Universe_OA/posts/2021/07/20210705_2037_dhruv9vats/</link><dc:creator>Dhruv Vats</dc:creator><description>&lt;div&gt;&lt;p&gt;It‚Äôs almost been a month since the start of GSoC‚Äôs coding period and the work, I‚Äôm glad to write, is progressing at a steady and satisfactory rate.&lt;/p&gt;
&lt;h5&gt;The Developments&lt;/h5&gt;&lt;p&gt;The last time around, my first ever not-so-meaningless contribution to open-source had just got merged, and I was really happy about it. But what that also did was, get me over the initial anxiety and intimidation I might have been feeling towards open-source. This, I think, has also helped speed things¬†along.&lt;/p&gt;
&lt;p&gt;While I started working on the optional features of my project around 2 weeks ago, I had to scrape the initial implementation because it turned out to be very, very slow and therefore had to be completely replaced with a better and more efficient approach, which was a bit less straightforward. But now, two weeks into experimenting and iterating, a new pull-request has been opened with the newly implemented efficient version of the feature, and while it's still a few minor commits away from its final form, the core functionality works as expected and, if everything goes as expected, which is never a guarantee, a hefty part of my proposal‚Äôs objectives will be complete.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;And while this does not guarantee anything, I‚Äôd be lying if I said that I am not hoping for something exciting to do as I might have time to try out other things. What exactly, I honestly don‚Äôt know, but if I find myself in that minority who actually like what there doing, it‚Äôll be an absolute privilege, which I‚Äôm looking forward to and wishing¬†for.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*DvmM4lhclsVa5tIwl5um4A.png"&gt;&lt;figcaption&gt;Just some pastel colors for you to look¬†at&lt;/figcaption&gt;&lt;/figure&gt;&lt;h5&gt;The Goal&lt;/h5&gt;&lt;p&gt;What follows might be a very steep change in topic, but is one, that I think lies at the root of many seemingly normal activities. This is just something that has been on my mind lately, and what writing is, if not a tool to better understand yourself?&lt;/p&gt;
&lt;p&gt;I feel like an invisible aura is building around me saying that you are at a stage in life where you need to man up, where you should have everything together and figured out, but whenever I try and assess myself in this context, I always, without fail, fell short of it and by a good margin. While the contrasting opposite of this would be saying that I am everything I ever wanted to be and have nothing to work towards, would be outright arrogant and even dangerous, there must be a balance somewhere, right?&lt;/p&gt;
&lt;p&gt;But why should I have it all figured out, what‚Äôs even the need? And while statements like these can be argued against using something like, because everyone is doing it, and this is the way, they give the vibes of being in a pipeline you‚Äôve been pushed into and now have no option but to pass through. And this, I think, many will agree, is not a very desirable situation.&lt;/p&gt;
&lt;p&gt;This need to progress towards something also spurs off many questions, one of which is ‚Äúthe why¬†?‚Äù. The why, is an oh-so-difficult question to answer that honestly makes me feel frustrated at times, not knowing to what end all the efforts are being¬†put.&lt;/p&gt;
&lt;p&gt;While it can be argued that this is a ridiculous thing to think about, and one should not set overly optimistic expectations, this, I feel, contradicts the notion of elegance that I somehow have associated with the fundamental workings of the world. If someone asked me to comment on the secrets of the Universe, I‚Äôd be very comfortable with using the words elegant and sophisticated, even though I basically know nothing about it? Why? Is this just a desire to find meaning in everything, or is there something else at¬†play?&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=805d42b1b5ce" width="1"&gt;&lt;/div&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2021/07/20210705_2037_dhruv9vats/</guid><pubDate>Mon, 05 Jul 2021 19:37:36 GMT</pubDate></item><item><title>Insight of Implementation of JAX to stingray- GSoC coding period!</title><link>http://openastronomy.org/Universe_OA/posts/2021/07/20210705_1420_rashmiraj137/</link><dc:creator>Raj Rashmi</dc:creator><description>&lt;div&gt;&lt;p&gt;In the last blog, I wrote about Introduction to JAX and Automatic Differentiation. In this one, my plan for the next stage of implementation. Currently, I am working on the modeling notebook (&lt;a href="https://github.com/StingraySoftware/notebooks/blob/main/Modeling/ModelingExamples.ipynb"&gt;https://github.com/StingraySoftware/notebooks/blob/main/Modeling/ModelingExamples.ipynb&lt;/a&gt;) to re-design it using JAX, especially to make optimization more robust by having JAX compute gradients on the likelihood function.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/380/1*u_c4S0h60T1IECOBQVTS1A.jpeg"&gt;&lt;/figure&gt;&lt;p&gt;My mentor Daniela highlighted the issue that the current implementation is not robust using NumPy. The plan is to keep working on the current modeling notebook replacing NumPy by jax.numpy and also use grad, jit, vmap, random functionality of JAX.&lt;br&gt;When it comes to re-design, understanding the current design and the possible drawback and issues with corresponding packages comes on you first and I am trying them out. One such challenge is importing emcee into jupyter notebook for sampling. Despite making sure, I download the dependency in the current virtual environment and then making sure I import emcee into the notebook, it is still acting weird and showing an error: emcee not installed! Can‚Äôt sample! It looks like a clash of dependencies.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/240/1*JtGB50sLscB1BBPt9k3pfw.jpeg"&gt;&lt;figcaption&gt;Trying to have fun while it¬†lasts!&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;For now, the plan is to solve every bug I face in the journey and then proceed with understanding how everything connects and the next step is to come up with the report of optimization using JAX. Stay tuned for more on how JAX can accelerate and augment the current modeling framework.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;I would recommend one video for anyone who wants to understand the functionality of JAX better and relate more to my study (click¬†&lt;a href="https://www.youtube.com/watch?v=0mVmRHMaOJ4&amp;amp;ab_channel=GoogleCloudTech"&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=1756040fa5ae" width="1"&gt;&lt;/div&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2021/07/20210705_1420_rashmiraj137/</guid><pubDate>Mon, 05 Jul 2021 13:20:55 GMT</pubDate></item><item><title>GSoC Post 2</title><link>http://openastronomy.org/Universe_OA/posts/2021/07/20210705_1417_ndanzanello/</link><dc:creator>ndanzanello</dc:creator><description>&lt;div&gt;&lt;p&gt;Hi! In the previous post I mentioned that the matching part between the quads was done. Following that, the past 2 weeks were devoted to:&lt;/p&gt;


&lt;!-- TEASER_END --&gt;

&lt;ul&gt;&lt;li&gt;first get the theta (rotation) and scale values related to each quad. To do this, we use a linear transformation between the pixel coordinates and the projection plane coordinates (that come from the celestial ones);&lt;/li&gt;&lt;li&gt;use some statistics in the thetas and scales above to get the parameters of the wcs (world coordinate system). Also, we have to decide where the reference point is. To do this, we use the A vertex that is closer to the median of all A vertices from the matched quads;&lt;/li&gt;&lt;li&gt;after the parameters of the wcs are ready, we write them into a fits file.&lt;/li&gt;&lt;/ul&gt;



&lt;p&gt;Basically, this would make this part over. But we noticed a problem that needed a debug: we were finding few matches. In examples with fainter stars, we wouldn‚Äôt even get one match. So, to solve this, we had to change the way we were making the quads, because we were not considering all the possible quads combinations of the stars we selected. After that, we could go, for example, from ten of thousands of quads to millions of quads! This also improves a lot the statistics that we need to do.&lt;/p&gt;



&lt;p&gt;So now we have to start dealing with some distortions too! &lt;img alt="üôÇ" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" style="height: 1em;"&gt;&lt;/p&gt;



&lt;p&gt;&lt;/p&gt;&lt;/div&gt;</description><category>gnuastro</category><guid>http://openastronomy.org/Universe_OA/posts/2021/07/20210705_1417_ndanzanello/</guid><pubDate>Mon, 05 Jul 2021 13:17:22 GMT</pubDate></item><item><title>astropy@GSoC Blog Post #3, Week 3</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210622_2223_suyog7130/</link><dc:creator>Suyog Garg</dc:creator><description>&lt;div&gt;So, it's the start of the 3rd week now. I will be virtually meeting Aarya and Moritz again Tom.&lt;br&gt;&lt;br&gt;For the past few weeks now, I have been pushing commits to a Draft PR¬†&lt;a href="https://github.com/astropy/astropy/pull/11835"&gt;https://github.com/astropy/astropy/pull/11835&lt;/a&gt;¬†on GitHub. I wanted to have something working quite early in the project, in order to be able to pinpoint accurately when something doesn't work. This is why I started with directly adding the &lt;b&gt;cdspyreadme&lt;/b&gt; code within Astropy. Afterwards, I am also writing the code from scratch. As more of the required features from &lt;b&gt;cdspyreadme&lt;/b&gt; get integrated into &lt;i&gt;cds.py&lt;/i&gt;, those files and codes added earlier will be removed.&lt;br&gt;&lt;br&gt;About the reading/writing to Machine Readable Table format, in fact I wrote about it briefly in my GSoC Proposal that I could attempt it as an extension. I don't have an opinion on whether or not it should have it's own format classes etc. However, since the title of my GSoC project is to &lt;b&gt;Add a CDS format writer to Astropy&lt;/b&gt;, I would prefer to work on the CDS format writer first and then on the MRT format. The MRT header anyway appears to be a bit simpler than the CDS header, so there shouldn't be much difficulty in the extension.&lt;br&gt;&lt;br&gt;So, in a nutshell, this is my workflow:&lt;br&gt;&lt;ul style="text-align: left;"&gt;&lt;li&gt;Try out directly using &lt;b&gt;cdspyreadme&lt;/b&gt; from within Astropy.&lt;/li&gt;&lt;li&gt;Add CdsData.write method.&lt;/li&gt;&lt;li&gt;Add a ByteByByte writer.&lt;/li&gt;&lt;li&gt;Write features to add complete ReadMe to the Header, starting off with having both ReadMe and Data in a single file.&lt;/li&gt;&lt;li&gt;Have features for writing separate CDS ReadMe and Data file.&lt;/li&gt;&lt;li&gt;Further work on some specific table columns, for instance, those containing Units and Coordinates.&lt;/li&gt;&lt;li&gt;Add appropriate tests along the way.&lt;/li&gt;&lt;li&gt;Resolve other issues that come up.&lt;/li&gt;&lt;li&gt;MRT format reader/writer.&lt;/li&gt;&lt;/ul&gt;&lt;br&gt;I have completed the first three tasks and will now work on the fourth. I think by the time this finishes, a separate &lt;i&gt;CDSColumn.py&lt;/i&gt; won't be required. I can open another PR which adds the Data writer, in the meantime.&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Let's see how it goes!&lt;/div&gt;
&lt;!-- TEASER_END --&gt;&lt;/div&gt;</description><category>Astropy</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210622_2223_suyog7130/</guid><pubDate>Tue, 22 Jun 2021 21:23:00 GMT</pubDate></item><item><title>Rotation and Coordinates</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210622_0048_jeffreypaul15/</link><dc:creator>Jeffrey Paul</dc:creator><description>&lt;div&gt;&lt;p&gt;Finally, the official ‚Äúcoding period‚Äù of &lt;strong&gt;GSoC&lt;/strong&gt; finally began a couple of days ago. From where we started of with Sunkit-Pyvista, to where we are today makes me feel a tad bit happy!¬†üòÑ&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/985/1*-X2tw67TTrMoj3F43QS0uA.jpeg"&gt;&lt;/figure&gt;&lt;p&gt;Weeks 1 and 2 were initially set out for me to complete adding rotation functionality to the library, which started off great, but ended up causing some confusion üòÖ.&lt;/p&gt;
&lt;p&gt;This was quickly sorted out and we went with not having to implement rotation functionality and moved on, learning that not everything will go according to plan and it‚Äôs okay for stuff to not work out at¬†times.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;The rest of the work that I had set out to do was completed well and it was all smooth sailing from then¬†on.&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;We worked on implementing the setting of the initial camera position from Astropy‚Äôs Skycoord.&lt;/li&gt;&lt;li&gt;A few 2D methods were converted to it‚Äôs 3D counter part to be¬†used.&lt;/li&gt;&lt;li&gt;Unit tests for the implemented methods were added as¬†well.&lt;/li&gt;&lt;/ol&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/225/0*alo7nZjT7uTO6BzJ"&gt;&lt;figcaption&gt;Mid-level solar flare, observed on Jan. 12,¬†2015.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;I‚Äôm don‚Äôt know the first thing when it comes to astrophysics or astronomy, I do know that there is some pretty cool stuff going on out there though! I may not know what that is, but there‚Äôs a small sense of satisfaction in knowing that maybe whatever I‚Äôm doing is going to help someone out there do their work¬†better.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;So far, so¬†good.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=1c9683c38461" width="1"&gt;&lt;/div&gt;</description><category>SunPy</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210622_0048_jeffreypaul15/</guid><pubDate>Mon, 21 Jun 2021 23:48:18 GMT</pubDate></item><item><title>Chapter 1: First Flight</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210621_2240_anandxkumar/</link><dc:creator>anandxkumar</dc:creator><description>&lt;div&gt;&lt;p&gt;Hey! Missed me? I‚Äôm back with another blog, the first related to the Coding Period. Got some progress and interesting observation to share!&lt;/p&gt;
&lt;h3&gt;Ready -&amp;gt; Set -&amp;gt; Code -&amp;gt; Analyze&lt;/h3&gt;
&lt;p&gt;The first thing I did in the coding period, was analyse the problem and get a feasible approach to resolve it.&lt;br&gt;&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; Find the complexity of the Legacy and LDM method.&lt;br&gt;
&lt;strong&gt;Solution:&lt;/strong&gt; Run some benchmarks and find the bottleneck step.&lt;br&gt;&lt;/p&gt;
&lt;p&gt;First I chose the &lt;strong&gt;Legacy&lt;/strong&gt; method because if its simpler architecture. I ran some benchmarks varying the &lt;code class="language-text"&gt;spectral range&lt;/code&gt; of &lt;code class="language-text"&gt;OH&lt;/code&gt; and &lt;code class="language-text"&gt;CO2&lt;/code&gt; molecule to get similar number of lines. I kept parameters like &lt;code class="language-text"&gt;pressure&lt;/code&gt;, &lt;code class="language-text"&gt;temperature&lt;/code&gt;, &lt;code class="language-text"&gt;broadening_max_width&lt;/code&gt;, &lt;code class="language-text"&gt;wstep&lt;/code&gt;, etc constant to see the dependence of Legacy method on &lt;strong&gt;Spectral range&lt;/strong&gt;. &lt;br&gt;&lt;/p&gt;
&lt;p&gt;In order to get similar number of lines, I created a function which will take the &lt;strong&gt;Spectrum Factory&lt;/strong&gt; &lt;code class="language-text"&gt;dataframe&lt;/code&gt; and select the target number of lines. But the issue with Pandas dataframe is that when modify the dataframe there are chances that the metadata will get lost and we will no longer be able to do Spectrum calculation. To avoid this we have to drop the right number of lines with &lt;code class="language-text"&gt;inplace=True&lt;/code&gt;. So we will need to fix the number of lines and then we can proceed ahead with the benchmarking. Every parameter is the same except the Spectral Range.  Full code &lt;a href="https://gist.github.com/anandxkumar/cbe12f47170e1d71a82f4b246bd01dcc"&gt;here&lt;/a&gt;.&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Earlier we assumed that the complexity of Legacy method is: &lt;br&gt;
&lt;strong&gt;&lt;code class="language-text"&gt;Voigt Broadening = Broadening_max_width * spectral_range/math.pow(wstep,2) * N&lt;/code&gt;&lt;/strong&gt; &lt;br&gt;&lt;/p&gt;
&lt;p&gt;Thus I was expecting to have different calculation time for both benchmarks. But to my surprise the computational times were almost equivalent! I re-ran each benchmarks &lt;strong&gt;100 times&lt;/strong&gt; just to be sure and more precise about it. Following were the observations:&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Number of lines - &lt;b&gt;{‚ÄòOH‚Äô: 28143, ‚ÄòCO‚Äô: 26778}&lt;/b&gt;&lt;/li&gt;
&lt;li&gt;Total Calculation time(Avg) -  &lt;b&gt;{‚ÄòOH‚Äô: 4.4087, ‚ÄòCO‚Äô: 3.8404000000000003}&lt;/b&gt;&lt;/li&gt;
&lt;li&gt;Total Voigt_Broadening TIME(Avg) - &lt;b&gt;{‚ÄòOH‚Äô: 3.1428814244270327, ‚ÄòCO‚Äô: 3.081623389720917}&lt;/b&gt;&lt;/li&gt;
&lt;li&gt;spectral_range - &lt;b&gt;{‚ÄòOH‚Äô: 38010, ‚ÄòCO‚Äô: 8010}&lt;/b&gt;&lt;/li&gt;
&lt;li&gt;Legacy_Scale - &lt;b&gt;{‚ÄòOH‚Äô: 4x10^14, ‚ÄòCO‚Äô: 8x10^13}&lt;/b&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are some inference we can make from the above observation:&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A)&lt;/strong&gt; The bottleneck step(Voigt Broadening) loosely depends on &lt;code class="language-text"&gt;Spectral Range&lt;/code&gt;.&lt;br&gt;
&lt;strong&gt;B)&lt;/strong&gt; The complexity of Voigt Broadening needs to be modified because there is a difference of order of &lt;strong&gt;~10&lt;/strong&gt; in the Legacy Scaled value of OH and CO2.&lt;br&gt;&lt;/p&gt;
&lt;p align="center"&gt;
&lt;span class="gatsby-resp-image-wrapper" style="display: block; margin-left: auto; margin-right: auto;"&gt;
&lt;a class="gatsby-resp-image-link" href="https://anandkumar-blog.netlify.app/static/d9b32b4e96e6cd9a91016a49ad940239/0b533/Blog2.png" rel="noopener" style="display: block;" target="_blank"&gt;
&lt;span class="gatsby-resp-image-background-image" style="padding-bottom: 100%; display: block;"&gt;&lt;/span&gt;
&lt;img alt="Blog2" class="gatsby-resp-image-image" src="https://anandkumar-blog.netlify.app/static/d9b32b4e96e6cd9a91016a49ad940239/0b533/Blog2.png" style="width: 100%; height: 100%; margin: 0; vertical-align: middle;" title="Blog2"&gt;
&lt;/a&gt;
&lt;/span&gt;&lt;br&gt;
&lt;b&gt;Credits - Me :p&lt;/b&gt;&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;So in order to do some analysis, we first need data of different steps in the broadening phase and conditions of various Spectrum which brings me to the &lt;strong&gt;Code&lt;/strong&gt; part in &lt;strong&gt;Coding Period.&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Profiler Class&lt;/h3&gt;
&lt;p&gt;The aim of this class is to replace all the print statements by a common &lt;code class="language-text"&gt;start&lt;/code&gt;, &lt;code class="language-text"&gt;stop&lt;/code&gt;, &lt;code class="language-text"&gt;_print&lt;/code&gt; method. Earlier each step computational time was done using &lt;code class="language-text"&gt;time()&lt;/code&gt; library. Now the whole codebase is being refactored with the Profiler class that will do all the work based on the &lt;code class="language-text"&gt;verbose&lt;/code&gt; level. In addition to this the biggest benefit is that each step will be stored in a dictionary with its computational time that will help me gather data to find which step is in actual bottleneck and further which part of the function is the most expensive time wise. A simple example is below:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Before:&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;if __debug__:
t0 = time()
..........
..........
if __debug__:
t1 = time()
.........
.........
if __debug__:
if self.verbose &amp;gt;= 3:
printg("... Initialized vectors in {0:.1f}s".format(t1 - t0))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;After:&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;self.profiler.start(
key="init_vectors", verbose=3, details="Initialized vectors"
)
.........
.........
self.profiler.stop("init_vectors")&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So using a common key we can make it happen. This will be stored in the conditons of &lt;code class="language-text"&gt;Spectrum&lt;/code&gt; object in the &lt;code class="language-text"&gt;'profiler'&lt;/code&gt; key. All these Spectrums and their conditions can be exported using a &lt;a href="https://radis.readthedocs.io/en/latest/spectrum/spectrum.html#spectrum-database"&gt;SpecDatabase&lt;/a&gt;. This will create a csv file comprising of all the parameters of all Spectrums which will be useful in getting some insights.
-&amp;gt; &lt;a href="https://github.com/radis/radis/pull/286"&gt;PR LINK&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Digging in whiting_jit&lt;/h3&gt;
&lt;p&gt;Based on several benchmarks, it is estimated that around &lt;strong&gt;70-80%&lt;/strong&gt; time is spent on calculating the broadening. The broadening part has the following hierarchy:&lt;br&gt;
&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;_calc_broadening()
-&amp;gt; _calc_lineshape()
-&amp;gt; _voigt_broadening()
-&amp;gt; _voigt_lineshape()
-&amp;gt; whiting_jit()&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;On close inspection we observed that &lt;strong&gt;80-90%&lt;/strong&gt; time is spent on &lt;code class="language-text"&gt;whiting_jit&lt;/code&gt; process. Going further down in &lt;code class="language-text"&gt;whiting_jit&lt;/code&gt;, &lt;strong&gt;60-80%&lt;/strong&gt; time is spent on &lt;strong&gt;lineshape calculation.&lt;/strong&gt; Below is the formula:&lt;br&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;lineshape = (
(1 - wl_wv) * exp(-2.772 * w_wv_2)
+ wl_wv * 1 / (1 + 4 * w_wv_2)
# ... 2nd order correction
+ 0.016 * (1 - wl_wv) * wl_wv * (exp(-0.4 * w_wv_225) - 10 / (10 + w_wv_225))
)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The whole process can be divided into 4 parts:&lt;br&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;    part_1 =   (1 - wl_wv) * exp(-2.772 * w_wv_2)

part_2 =    wl_wv * 1 / (1 + 4 * w_wv_2)

# ... 2nd order correction
part_3 =  0.016 * (1 - wl_wv) * wl_wv * exp(-0.4 * w_wv_225)

part_4 =  - 10 / (10 + w_wv_225)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The complexity of each part comes out: &lt;br&gt;
&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;    o1 = broadening__max_width * n_lines / wstep

O(part_1) = n_lines * o1
O(part_2) = n_lines * 4 * o1
O(part_3) = (n_lines)**2 * o1
O(part_4) = o1 &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Running several benchmark showed us that &lt;strong&gt;part_3&lt;/strong&gt; takes the most time out of all steps. So clearly we can see that the complexity of Legacy method is not dependent on
Spectral Range but rather &lt;code class="language-text"&gt;Number of Calculated Lines&lt;/code&gt;,&lt;code class="language-text"&gt;broadening__max_width&lt;/code&gt; and &lt;code class="language-text"&gt;wstep&lt;/code&gt;. It may seem that the complexity of Legacy method is:&lt;br&gt;&lt;/p&gt;
&lt;p align="center"&gt;&lt;b&gt; n_lines^2 * broadening__max_width * n_lines / wstep&lt;/b&gt;&lt;/p&gt; &lt;br&gt;
&lt;p&gt;But inorder to prove this we need more benchmarks and evidence to verify this and it may involve normalization of all steps in lineshape calculation!&lt;br&gt; &lt;/p&gt;
&lt;p&gt;So the goal for the next 2 weeks is clear:&lt;br&gt;
&lt;b&gt;i)&lt;/b&gt; Refactor the entire codebase with Profiler.&lt;br&gt;
&lt;b&gt;ii)&lt;/b&gt; Find the complexity of &lt;strong&gt;Legacy Method&lt;/strong&gt; with the help of more benchmark and analysis.&lt;br&gt;
&lt;b&gt;iii)&lt;/b&gt; Do the same for &lt;strong&gt;LDM Method&lt;/strong&gt;!&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Ok I guess time‚Äôs up! See you after 2 weeks :)&lt;/p&gt;&lt;/div&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210621_2240_anandxkumar/</guid><pubDate>Mon, 21 Jun 2021 21:40:32 GMT</pubDate></item><item><title>About my Google Summer of Code Project: Part 1</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210621_1658_adwaitbhope/</link><dc:creator>Adwait Bhope</dc:creator><description>&lt;div&gt;&lt;p&gt;I had been eyeing Google Summer of Code last year (and the year before that), but never really got around to doing anything about it. It‚Äôs a wonderful learning experience and being in my final year of college this was the last opportunity I was going to get. So I decided to give it a¬†shot.&lt;/p&gt;
&lt;p&gt;I started late, sometime during late February. I picked out a few organizations that looked interesting to me. &lt;a href="https://openastronomy.org/"&gt;openastronomy&lt;/a&gt; particularly caught my eye because I was working on another project of mine related to Astronomy. In fact we were using one of the Python libraries under openastronomy. Now this is an umbrella organization, which means that there are multiple sub-organizations‚Ää‚Äî‚Ääsunpy, astropy, radis, poliastro, and a few more. They‚Äôre used extensively by the scientific community in their research. The project I selected was under sunpy, which is a Python library for solar data analysis. The project is about resampling data to increase or decrease its resolution‚Ää‚Äî‚Äämore on that later. Again, I had recently performed this operation in one of my projects, so it seemed only natural for me to go with this one. I worked on some issues on GitHub and submitted PRs, tried to get a hold of the codebase, put together a proposal, got feedback from the project mentors and friends, and submitted it. After about a month of impatient waiting, I received an email saying that my proposal was accepted! Awesome!&lt;/p&gt;
&lt;p&gt;Now, I plan to continue writing these blogs throughout the project and since this is the first one, let me take a moment to talk about the project. So, there‚Äôs a sunpy-affiliated package called ndcube, which exists to provide users an easier way of handling coordinates. Astronomical data like images taken from cameras are usually stored as n-dimensional arrays. A dimension could represent spatial or temporal axes. In such an array, the pixel coordinates map to some coordinates in the real world. These could be RA and Dec, or in the case of solar data, Helioprojective Latitude and Longitude. Nevertheless, there needs to be a mapping from the pixels to the real world. This is given by the World Coordinate System, which is just a set of (complicated) mathematical transformations. ndcube is a package that correlates the actual data with its transformations in such a way that you can manipulate the data, and the transformations will continue to remain consistent. It can be used with any type of data like images, spectra, timeseries data, and so¬†on.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*MHKTGHeR4F3nHd2gO1ocUw.png"&gt;&lt;figcaption&gt;Open Astronomy and¬†SunPy&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Often, researchers like to upsample or downsample the resolution of their data, perhaps to improve the Signal to Noise Ratio or even just to get their data onto the same grid. My project under Google Summer of Code is exactly this‚Ää‚Äî‚Ääto implement this functionality under ndcube. Luckily, there exists a package called reproject that has a few algorithms implemented already. My job would be to expose this through a succinct API under¬†ndcube.&lt;/p&gt;
&lt;p&gt;So far, my mentors and I have broken down this work and set smaller and more achievable targets to begin with, and I‚Äôve started working on them. Unfortunately, my Community Bonding Period was quite stagnant thanks to my college commitments, but now that they‚Äôre out of the way, I have more time on my hands to devote to the project. I‚Äôll be publishing more blogs about my progress in the coming weeks, hopefully more frequently. Talk to you in the next¬†one!&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=b56e7277046e" width="1"&gt;&lt;/div&gt;</description><category>SunPy</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210621_1658_adwaitbhope/</guid><pubDate>Mon, 21 Jun 2021 15:58:34 GMT</pubDate></item><item><title>GSoC Progress Report? Almost Done!</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210621_1523_dhruv9vats/</link><dc:creator>Dhruv Vats</dc:creator><description>&lt;div&gt;&lt;p&gt;With only 2 weeks into the coding period, it feels good to say that the bulk of the work associated with the 2 milestones is &lt;em&gt;almost &lt;/em&gt;done. Almost because the newly added functionality is yet to be battle-tested and while there is always room for change and improvements, my focus will be shifting from the main objective to the optional objectives.&lt;/p&gt;
&lt;h5&gt;The Project&lt;/h5&gt;&lt;p&gt;Scientific jargon¬†ahead!&lt;/p&gt;
&lt;p&gt;The study and interpretation of time-series data have become an integral part of modern-day astronomical studies and a common approach for characterizing the properties of time-series data is to estimate the power spectrum of the data using the periodogram. But the periodogram as an estimate suffers from¬†being:&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;ol&gt;&lt;li&gt;Statistically inconsistent (that is, its variance does not go to zero as the number of data samples reach infinity),&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;2. Biased for finite samples,¬†and&lt;/p&gt;
&lt;p&gt;3. Suffers from spectral¬†leakage.&lt;/p&gt;
&lt;figure&gt;&lt;a href="https://github.com/StingraySoftware/stingray"&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/700/1*5GHLU2O-S9d06amIH5ESBA.png"&gt;&lt;/a&gt;&lt;figcaption&gt;Stingray is a spectral-timing software package for astrophysical X-ray (and other)¬†data.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;My project aimed at implementing and integrating a superior spectral estimation technique, known as the Multitaper periodogram, into a Software Package called Stingray, a sub-organization of OpenAstronomy.&lt;/p&gt;
&lt;p&gt;This Multitaper algorithm uses windows or tapers (bell-shaped functions), which are multiplied with the time-series data before finding its frequency domain estimate. These windows, called the discrete prolate spheroidal sequences (DPSS), help mitigate the problems mentioned above.&lt;/p&gt;
&lt;p&gt;Any more technical stuff and this blog will start taking the shape of my proposal, so I‚Äôll leave it here, but for anyone more interested, &lt;a href="https://www.dropbox.com/s/g2m2p10en8ygpmz/Proposal.pdf?dl=0"&gt;here&lt;/a&gt; is the proposal.&lt;/p&gt;
&lt;h5&gt;The Process&lt;/h5&gt;&lt;p&gt;I started working on the project in early March, before submitting the proposal, and it initially included writing a wrapper around an external package to make the use of this tool coherent with the rest of the project. While a proof-of-concept implementation was put together, it was later decided to use SciPy to do the grunt work, as it already was a dependency, somewhat changing the initial milestones.&lt;/p&gt;
&lt;p&gt;So in a way, I was already working on the project before the results were announced and ended up opening a pull request with the newly added method 1 week into the coding period and it got merged a week later, which happened to be fairly early in the GSoC program coding timeline. Perks of open-source?!&lt;/p&gt;
&lt;p&gt;So while there are sure to be improvements and additions in terms of coherency and features, this does give a bit more breathing room and time for experimentation and exploration, and I‚Äôll try and make good use of it, primarily by working on the optional but quite good-to-have features.&lt;/p&gt;
&lt;p&gt;For anyone interested, &lt;a href="https://github.com/StingraySoftware/stingray/pull/578"&gt;here&lt;/a&gt; is the¬†PR.&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=6239f301b23" width="1"&gt;&lt;/div&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210621_1523_dhruv9vats/</guid><pubDate>Mon, 21 Jun 2021 14:23:27 GMT</pubDate></item><item><title>JAX-based automatic differentiation: Introduction of modern statistical modeling to Stingray</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210621_1404_rashmiraj137/</link><dc:creator>Raj Rashmi</dc:creator><description>&lt;div&gt;&lt;p&gt;I assume everyone reading this is already aware of two classical forms of differentiation, namely symbolic and finite differentiation. Symbolic differentiation operates on expanded mathematical expressions which lead to inefficient code and introduction of truncation error while finite differentiation deals with round-off errors. Optimized calculation of derivatives is crucial when it comes to training neural networks or mathematical modeling using bayesian inference. Both classical methods are slow at computing partial derivatives of a function with respect to many inputs, as is needed for gradient-based optimization algorithms. Here, automatic differentiation comes to the rescue. Automatic differentiation is a powerful tool to automate the calculation of derivatives and is preferable to more traditional methods, especially when differentiating complex algorithms and mathematical functions.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*NzyxsrkiLjjyjiIuCf123w.png"&gt;&lt;figcaption&gt;Photo Source: Wikipedia&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Stingray is astrophysical spectral timing software, a library in python built to perform time series analysis and related tasks on astronomical light curves. JAX is a python library designed for high-performance numerical computing. Its API for numerical functions is based on NumPy, a collection of functions used in scientific computing. Both Python and NumPy are widely used and familiar, making JAX simple, flexible, and easy to adopt. It can differentiate through a large subset of python‚Äôs features, including loops, ifs, recursion, and closures, and it can even take derivatives of derivatives. Such modern differentiation packages deploy a broad range of computational techniques to improve the applicability, run time, and memory management.&lt;/p&gt;
&lt;p&gt;JAX utilizes the grad function transformation to convert a function into a function that returns the original function‚Äôs gradient, just like Autograd. Beyond that, JAX offers a function transformation jit for just-in-time compilation of existing functions and vmap and pmap for vectorization and parallelization, respectively.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*5hEIw7ou4eeAX-HnP_23_A.png"&gt;&lt;figcaption&gt;&lt;em&gt;Mini-MLP(Multiple layer Perceptron) execution time for 10,000 updates with a batch size of 1024. Source: AI¬†Zone&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;As seen in the above figure, PyTorch has much more effective in terms of execution speed than TensorFlow when it came to implementing fully connected neural layers. For low-level implementations, on the other hand, JAX offers impressive speed-ups of an order of magnitude or more over the comparable Autograd library. JAX is faster than any other library when MLP implementation was limited to matrix multiplication operations.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;How do we decide the ideal library to go¬†with?&lt;/strong&gt;&lt;/h4&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/626/1*8-_tFBCgszXxQwkMEwEcjA.jpeg"&gt;&lt;/figure&gt;&lt;p&gt;Our choice will first depend on the history of the project we start working on, if the code already uses PyTorch then most probably we will end up using PyTorch for writing our code. For general differentiable programming with low-level implementations of abstract mathematical concepts, JAX offers substantial advantages in speed and scale over Autograd while retaining much of Autograd‚Äôs simplicity and flexibility, while also offering surprisingly competitive performance against PyTorch and TensorFlow.&lt;/p&gt;
&lt;p&gt;I am implementing modern stingray modeling to Stingray software as a part of my GSoC project. Reference to Stingray source code: &lt;a href="https://github.com/StingraySoftware"&gt;https://github.com/StingraySoftware&lt;/a&gt;. Reference to JAX-based automatic differentiation: &lt;a href="https://jax.readthedocs.io/en/latest/jax-101/index.html"&gt;https://jax.readthedocs.io/en/latest/jax-101/index.html&lt;/a&gt;.&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=1bc26da7571f" width="1"&gt;&lt;/div&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210621_1404_rashmiraj137/</guid><pubDate>Mon, 21 Jun 2021 13:04:09 GMT</pubDate></item></channel></rss>