<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Universe OpenAstronomy</title><link>http://openastronomy.org/Universe_OA/</link><description>This is an aggregator of openastronomy people</description><atom:link href="http://openastronomy.org/Universe_OA/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Tue, 22 Jun 2021 04:33:44 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Rotation and Coordinates</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210622_0048_jeffreypaul15/</link><dc:creator>Jeffrey Paul</dc:creator><description>&lt;div&gt;&lt;p&gt;Finally, the official ‚Äúcoding period‚Äù of &lt;strong&gt;GSoC&lt;/strong&gt; finally began a couple of days ago. From where we started of with Sunkit-Pyvista, to where we are today makes me feel a tad bit happy!¬†üòÑ&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/985/1*-X2tw67TTrMoj3F43QS0uA.jpeg"&gt;&lt;/figure&gt;&lt;p&gt;Weeks 1 and 2 were initially set out for me to complete adding rotation functionality to the library, which started off great, but ended up causing some confusion üòÖ.&lt;/p&gt;
&lt;p&gt;This was quickly sorted out and we went with not having to implement rotation functionality and moved on, learning that not everything will go according to plan and it‚Äôs okay for stuff to not work out at¬†times.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;The rest of the work that I had set out to do was completed well and it was all smooth sailing from then¬†on.&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;We worked on implementing the setting of the initial camera position from Astropy‚Äôs Skycoord.&lt;/li&gt;&lt;li&gt;A few 2D methods were converted to it‚Äôs 3D counter part to be¬†used.&lt;/li&gt;&lt;li&gt;Unit tests for the implemented methods were added as¬†well.&lt;/li&gt;&lt;/ol&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/225/0*alo7nZjT7uTO6BzJ"&gt;&lt;figcaption&gt;Mid-level solar flare, observed on Jan. 12,¬†2015.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;I‚Äôm don‚Äôt know the first thing when it comes to astrophysics or astronomy, I do know that there is some pretty cool stuff going on out there though! I may not know what that is, but there‚Äôs a small sense of satisfaction in knowing that maybe whatever I‚Äôm doing is going to help someone out there do their work¬†better.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;So far, so¬†good.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=1c9683c38461" width="1"&gt;&lt;/div&gt;</description><category>SunPy</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210622_0048_jeffreypaul15/</guid><pubDate>Mon, 21 Jun 2021 23:48:18 GMT</pubDate></item><item><title>Chapter 1: First Flight</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210621_2240_anandxkumar/</link><dc:creator>anandxkumar</dc:creator><description>&lt;div&gt;&lt;p&gt;Hey! Missed me? I‚Äôm back with another blog, the first related to the Coding Period. Got some progress and interesting observation to share!&lt;/p&gt;
&lt;h3&gt;Ready -&amp;gt; Set -&amp;gt; Code -&amp;gt; Analyze&lt;/h3&gt;
&lt;p&gt;The first thing I did in the coding period, was analyse the problem and get a feasible approach to resolve it.&lt;br&gt;&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; Find the complexity of the Legacy and LDM method.&lt;br&gt;
&lt;strong&gt;Solution:&lt;/strong&gt; Run some benchmarks and find the bottleneck step.&lt;br&gt;&lt;/p&gt;
&lt;p&gt;First I chose the &lt;strong&gt;Legacy&lt;/strong&gt; method because if its simpler architecture. I ran some benchmarks varying the &lt;code class="language-text"&gt;spectral range&lt;/code&gt; of &lt;code class="language-text"&gt;OH&lt;/code&gt; and &lt;code class="language-text"&gt;CO2&lt;/code&gt; molecule to get similar number of lines. I kept parameters like &lt;code class="language-text"&gt;pressure&lt;/code&gt;, &lt;code class="language-text"&gt;temperature&lt;/code&gt;, &lt;code class="language-text"&gt;broadening_max_width&lt;/code&gt;, &lt;code class="language-text"&gt;wstep&lt;/code&gt;, etc constant to see the dependence of Legacy method on &lt;strong&gt;Spectral range&lt;/strong&gt;. &lt;br&gt;&lt;/p&gt;
&lt;p&gt;In order to get similar number of lines, I created a function which will take the &lt;strong&gt;Spectrum Factory&lt;/strong&gt; &lt;code class="language-text"&gt;dataframe&lt;/code&gt; and select the target number of lines. But the issue with Pandas dataframe is that when modify the dataframe there are chances that the metadata will get lost and we will no longer be able to do Spectrum calculation. To avoid this we have to drop the right number of lines with &lt;code class="language-text"&gt;inplace=True&lt;/code&gt;. So we will need to fix the number of lines and then we can proceed ahead with the benchmarking. Every parameter is the same except the Spectral Range.  Full code &lt;a href="https://gist.github.com/anandxkumar/cbe12f47170e1d71a82f4b246bd01dcc"&gt;here&lt;/a&gt;.&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Earlier we assumed that the complexity of Legacy method is: &lt;br&gt;
&lt;strong&gt;&lt;code class="language-text"&gt;Voigt Broadening = Broadening_max_width * spectral_range/math.pow(wstep,2) * N&lt;/code&gt;&lt;/strong&gt; &lt;br&gt;&lt;/p&gt;
&lt;p&gt;Thus I was expecting to have different calculation time for both benchmarks. But to my surprise the computational times were almost equivalent! I re-ran each benchmarks &lt;strong&gt;100 times&lt;/strong&gt; just to be sure and more precise about it. Following were the observations:&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Number of lines - &lt;b&gt;{‚ÄòOH‚Äô: 28143, ‚ÄòCO‚Äô: 26778}&lt;/b&gt;&lt;/li&gt;
&lt;li&gt;Total Calculation time(Avg) -  &lt;b&gt;{‚ÄòOH‚Äô: 4.4087, ‚ÄòCO‚Äô: 3.8404000000000003}&lt;/b&gt;&lt;/li&gt;
&lt;li&gt;Total Voigt_Broadening TIME(Avg) - &lt;b&gt;{‚ÄòOH‚Äô: 3.1428814244270327, ‚ÄòCO‚Äô: 3.081623389720917}&lt;/b&gt;&lt;/li&gt;
&lt;li&gt;spectral_range - &lt;b&gt;{‚ÄòOH‚Äô: 38010, ‚ÄòCO‚Äô: 8010}&lt;/b&gt;&lt;/li&gt;
&lt;li&gt;Legacy_Scale - &lt;b&gt;{‚ÄòOH‚Äô: 4x10^14, ‚ÄòCO‚Äô: 8x10^13}&lt;/b&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are some inference we can make from the above observation:&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A)&lt;/strong&gt; The bottleneck step(Voigt Broadening) loosely depends on &lt;code class="language-text"&gt;Spectral Range&lt;/code&gt;.&lt;br&gt;
&lt;strong&gt;B)&lt;/strong&gt; The complexity of Voigt Broadening needs to be modified because there is a difference of order of &lt;strong&gt;~10&lt;/strong&gt; in the Legacy Scaled value of OH and CO2.&lt;br&gt;&lt;/p&gt;
&lt;p align="center"&gt;
&lt;span class="gatsby-resp-image-wrapper" style="display: block; margin-left: auto; margin-right: auto;"&gt;
&lt;a class="gatsby-resp-image-link" href="https://anandkumar-blog.netlify.app/static/d9b32b4e96e6cd9a91016a49ad940239/0b533/Blog2.png" rel="noopener" style="display: block;" target="_blank"&gt;
&lt;span class="gatsby-resp-image-background-image" style="padding-bottom: 100%; display: block;"&gt;&lt;/span&gt;
&lt;img alt="Blog2" class="gatsby-resp-image-image" src="https://anandkumar-blog.netlify.app/static/d9b32b4e96e6cd9a91016a49ad940239/0b533/Blog2.png" style="width: 100%; height: 100%; margin: 0; vertical-align: middle;" title="Blog2"&gt;
&lt;/a&gt;
&lt;/span&gt;&lt;br&gt;
&lt;b&gt;Credits - Me :p&lt;/b&gt;&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;So in order to do some analysis, we first need data of different steps in the broadening phase and conditions of various Spectrum which brings me to the &lt;strong&gt;Code&lt;/strong&gt; part in &lt;strong&gt;Coding Period.&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Profiler Class&lt;/h3&gt;
&lt;p&gt;The aim of this class is to replace all the print statements by a common &lt;code class="language-text"&gt;start&lt;/code&gt;, &lt;code class="language-text"&gt;stop&lt;/code&gt;, &lt;code class="language-text"&gt;_print&lt;/code&gt; method. Earlier each step computational time was done using &lt;code class="language-text"&gt;time()&lt;/code&gt; library. Now the whole codebase is being refactored with the Profiler class that will do all the work based on the &lt;code class="language-text"&gt;verbose&lt;/code&gt; level. In addition to this the biggest benefit is that each step will be stored in a dictionary with its computational time that will help me gather data to find which step is in actual bottleneck and further which part of the function is the most expensive time wise. A simple example is below:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Before:&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;if __debug__:
t0 = time()
..........
..........
if __debug__:
t1 = time()
.........
.........
if __debug__:
if self.verbose &amp;gt;= 3:
printg("... Initialized vectors in {0:.1f}s".format(t1 - t0))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;After:&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;self.profiler.start(
key="init_vectors", verbose=3, details="Initialized vectors"
)
.........
.........
self.profiler.stop("init_vectors")&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So using a common key we can make it happen. This will be stored in the conditons of &lt;code class="language-text"&gt;Spectrum&lt;/code&gt; object in the &lt;code class="language-text"&gt;'profiler'&lt;/code&gt; key. All these Spectrums and their conditions can be exported using a &lt;a href="https://radis.readthedocs.io/en/latest/spectrum/spectrum.html#spectrum-database"&gt;SpecDatabase&lt;/a&gt;. This will create a csv file comprising of all the parameters of all Spectrums which will be useful in getting some insights.
-&amp;gt; &lt;a href="https://github.com/radis/radis/pull/286"&gt;PR LINK&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Digging in whiting_jit&lt;/h3&gt;
&lt;p&gt;Based on several benchmarks, it is estimated that around &lt;strong&gt;70-80%&lt;/strong&gt; time is spent on calculating the broadening. The broadening part has the following hierarchy:&lt;br&gt;
&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;_calc_broadening()
-&amp;gt; _calc_lineshape()
-&amp;gt; _voigt_broadening()
-&amp;gt; _voigt_lineshape()
-&amp;gt; whiting_jit()&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;On close inspection we observed that &lt;strong&gt;80-90%&lt;/strong&gt; time is spent on &lt;code class="language-text"&gt;whiting_jit&lt;/code&gt; process. Going further down in &lt;code class="language-text"&gt;whiting_jit&lt;/code&gt;, &lt;strong&gt;60-80%&lt;/strong&gt; time is spent on &lt;strong&gt;lineshape calculation.&lt;/strong&gt; Below is the formula:&lt;br&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;lineshape = (
(1 - wl_wv) * exp(-2.772 * w_wv_2)
+ wl_wv * 1 / (1 + 4 * w_wv_2)
# ... 2nd order correction
+ 0.016 * (1 - wl_wv) * wl_wv * (exp(-0.4 * w_wv_225) - 10 / (10 + w_wv_225))
)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The whole process can be divided into 4 parts:&lt;br&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;    part_1 =   (1 - wl_wv) * exp(-2.772 * w_wv_2)

part_2 =    wl_wv * 1 / (1 + 4 * w_wv_2)

# ... 2nd order correction
part_3 =  0.016 * (1 - wl_wv) * wl_wv * exp(-0.4 * w_wv_225)

part_4 =  - 10 / (10 + w_wv_225)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The complexity of each part comes out: &lt;br&gt;
&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;    o1 = broadening__max_width * n_lines / wstep

O(part_1) = n_lines * o1
O(part_2) = n_lines * 4 * o1
O(part_3) = (n_lines)**2 * o1
O(part_4) = o1 &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Running several benchmark showed us that &lt;strong&gt;part_3&lt;/strong&gt; takes the most time out of all steps. So clearly we can see that the complexity of Legacy method is not dependent on
Spectral Range but rather &lt;code class="language-text"&gt;Number of Calculated Lines&lt;/code&gt;,&lt;code class="language-text"&gt;broadening__max_width&lt;/code&gt; and &lt;code class="language-text"&gt;wstep&lt;/code&gt;. It may seem that the complexity of Legacy method is:&lt;br&gt;&lt;/p&gt;
&lt;p align="center"&gt;&lt;b&gt; n_lines^2 * broadening__max_width * n_lines / wstep&lt;/b&gt;&lt;/p&gt; &lt;br&gt;
&lt;p&gt;But inorder to prove this we need more benchmarks and evidence to verify this and it may involve normalization of all steps in lineshape calculation!&lt;br&gt; &lt;/p&gt;
&lt;p&gt;So the goal for the next 2 weeks is clear:&lt;br&gt;
&lt;b&gt;i)&lt;/b&gt; Refactor the entire codebase with Profiler.&lt;br&gt;
&lt;b&gt;ii)&lt;/b&gt; Find the complexity of &lt;strong&gt;Legacy Method&lt;/strong&gt; with the help of more benchmark and analysis.&lt;br&gt;
&lt;b&gt;iii)&lt;/b&gt; Do the same for &lt;strong&gt;LDM Method&lt;/strong&gt;!&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Ok I guess time‚Äôs up! See you after 2 weeks :)&lt;/p&gt;&lt;/div&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210621_2240_anandxkumar/</guid><pubDate>Mon, 21 Jun 2021 21:40:32 GMT</pubDate></item><item><title>About my Google Summer of Code Project: Part 1</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210621_1658_adwaitbhope/</link><dc:creator>Adwait Bhope</dc:creator><description>&lt;div&gt;&lt;p&gt;I had been eyeing Google Summer of Code last year (and the year before that), but never really got around to doing anything about it. It‚Äôs a wonderful learning experience and being in my final year of college this was the last opportunity I was going to get. So I decided to give it a¬†shot.&lt;/p&gt;
&lt;p&gt;I started late, sometime during late February. I picked out a few organizations that looked interesting to me. &lt;a href="https://openastronomy.org/"&gt;openastronomy&lt;/a&gt; particularly caught my eye because I was working on another project of mine related to Astronomy. In fact we were using one of the Python libraries under openastronomy. Now this is an umbrella organization, which means that there are multiple sub-organizations‚Ää‚Äî‚Ääsunpy, astropy, radis, poliastro, and a few more. They‚Äôre used extensively by the scientific community in their research. The project I selected was under sunpy, which is a Python library for solar data analysis. The project is about resampling data to increase or decrease its resolution‚Ää‚Äî‚Äämore on that later. Again, I had recently performed this operation in one of my projects, so it seemed only natural for me to go with this one. I worked on some issues on GitHub and submitted PRs, tried to get a hold of the codebase, put together a proposal, got feedback from the project mentors and friends, and submitted it. After about a month of impatient waiting, I received an email saying that my proposal was accepted! Awesome!&lt;/p&gt;
&lt;p&gt;Now, I plan to continue writing these blogs throughout the project and since this is the first one, let me take a moment to talk about the project. So, there‚Äôs a sunpy-affiliated package called ndcube, which exists to provide users an easier way of handling coordinates. Astronomical data like images taken from cameras are usually stored as n-dimensional arrays. A dimension could represent spatial or temporal axes. In such an array, the pixel coordinates map to some coordinates in the real world. These could be RA and Dec, or in the case of solar data, Helioprojective Latitude and Longitude. Nevertheless, there needs to be a mapping from the pixels to the real world. This is given by the World Coordinate System, which is just a set of (complicated) mathematical transformations. ndcube is a package that correlates the actual data with its transformations in such a way that you can manipulate the data, and the transformations will continue to remain consistent. It can be used with any type of data like images, spectra, timeseries data, and so¬†on.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*MHKTGHeR4F3nHd2gO1ocUw.png"&gt;&lt;figcaption&gt;Open Astronomy and¬†SunPy&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Often, researchers like to upsample or downsample the resolution of their data, perhaps to improve the Signal to Noise Ratio or even just to get their data onto the same grid. My project under Google Summer of Code is exactly this‚Ää‚Äî‚Ääto implement this functionality under ndcube. Luckily, there exists a package called reproject that has a few algorithms implemented already. My job would be to expose this through a succinct API under¬†ndcube.&lt;/p&gt;
&lt;p&gt;So far, my mentors and I have broken down this work and set smaller and more achievable targets to begin with, and I‚Äôve started working on them. Unfortunately, my Community Bonding Period was quite stagnant thanks to my college commitments, but now that they‚Äôre out of the way, I have more time on my hands to devote to the project. I‚Äôll be publishing more blogs about my progress in the coming weeks, hopefully more frequently. Talk to you in the next¬†one!&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=b56e7277046e" width="1"&gt;&lt;/div&gt;</description><category>SunPy</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210621_1658_adwaitbhope/</guid><pubDate>Mon, 21 Jun 2021 15:58:34 GMT</pubDate></item><item><title>GSoC Progress Report? Almost Done!</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210621_1523_dhruv9vats/</link><dc:creator>Dhruv Vats</dc:creator><description>&lt;div&gt;&lt;p&gt;With only 2 weeks into the coding period, it feels good to say that the bulk of the work associated with the 2 milestones is &lt;em&gt;almost &lt;/em&gt;done. Almost because the newly added functionality is yet to be battle-tested and while there is always room for change and improvements, my focus will be shifting from the main objective to the optional objectives.&lt;/p&gt;
&lt;h5&gt;The Project&lt;/h5&gt;&lt;p&gt;Scientific jargon¬†ahead!&lt;/p&gt;
&lt;p&gt;The study and interpretation of time-series data have become an integral part of modern-day astronomical studies and a common approach for characterizing the properties of time-series data is to estimate the power spectrum of the data using the periodogram. But the periodogram as an estimate suffers from¬†being:&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;ol&gt;&lt;li&gt;Statistically inconsistent (that is, its variance does not go to zero as the number of data samples reach infinity),&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;2. Biased for finite samples,¬†and&lt;/p&gt;
&lt;p&gt;3. Suffers from spectral¬†leakage.&lt;/p&gt;
&lt;figure&gt;&lt;a href="https://github.com/StingraySoftware/stingray"&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/700/1*5GHLU2O-S9d06amIH5ESBA.png"&gt;&lt;/a&gt;&lt;figcaption&gt;Stingray is a spectral-timing software package for astrophysical X-ray (and other)¬†data.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;My project aimed at implementing and integrating a superior spectral estimation technique, known as the Multitaper periodogram, into a Software Package called Stingray, a sub-organization of OpenAstronomy.&lt;/p&gt;
&lt;p&gt;This Multitaper algorithm uses windows or tapers (bell-shaped functions), which are multiplied with the time-series data before finding its frequency domain estimate. These windows, called the discrete prolate spheroidal sequences (DPSS), help mitigate the problems mentioned above.&lt;/p&gt;
&lt;p&gt;Any more technical stuff and this blog will start taking the shape of my proposal, so I‚Äôll leave it here, but for anyone more interested, &lt;a href="https://www.dropbox.com/s/g2m2p10en8ygpmz/Proposal.pdf?dl=0"&gt;here&lt;/a&gt; is the proposal.&lt;/p&gt;
&lt;h5&gt;The Process&lt;/h5&gt;&lt;p&gt;I started working on the project in early March, before submitting the proposal, and it initially included writing a wrapper around an external package to make the use of this tool coherent with the rest of the project. While a proof-of-concept implementation was put together, it was later decided to use SciPy to do the grunt work, as it already was a dependency, somewhat changing the initial milestones.&lt;/p&gt;
&lt;p&gt;So in a way, I was already working on the project before the results were announced and ended up opening a pull request with the newly added method 1 week into the coding period and it got merged a week later, which happened to be fairly early in the GSoC program coding timeline. Perks of open-source?!&lt;/p&gt;
&lt;p&gt;So while there are sure to be improvements and additions in terms of coherency and features, this does give a bit more breathing room and time for experimentation and exploration, and I‚Äôll try and make good use of it, primarily by working on the optional but quite good-to-have features.&lt;/p&gt;
&lt;p&gt;For anyone interested, &lt;a href="https://github.com/StingraySoftware/stingray/pull/578"&gt;here&lt;/a&gt; is the¬†PR.&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=6239f301b23" width="1"&gt;&lt;/div&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210621_1523_dhruv9vats/</guid><pubDate>Mon, 21 Jun 2021 14:23:27 GMT</pubDate></item><item><title>The first weeks of X-Rays and Electron.Js</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210620_1829_rachittshah/</link><dc:creator>Rachitt Shah</dc:creator><description>&lt;div&gt;&lt;p&gt;Python, X-rays, and JSON files that don‚Äôt comply and make you¬†cry.&lt;/p&gt;
&lt;p&gt;GSoC has officially started!&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/498/0*AXMzJYLpLP0sFPgE"&gt;&lt;figcaption&gt;We‚Äôre going to¬†code!&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Since is a Desktop App made with Electron and Python, my first priority was getting DAVE to compile again. The process was tedious and took a good amount of time to fix the flask backends which power¬†DAVE.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;Another error that came while getting DAVE up was that DAVE uses Stingray‚Äôs APIs which are lower than 2.0, which gave a lot of broken packages. Fixing this was something that would be covered in the POC enhancements, however, it was important to tackle this to build the¬†MVP.&lt;/p&gt;
&lt;p&gt;Looked at some major changes in the coming 0.3 for Stingray and how to accommodate them, so we don‚Äôt have issues¬†later.&lt;/p&gt;
&lt;p&gt;Had exams alongside, so the first week was less effort extensive for¬†GSoC.&lt;/p&gt;
&lt;p&gt;Waiting for the next week to write more about my progress and¬†journey!&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=84f7557d97a7" width="1"&gt;&lt;/div&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210620_1829_rachittshah/</guid><pubDate>Sun, 20 Jun 2021 17:29:09 GMT</pubDate></item><item><title>GSoC Post 1</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210620_1617_ndanzanello/</link><dc:creator>ndanzanello</dc:creator><description>&lt;div&gt;&lt;p&gt;Hey there. I am working on the Astrometry project from Gnuastro and I will explain below the first things that I have been doing.&lt;br&gt;&lt;br&gt;Basically, we have two catalogs: one is the query catalog, which we want to find its wcs, and the reference catalog, that gives some stars positions in celestial coordinates. We begin finding ‚Äúquads‚Äù, a group of 4 stars, on both catalogs. This part was already done, but the matching part between the quads needed some fixes.&lt;/p&gt;


&lt;!-- TEASER_END --&gt;

&lt;p&gt;The first thing we needed to fix was the vertices found on each catalog. It‚Äôs very important that all the vertices are labeled the same. First, we label the A and B vertices as the most separated ones. In the query catalog it‚Äôs just the Euclidean distance between the points, but on the reference catalog we have to use the angular distance between the points to get the same vertices. Prior to that, it was also using the Euclidean distance to the vertices on the reference catalog, so it would give different most separated A and B for the two catalogs.&lt;br&gt;After that, we have to choose the C and D vertices. First we label randomly the two remaining vertices as C and D and then we compare the ACB and ADB angles that are less than 180 degrees and choose C to be the one that has the lesser angle.&lt;/p&gt;



&lt;p&gt;Now, we have the A, B, C and D vertices to be the same when dealing with the same quads and we have to compute their hashes. The hashes were calculated using Cx = (c1-a1)/(b1-a1), where a1, b1 and c1 are the coordinates along the axis 1. Now we have the problem related to the rotations: the distance between the points is the same, but the distance along each axis is not the same! So the Cx would be different for different axis. The same would happen for Cy, Dx and Dy.&lt;/p&gt;



&lt;p&gt;To solve this, first we transform the celestial coordinates of the reference catalog into projection plane coordinates (TAN projection) using the midpoint of AB as the coordinates of the native pole.&lt;br&gt; We proceed defining new two axis (x and y, where the hashes will be calculated) using the A-B vector as a 45 degrees line contained in these axis. Then, we project the C-A and D-A vectors in these axis and get the hashes.&lt;/p&gt;



&lt;p&gt;The image below show an overview of the steps explained above.&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="" class="wp-image-56" src="https://ndanzanello.files.wordpress.com/2021/06/match_overview.png?w=1024"&gt;&lt;/figure&gt;



&lt;p&gt;&lt;/p&gt;



&lt;p&gt;&lt;/p&gt;&lt;/div&gt;</description><category>gnuastro</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210620_1617_ndanzanello/</guid><pubDate>Sun, 20 Jun 2021 15:17:45 GMT</pubDate></item><item><title>GSoC - 1</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210620_0300_gagan-aryan/</link><dc:creator>Gagan Aryan</dc:creator><description>&lt;div&gt;&lt;p&gt;This is the first blog that documents the coding period of my GSoC21 journey. I learnt a few interesting things in these two weeks, as I expected I would. So, let‚Äôs dive in and see if you knew few of these stuff I learnt.&lt;/p&gt;
&lt;h3 id="starting-off-"&gt;Starting off !!!&lt;/h3&gt;
&lt;p&gt;I started off by getting a brief idea of the scope of the changes that could be done to the dataframe. This was the task I had decided on for the first week. Whenever we are involved in a project that runs for a period of anywhere between 2-4 months it is important to have a timeline or a roadmap of sorts to be able to look back to. This doesn‚Äôt really have to be something rigid. We can chose to deviate from it and infact deviations are bound to happen due to multiple reasons. It can happen because of an unexpected bug in between, or because you came across some alternative that you did not consider at the start or simply because it is one of those projects that gives better insights as you dwell into it.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;Every good GSoC proposal consists of a tentative timeline that depicts the work we plan on doing as the weeks progress. Here is the timeline I had submitted in my proposal.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Timeline1" src="https://gagan-aryan.netlify.app/images/gsoc-1/timeline1.png"&gt;&lt;br&gt;
&lt;img alt="Timeline2" src="https://gagan-aryan.netlify.app/images/gsoc-1/timeline2.png"&gt;&lt;/p&gt;
&lt;p&gt;So as per this I was supposed to finish off the refactors to the dataframe and also finish setting up the benchamarks. But I was unable to complete these. I had underestimated the work it would take to complete them. Nonetheless, I also did have some time to look up at the things I am supposed to do in the second half of the coding period.&lt;/p&gt;
&lt;h3 id="memory-and-time-performance-benchmarks---tic-tok"&gt;Memory and Time performance benchmarks - Tic-Tok&lt;/h3&gt;
&lt;p&gt;Before making any changes to the codebase Erwan suggested me to have the benchmarks setup. So what do I mean by this? To make sure that the changes I am making to the code are indeed reducing the memory consumption of the computations we use a few tools that help us track the memory consumption for various calculations as a function of git commits. There are multiple tools that help us do this. Radis already used a tool developed by &lt;a href="https://github.com/airspeed-velocity/asv"&gt;airspeed velocity&lt;/a&gt; to track the memory computions. I ran into a lot of troubles in setting these up and a lost a lot of valuable time in the process ultimately Erwan fixed it and I was able to run the benchmarks on my machine.&lt;/p&gt;
&lt;p&gt;The benchmarks still seem to take a lot of time to run though and for them to be feasible to be used a tool through which I can check the performance regularly there are a few things I need to learn. I hope to pick these up in the next few days.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Performance" src="https://gagan-aryan.netlify.app/images/gsoc-1/brace-yourselves.png"&gt;&lt;/p&gt;
&lt;p&gt;We are also trying to look at a few other alternatives that can be used instead of asv. I will update the you guys regarding this in the next blog post.&lt;/p&gt;
&lt;h3 id="oh-pandas-here-i-deal-with-you-"&gt;Oh Pandas here I deal with you !&lt;/h3&gt;
&lt;h4 id="lets-ditch-a-few-columns"&gt;Let‚Äôs ditch a few columns&lt;/h4&gt;
&lt;p&gt;We can reduce the memory usage of pandas by using one really simple trick - avoid giving loading the columns that are not required for computation. Below I demostrate how just dropping a few columns can provide significant improvement in the memory consumption. I am using &lt;code&gt;HITEMP-CH4&lt;/code&gt; database for demonstration.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;div class="chroma"&gt;
&lt;table class="lntable"&gt;&lt;tr&gt;&lt;td class="lntd"&gt;
&lt;pre class="chroma"&gt;&lt;code&gt;&lt;span class="lnt"&gt; 1
&lt;/span&gt;&lt;span class="lnt"&gt; 2
&lt;/span&gt;&lt;span class="lnt"&gt; 3
&lt;/span&gt;&lt;span class="lnt"&gt; 4
&lt;/span&gt;&lt;span class="lnt"&gt; 5
&lt;/span&gt;&lt;span class="lnt"&gt; 6
&lt;/span&gt;&lt;span class="lnt"&gt; 7
&lt;/span&gt;&lt;span class="lnt"&gt; 8
&lt;/span&gt;&lt;span class="lnt"&gt; 9
&lt;/span&gt;&lt;span class="lnt"&gt;10
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class="lntd"&gt;
&lt;pre class="chroma"&gt;&lt;code class="language-python"&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;radis.io.hitran&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;hit2df&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hit2df&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"06_HITEMP2020_2000.0-2500.0.par"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;memory_usage&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"deep"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="n"&gt;memory&lt;/span&gt; &lt;span class="n"&gt;usage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;30.5&lt;/span&gt; &lt;span class="n"&gt;MB&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s2"&gt;"id"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"iso"&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;inplace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;memory_usage&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"deep"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="n"&gt;memory&lt;/span&gt; &lt;span class="n"&gt;usage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;25.4&lt;/span&gt; &lt;span class="n"&gt;MB&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;The peak memory usage before dropping the columns was 30.5 MB and once I remove a few columns the peak memory usage becomes 25.4 MB. I have already implemented the dropping of id column and handled the case of single isotope as well by dropping the column and istead just storing the information of the isotope as a meta attribute. We have also finalised on the discarding of the other columns by considering the physics of these quantities. Let‚Äôs check out a few of them. Since I haven‚Äôt already implemented the optimisations that follow I will save the implementation details for the next blog.&lt;/p&gt;
&lt;h4 id="einsteins-coeffecients-and-linestrengths"&gt;Einstein‚Äôs Coeffecients and Linestrengths&lt;/h4&gt;
&lt;p&gt;There are four parameters of interest to describe the intensity of a line : Linestrength $(int)$, Einstein emission coefficient $(A)$ and Einstein absorption coefificent $(B_{lu})$, Einstein induced emission coefficient $(B_{ul})$. All of them are somehow linked to the Squared Transition Dipole Moment $(R)$. &lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;$$ B_{lu}=10^{-36}\cdot\frac{8{\pi}^3}{3h^2} R_s^2 \cdot 10^{-7} $$&lt;br&gt;
$$ B_{ul}=10^{-36}\cdot\frac{8{\pi}^3}{3h^2} \frac{gl}{gu} R_s^2 \cdot 10^{-7} $$&lt;br&gt;
$$ A_{ul}=10^{-36}\cdot\frac{\frac{64{\pi}^4}{3h} {\nu}^3 gl}{gu} R_s^2 $$&lt;/p&gt;
&lt;p&gt;So now the idea would be to drop the $int$ column and use $A_{ul}$ to calculate the value of $int$ from it. The reason to drop $int$ and not $A_{ul}$ some databases like &lt;code&gt;ExoMol&lt;/code&gt; databases only provide the value of $A_{ul}$.&lt;/p&gt;
&lt;h4 id="concat-better"&gt;Concat better&lt;/h4&gt;
&lt;p&gt;For anyone who wants concate multiple datafiles pandas tends to become useless as the memory scales up. I started out experimenting concat operations inorder to cluster the isotopes of each type, run computations on them and later concat them. But I later learnt that since this data is already in the form of a single dataframe, indexing is a better parameter to track the memory consumption. Nonetheless there are a few other places in Radis where we process multiple files and concat them, hence this experiment would help us decide how we can chose to replace the current approach with a better one. I tried out three methods. I was using some random dummy datafiles of around 780 MBs.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Normal &lt;code&gt;pandas.concat&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Concat with a doubly ended queue&lt;/li&gt;
&lt;li&gt;Concat with parquet&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here are the results of each of these methods -&lt;/p&gt;
&lt;div class="tab" id="cbbfde2b70afc7e2"&gt;
&lt;div class="tab__links"&gt;
&lt;button class="tab__link"&gt;pandas.concat&lt;/button&gt;
&lt;button class="tab__link"&gt;deque&lt;/button&gt;
&lt;button class="tab__link"&gt;parquet&lt;/button&gt;
&lt;/div&gt;
&lt;div class="tab__content" id="c6b1e010ed52c9f8"&gt;
&lt;h4 id="pandasconcat"&gt;pandas.concat&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;CPU Time - 0:02:43.797588
Peak Memory Usage - 4.1050 GB
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="tab__content" id="a412292f0cf68165"&gt;
&lt;h4 id="pandasconcat-with-a-doubly-queue"&gt;pandas.concat with a doubly queue&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;CPU Time - 0:02:34.484612
Peak Memory Usage - 3.7725 GB
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="tab__content" id="c655ff276a8106d7"&gt;
&lt;h4 id="concat-with-parquet"&gt;Concat with parquet&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;CPU Time - 0:01:37.984875
Peak Memory Usage - 1.6829 GB
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Looking at the results, parquet seems like a really good option to me. But we will run for a few more examples and later check which one suits the best.&lt;/p&gt;
&lt;h3 id="the-next-two-weeks"&gt;The next two weeks&lt;/h3&gt;
&lt;p&gt;The project is making progress in all fronts. I feel I need to reorganize my thoughts a bit. My main work for now would be to complete the task list of &lt;a href="https://github.com/radis/radis/pull/287"&gt;this pr&lt;/a&gt;. And then look at other stuff.&lt;/p&gt;
&lt;div class="footnotes"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;&lt;a href="https://www.sciencedirect.com/science/article/pii/S0022407398000788?via%3Dihub"&gt;Rothmann Paper (Eqs.(A7), (A8), (A9)&lt;/a&gt; &lt;a class="footnote-backref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fnref:1"&gt;‚Ü©Ô∏é&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210620_0300_gagan-aryan/</guid><pubDate>Sun, 20 Jun 2021 02:00:06 GMT</pubDate></item><item><title>A Summer of Coding and Astronomy‚Ää‚Äî‚ÄäGSoC‚Äô21 at OpenAstronomy</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210607_1403_rachittshah/</link><dc:creator>Rachitt Shah</dc:creator><description>&lt;div&gt;&lt;h4&gt;A Summer of Coding and Astronomy‚Ää‚Äî‚ÄäGSoC‚Äô21 at OpenAstronomy&lt;/h4&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/400/0*5kDNG2NTio40EKZh.gif"&gt;&lt;/figure&gt;&lt;h4&gt;‚ÄúIn real open source, you have the right to control your own destiny.‚Äù&lt;/h4&gt;&lt;p&gt;‚Äî Linus¬†Torvalds&lt;/p&gt;
&lt;p&gt;Google Summer of Code is probably the most notable and interesting programs a student can be a part of an undergrad can be a part of. From reading GSoC blogs to write my story, it feels unnatural.&lt;/p&gt;
&lt;p&gt;This post is more of an introduction to GSoC and my project at OpenAstronomy, I‚Äôll be covering my prep and journey later¬†on.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;h4&gt;About my GSoC¬†project!&lt;/h4&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/0*VCNaN41xLMATNTgC"&gt;&lt;figcaption&gt;The DAVE¬†engine!&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;Generating periodograms for astronomical data is the core task of Stingray. Because periodograms are often noisy, several methods to denoise periodograms exist in the literature, among them the multi-taper periodogram Stingray aims to provide a comprehensive library of reliable, well-tested implementations of common algorithms for time series analysis in Astronomy. DAVE is an elegant GUI to the library, developed during a previous¬†GSoC.&lt;/li&gt;&lt;li&gt;Due to the fast-evolving Python and Javascript landscape, this GUI is not compatible with the current versions of the dependencies. Also, Stingray has now new features that were not implemented in the original¬†GUI.&lt;/li&gt;&lt;li&gt;In this project, I would be refreshing the GUI dependencies, update the package building infrastructure, and add the new functionality introduced in recent versions of Stingray.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;I would be making sure DAVE would be up and running again over the summer, couldn‚Äôt have been¬†happier!&lt;/p&gt;
&lt;h4&gt;About me&lt;/h4&gt;&lt;p&gt;I‚Äôm Rachitt Shah, a second-year undergrad at VIT Pune. I‚Äôm also doing Google Season of Docs with the STE||AR group for their project¬†HPX.&lt;/p&gt;
&lt;p&gt;I‚Äôm a growth associate at gradCapital, a student-centric VC fund that aims to help student startups.&lt;/p&gt;
&lt;p&gt;I love product management, tech(of course!) and venture capital.&lt;br&gt;Here‚Äôs my social profiles link¬†-&lt;/p&gt;
&lt;p&gt;&lt;a href="https://twitter.com/rachittshah"&gt;https://twitter.com/rachittshah&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.linkedin.com/in/rachitt-shah/"&gt;https://www.linkedin.com/in/rachitt-shah/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you have any questions about open-source and dev, don‚Äôt hesitate to reach out. &lt;br&gt;May the source be with¬†you!&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=a7d716df482e" width="1"&gt;&lt;/div&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210607_1403_rachittshah/</guid><pubDate>Mon, 07 Jun 2021 13:03:09 GMT</pubDate></item><item><title>Chapter 0: Prologue</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210606_2240_anandxkumar/</link><dc:creator>anandxkumar</dc:creator><description>&lt;div&gt;&lt;p&gt;Hi There and Namaste! This is going to be the second blog and first blog related to GSoC where I will be sharing my experience Community Bonding Period Experience with &lt;b&gt;Radis&lt;/b&gt;. Before moving ahead lets learn about GSoC and my perspective about it.&lt;/p&gt;
&lt;h3&gt;Google Summer of Code&lt;/h3&gt;
&lt;p&gt;GSoC or the way I like to say it &lt;strong&gt;(Great Summer Opportunity to Code ;)&lt;/strong&gt; is a program conducted and funded by Google to promote college students around the world to engage with Open Source Community and contribute to the organization for a tenure of 3 months. In the process, code is created and released for the world to see and use. But the main aim of GSoC is to promote students to stick to the organizations and help to grow the Open Source Community. This is a great initiative by Google that brings thousands of students every year and help them get an opportunity to peek into the world of open source development, learn new skills and also get compensated for the work, quite generously.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;I remember during second year of my college, it was around end of March and my roommate was applying for GSoC and I was like what is this program? There I got to know about it but since the deadline was near I was afraid of doing all the stuffs in a week of time, so I didn‚Äôt apply for it. Fast forwarding to next year, I was prepared enough this time and I feel priviledged to be a part of GSoC as part of OpenAstronomy. &lt;/p&gt;
&lt;h3&gt;My GSoC Project&lt;/h3&gt;
&lt;p&gt;I‚Äôm part of &lt;b&gt;&lt;a href="https://github.com/radis/radis"&gt;Radis&lt;/a&gt;&lt;/b&gt; organization which is a sub-org of &lt;a href="https://github.com/OpenAstronomy"&gt;OpenAstronomy&lt;/a&gt;. Radis is a fast line-by-line code used to synthesize high-resolution infrared molecular
spectra and a post-processing library to analyze spectral lines. It can synthesize absorption
and emission spectrum for multiple molecular species at both equilibrium and
non-equilibrium conditions.&lt;br&gt;
Radis computes every spectral line (absorption/emission) from the molecule considering
the effect of parameters like Temperature, Pressure. Due to these parameters, we don‚Äôt get
a discrete line but rather a shape with a width. This is called line broadening and for any spectral synthesis code, this is the bottleneck step. Ok let us C what my GSoC project is all about! &lt;br&gt;&lt;/p&gt;
&lt;p&gt;Radis has 2 methods to calculate the lineshape of lines.&lt;br&gt;
‚óè Legacy Method&lt;br&gt;
‚óè DLM Method&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The goal of this project is to derive an equation comprising all parameters that affect the
performance for calculating Voigt broadening by running several benchmarks for different
parameters involved in the calculation of lineshapes to check their significance in
computation time. Then we need to find the critical value for the derived equation (&lt;code class="language-text"&gt;Rc&lt;/code&gt;)
which will tell us which optimization technique to select based on the computed &lt;code class="language-text"&gt;R&lt;/code&gt; value in
&lt;b&gt;calc_spectrum()&lt;/b&gt;. An &lt;code class="language-text"&gt;optimization = "auto"&lt;/code&gt; will be added that will choose the best method based on the parameters provided.&lt;/p&gt;
&lt;h3&gt;Community Bonding Period&lt;/h3&gt;
&lt;p&gt;The first phase of GSoC is the &lt;b&gt;Community Bonding Period&lt;/b&gt; which is a 3 weeks long period. Its main aim is allow the student to get familiar with the community and the codebase. It serves as a warm-up period before the coding period. The first thing I did was that I went though the original Radis &lt;a href="https://www.sciencedirect.com/science/article/abs/pii/S0022407318305867?via%3Dihub"&gt;paper&lt;/a&gt; and also the DLM implementation &lt;a href="https://ui.adsabs.harvard.edu/abs/2021JQSRT.26107476V/abstract"&gt;paper&lt;/a&gt; because our project objective is based on these 2 implementations. It helped me understand the main purpose of RADIS, its architecture and the science behind different steps of both equilibrium and non-equilibrium spectrum, though I have to accept these papers are way too technical for me :p (Complex Spectroscopy related formulas).&lt;br&gt; I believed inorder to get myself ready for the coding period, I shall focus on solving some related issues to make me more familiar with the codebase.&lt;br&gt;&lt;/p&gt;
&lt;p&gt;In order to compute any spectrum we need to determine several parameters like: minimum-maximum wavenumber, molecule, Temperature of gas, mole fraction, wstep, etc.&lt;br&gt;
&lt;code class="language-text"&gt;wstep&lt;/code&gt; determines the wavenumber grid‚Äôs resolution. Smaller the value, higher the resolution and vice-versa. By default radis uses &lt;code class="language-text"&gt;wstep=0.01&lt;/code&gt;. You can manually set the wstep value in &lt;b&gt;calc_spectrum()&lt;/b&gt; and &lt;strong&gt;SpectrumFactory&lt;/strong&gt;. To get more accurate result you can further reduce the value, and to increase the performance you can increase the value.&lt;/p&gt;
&lt;p&gt;Based on wstep, it will determine the number of gridpoints per linewidth. To make sure that there are enough gridpoints, Radis will raise an &lt;strong&gt;Accuracy Warning&lt;/strong&gt;. If number of gridpoints are less than &lt;code class="language-text"&gt;GRIDPOINTS_PER_LINEWIDTH_WARN_THRESHOLD&lt;/code&gt; and raises an &lt;strong&gt;Accuracy Error&lt;/strong&gt; if number of gridpoints are less than &lt;code class="language-text"&gt;GRIDPOINTS_PER_LINEWIDTH_ERROR_THRESHOLD&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;So inorder to select the optimum value of &lt;code class="language-text"&gt;wstep&lt;/code&gt; I had to refactor the codebase such that we could compute the minimum FWHM (&lt;code class="language-text"&gt;min_width&lt;/code&gt;) value after calculating the HWHM of all lines and and set &lt;code class="language-text"&gt;wstep = min_width / GRIDPOINTS_PER_LINEWIDTH_WARN_THRESHOLD&lt;/code&gt;. All &lt;code class="language-text"&gt;wstep&lt;/code&gt; dependent parameters had to be refactored to make sure they are not being called before the calculating &lt;code class="language-text"&gt;min_width&lt;/code&gt;. At the end this feature was successfully merged in the develop branch of Radis and now users can use &lt;code class="language-text"&gt;wstep = "auto"&lt;/code&gt; to automatically get the optimal value of &lt;code class="language-text"&gt;wstep&lt;/code&gt;. This feature will be available from version &lt;b&gt;0.9.30&lt;/b&gt;. Here is the &lt;a href="https://github.com/radis/radis/pull/271"&gt;link&lt;/a&gt; of the merged PR.&lt;/p&gt;
&lt;p&gt;In short, the Community Bonding Period has been great and I have learned alot about Radis during this time. In the next 2 weeks I will be focussing on building a benchmarking framework and run various benchmarks for both Legacy and DLM method and determine the most influential paramters for performance.&lt;/p&gt;
&lt;p&gt;I‚Äôm very excited for the upcoming months. I know that this summer is going to be a life long experience and I‚Äôm really looking forward to do amazing things for the community and want to thank Google, OpenAstronomy, Radis and my mentors &lt;a href="https://github.com/erwanp"&gt;Erwan Pannier&lt;/a&gt;, &lt;a href="https://github.com/dcmvdbekerom"&gt;Dirk van den Bekerom&lt;/a&gt; and &lt;a href="https://github.com/pkj-m"&gt;Pankaj Mishra&lt;/a&gt; for this opportunity.
I‚Äôm ready for this amazing adventure.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;b&gt;LETS DO THIS&lt;/b&gt;&lt;br&gt;
&lt;img src="https://anandkumar-blog.netlify.app/2b4e6a4b663f4bc49d559484b8dd37b1/Start.gif"&gt;&lt;br&gt;
ps: Am a huge Spiderman Fan :p
&lt;/p&gt;&lt;/div&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210606_2240_anandxkumar/</guid><pubDate>Sun, 06 Jun 2021 21:40:32 GMT</pubDate></item><item><title>astropy@GSoC Blog Post #1</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210606_1659_suyog7130/</link><dc:creator>Suyog Garg</dc:creator><description>&lt;div&gt;&lt;div&gt;Hey there,&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;How are you?&lt;/div&gt;&lt;div&gt;Chances are that you are coming across me for the first time.&lt;/div&gt;&lt;div&gt;Nice meeting you too! üòÑ&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;  Since this is an introductory astropy@GSoC Blog Post, I would keep things   brief.&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;  &lt;br&gt;  &lt;div class="separator" style="clear: both; text-align: center;"&gt;    &lt;img alt="" height="336" src="https://lh3.googleusercontent.com/-kVduvrsYzQ4/YL0ClAcy0hI/AAAAAAAA3Yk/PrbBQBkgxu8Y_f7-qpLAPlI6tr0zISXFgCLcBGAsYHQ/w336-h336/image.png" width="336"&gt;&lt;div style="margin-left: 1em; margin-right: 1em;"&gt;¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†¬† ¬†&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://lh3.googleusercontent.com/-OkQcow92n4s/YL0DELFw56I/AAAAAAAA3Y0/43e2Ak8Bsy8VaTSw6RYB3ryocKQUCnM1ACLcBGAsYHQ/image.png" style="margin-left: 1em; margin-right: 1em; text-align: center;"&gt;&lt;img alt="" height="154" src="https://lh3.googleusercontent.com/-OkQcow92n4s/YL0DELFw56I/AAAAAAAA3Y0/43e2Ak8Bsy8VaTSw6RYB3ryocKQUCnM1ACLcBGAsYHQ/image.png" width="320"&gt;&lt;/a&gt;&lt;img alt="" height="169" src="https://lh3.googleusercontent.com/-qshucfTcxpY/YL0CwsGOS2I/AAAAAAAA3Yo/OGuKkhEkZtkmz3xw6qVy3YYTANGN5Zi2gCLcBGAsYHQ/w169-h169/image.png" style="text-align: center;" width="169"&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;As you probably already know, my name is Suyog and I am a participant for Google Summer of Code (GSoC) 2021. Over the course of the next 10 weeks or so, I will be working on the Astropy project under the umbrella organisation OpenAstronomy. During this while, I aim to add a CDS format writer to the Astropy library with the help of my affable mentors Aarya and Moritz.¬† ¬†&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;I had actually also applied for GSoC last summer, however I had failed to pass   one of the eligibility criteria, and so wasn't selected. This astropy@GSoC   project, therefore, is quite an awesome opportunity for me. I am looking   forward to making the most of it and enjoying the time all the same. &lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;There are two preliminary observations:&lt;/div&gt;&lt;div&gt;  ¬† ¬† 1. The associated stipend, albeit somewhat lower than what used   to be the case a few years back, is freaking awesome. üòâ &lt;/div&gt;&lt;div&gt;  ¬† ¬† 2. Dunno, why this project is marked as Difficultly Low!?   Nothing as easy as being just the third person to write a Table writer for the   world's largest Astronomy code library! üòÇüòé &lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Alright. Bye.&lt;/div&gt;&lt;div&gt;See ya the next time! üôã‚Äç‚ôÇÔ∏è&lt;/div&gt;&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;  Stay tuned for more GSoC updates, or what is far better, for the next post in   general. &lt;/div&gt;
&lt;!-- TEASER_END --&gt;&lt;/div&gt;</description><category>Astropy</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210606_1659_suyog7130/</guid><pubDate>Sun, 06 Jun 2021 15:59:00 GMT</pubDate></item></channel></rss>