<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Universe OpenAstronomy (Posts by Raj Rashmi)</title><link>http://openastronomy.org/Universe_OA/</link><description></description><atom:link href="http://openastronomy.org/Universe_OA/authors/raj-rashmi.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Sat, 21 Jun 2025 02:00:07 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Google Summer of Code- Final Evaluation</title><link>http://openastronomy.org/Universe_OA/posts/2021/08/20210823_1027_rashmiraj137/</link><dc:creator>Raj Rashmi</dc:creator><description>&lt;h4&gt;&lt;strong&gt;Topic: Implement JAX based automatic differentiation to Stingray&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;The project involved the study of modern statistical modelling to augment the accuracy, speed, and robustness of the likelihood function, into a software package called Stingray. This report demonstrates the experiment done for a combination of different optimizers to fit the scipy.optimize function. Another emphasis is to investigate the gradient calculation using JAX and compare it with scipy.optimize.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Introduction:&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;The proposed milestone was to investigate the room for improvement to enhance the overall performance of modelling to Stingray, using JAX. However, the current stage of the model is still a sandbox model. Stingray is astrophysical spectral timing software, a library in python built to perform time series analysis and related tasks on astronomical light curves. JAX is a python library designed for high-performance numerical computing. Its API for numerical functions is based on NumPy, a collection of functions used in scientific computing. Both Python and NumPy are widely used and familiar, making JAX simple, flexible, and easy to adopt. It can differentiate through a large subset of python’s features, including loops, ifs, recursion, and closures, and it can even take derivatives of derivatives. Such modern differentiation packages deploy a broad range of computational techniques to improve applicability, run time, and memory management.&lt;/p&gt;
&lt;p&gt;JAX utilizes the grad function transformation to convert a function into a function that returns the original function’s gradient, just like Autograd. Beyond that, JAX offers a function transformation jit for just-in-time compilation of existing functions and vmap and pmap for vectorization and parallelization, respectively.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;h4&gt;Experiment:&lt;/h4&gt;&lt;p&gt;The powerlaw and lorentzian function are the most used to describe periodograms in astronomy. In practice, we use the sum of these components to design a realistic model. For the analysis here we consider a quasi-periodic oscillation and a constant and try to fail the algorithm by, (i) reduce the amplitude, (ii) start the optimization process with parameters very far away from the true parameters, (iii) try different optimizers to experiment on different sensitive aspect of the current likelihood calculation. The current ongoing milestone is to try alternatives of scipy.optimize but this requires series of tests for the same.&lt;/p&gt;
&lt;p&gt;The above tests can be visualized in the notebook added on Github: &lt;a href="https://github.com/rashmiraj137/GSoC-Project"&gt;https://github.com/rashmiraj137/GSoC-Project&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;Results:&lt;/h4&gt;&lt;p&gt;During the experiment, it was observed that the algorithm is sensitive to input parameters i.e., it fails for couple of combinations like when the amplitude is set far away from true value, precisely blow absolute value of 1. In general, if we set the parameters very far away from the true value, it fails to approximate the likelihood function. In the notebook (&lt;a href="https://github.com/rashmiraj137/GSoC-Project/blob/main/GSoC_Evaluation%20Notebook.ipynb"&gt;link&lt;/a&gt;), we demonstrate the room for improvement in the current algorithm by choosing a different set of parameters. The current data fit for the evaluation of likelihood happens using scipy.optimize.minimize function. However, there exists numerous ways to do this. SciPy optimize provides functions for minimizing (or maximizing) objective functions, possibly subject to constraints. It includes solvers for nonlinear problems (with support for both local and global optimization algorithms), linear programming, constrained and nonlinear least-squares, root finding, and curve fitting. The problem with the current minimization algorithm is that it converges at local minimum instead of global, i.e. it is not very robust. Recently, Machine Learning has evident development in such optimization tools. The strategy was to find alternatives that potentially accelerate the code, makes it robust.&lt;/p&gt;
&lt;p&gt;For this experiment case, we choose a couple of optimizers and compare the robustness to Powell (the current optimizer used). So we visualize the fit for couple of optimizers like :&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-neldermead.html"&gt;minimize(method=’Nelder-Mead’)&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-powell.html"&gt;minimize(method=’Powell’)&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-cg.html"&gt;minimize(method=’CG’)&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-bfgs.html"&gt;minimize(method=’BFGS’)&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-newtoncg.html"&gt;minimize(method=’Newton-CG’)&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-lbfgsb.html"&gt;minimize(method=’L-BFGS-B’)&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-tnc.html"&gt;minimize(method=’TNC’)&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-cobyla.html"&gt;minimize(method=’COBYLA’)&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-slsqp.html"&gt;minimize(method=’SLSQP’)&lt;/a&gt;&lt;/li&gt;&lt;li&gt;minimize(method = “trust-constr”)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This notebook (&lt;a href="https://github.com/rashmiraj137/GSoC-Project/blob/main/GSoC_Evaluation%20Notebook.ipynb"&gt;link&lt;/a&gt;) has results using each method and it was observed that Nelder-Mead is more robust as compared to other optimizers. Another optimizer like dogleg, trust-ncg, might be good as well, but the jacobian and hess need to be calculated for them.&lt;/p&gt;
&lt;h4&gt;Future Work:&lt;/h4&gt;&lt;p&gt;JAX has now its own version of scipy.optimize.minimize but it has couple of bugs and is not as robust as scipy.optimize.minimize. Finding an alternative for scipy.optimize.minimize such that it doesn’t fails even if the start parameters are far away from the true parameters was a goal for this project but unfortunately JAX did not assist that well enough. But there might be a superior algorithm to scipy.optimize.minimize that can be useful.&lt;/p&gt;
&lt;h4&gt;Repositories:&lt;/h4&gt;&lt;p&gt;&lt;a href="https://github.com/StingraySoftware/stingray"&gt;https://github.com/StingraySoftware/stingray&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/StingraySoftware/notebooks"&gt;GitHub - StingraySoftware/notebooks: Tutorial notebooks for Stingray&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;Blog Post:&lt;/h4&gt;&lt;p&gt;1. &lt;a href="https://raj-rashmi741.medium.com/jax-based-automatic-differentiation-introduction-of-modern-statistical-modeling-to-stingray-1bc26da7571f"&gt;JAX-based automatic differentiation: Introduction of modern statistical modeling to Stingray.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;2. Insight of Implementation of JAX to stingray- GSoC coding period!&lt;/p&gt;
&lt;p&gt;3. &lt;a href="https://raj-rashmi741.medium.com/gsoc-update-2d16a70cc267?source=your_stories_page-------------------------------------"&gt;GSoC update!&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://raj-rashmi741.medium.com/time-to-review-my-gsoc-project-c34297f2dc81"&gt;4. Time to review my GSoC Project.&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;Profiles:&lt;/h4&gt;&lt;p&gt;GitHub: &lt;a href="https://github.com/rashmiraj137"&gt;https://github.com/rashmiraj137&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;LinkedIn: &lt;a href="https://www.linkedin.com/in/rashmi-raj-4b8a2b106/"&gt;https://www.linkedin.com/in/rashmi-raj-4b8a2b106/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Medium: &lt;a href="https://raj-rashmi741.medium.com/"&gt;https://raj-rashmi741.medium.com/&lt;/a&gt;&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=578c0088bcbd" width="1"&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2021/08/20210823_1027_rashmiraj137/</guid><pubDate>Mon, 23 Aug 2021 09:27:57 GMT</pubDate></item><item><title>Time to review my GSoC Project</title><link>http://openastronomy.org/Universe_OA/posts/2021/08/20210816_1603_rashmiraj137/</link><dc:creator>Raj Rashmi</dc:creator><description>&lt;p&gt;With the end of the GSoC project, I will give this blog to summarise the JAX based optimization to analyze its applicability to enhance the loglikelihood calculation. The goal is to analyze, (i) the performance of different optimizers to evaluate the loglikelihood function, (ii) demonstrated the robustness of JAX to calculate gradients. And talk about the current code and corresponding improvement due to JAX.&lt;/p&gt;
&lt;p&gt;The application of loglikelihood fitting to periodograms is discussed in [1]. Let us start with analyzing best-fit power spectrum (i) with different sets of optimizers namely: &lt;em&gt;minimize(method=’Nelder-Mead’, ’Powell’, ’CG’, ’BFGS’, ’Newton-CG’, ’L-BFGS-B’, ’TNC’, ’COBYLA’, ’SLSQP’, ’trust-constr’, ’dogleg’, ’trust-ncg’, ’trust-krylov’, ’trust-exact’). &lt;/em&gt;The problem setting shifts the start and test parameters to study the graph of best fit optimizer using different “methods” listed above. First, we will stick with the Powell optimizer and try to check what is the current sensitivity of the implementation.&lt;/p&gt;
&lt;p&gt;Currently, we seek to find a solution to the problem when the optimization algorithm often gets stuck in local minima, terminate without meeting its formal success criteria, or fails due to any contributing factor. Possible ways are: (1) add more Lorentzian components, (2) reduce the amplitude, (3) start the optimization process with parameters very far away from the true parameters, (4) experiment with the different optimizers/ “methods” to investigate if there is more superior algorithm compared to Powell.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*oEfdtoL2Fa0XAbjmugMvnA.png"&gt;&lt;figcaption&gt;Reference: blog.gitguardian.com&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;So far the &lt;em&gt;Powell&lt;/em&gt; and &lt;em&gt;Nelder-Mead &lt;/em&gt;gives almost the same best-fit curve compared to other optimizers, surprisingly even better than &lt;em&gt;BFGS(which is a well-known &lt;/em&gt;numerical optimizer for an iterative method for solving unconstrained nonlinear optimization problems. This directs to more investigation with (1) and (2) and (3). Both (2) and (3) makes the algorithm fail with the current &lt;em&gt;scipy.optimize.minimize, &lt;/em&gt;and we can see the&lt;em&gt; &lt;/em&gt;graph as given below.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*1SA0BwGc48I8qRozxuK6CQ.png"&gt;&lt;/figure&gt;&lt;p&gt;I am still holding on to try &lt;a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.scipy.optimize.minimize.html"&gt;jax.scipy.optimize.minimize&lt;/a&gt; instead of &lt;a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.scipy.optimize.minimize.html"&gt;scipy.optimize.minimize&lt;/a&gt; and analyze the increment in robustness. Another way to enhance the current algorithm alongside experimenting with different optimisers is:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Use a different gradient finding method.&lt;/li&gt;&lt;li&gt;Speed up objective function.&lt;/li&gt;&lt;li&gt;Reduce the number of design variables.&lt;/li&gt;&lt;li&gt;Choose a better initial guess.&lt;/li&gt;&lt;li&gt;Use parallel processing.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;In my next blog, I will provide a more detailed explanation of current events. In this blog, I highlighted the emphasis of analysis.&lt;/p&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;p&gt;[1] Maximum likelihood fitting of X-ray power density spectra: Application to high-frequency quasi-periodic oscillations from the neutron star X-ray binary 4U1608-522. Didier Barret, Simon Vaughan. &lt;a href="https://arxiv.org/abs/1112.0535"&gt;https://arxiv.org/abs/1112.0535&lt;/a&gt;&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=c34297f2dc81" width="1"&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2021/08/20210816_1603_rashmiraj137/</guid><pubDate>Mon, 16 Aug 2021 15:03:25 GMT</pubDate></item><item><title>GSoC update!</title><link>http://openastronomy.org/Universe_OA/posts/2021/08/20210803_0200_rashmiraj137/</link><dc:creator>Raj Rashmi</dc:creator><description>&lt;p&gt;GSoC started four months ago and it is not just about knowing more about the open-source that made the experience great! My mentors made it way cooler than I thought it would be. I was writing my Master thesis, for the last three months and surely, it has been a super productive summer for me! The best part is I get to do things at my own pace. My project particularly hasn’t been very easy to implement. I need to bridge a Machine Learning algorithm in the existing codebase. The fun part is venturing with different notebooks and figuring out with intuition, what could be efficient in terms of computational time, efficiency, cost etc. But as of now, the struggle has been to define the problem as exactly to achieve the result. But I will keep working on finding a solution with my mentor Daniela, and trust that struggle will bring some positive construction in Stingray.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/0*2GcUC2cgvKapk8Lj"&gt;&lt;/figure&gt;&lt;p&gt;The current data fit for the evaluation of likelihood happens using scipy.optimize.minimize function. However, there exists numerous ways to do this. SciPy optimize provides functions for minimizing (or maximizing) objective functions, possibly subject to constraints. It includes solvers for nonlinear problems (with support for both local and global optimization algorithms), linear programming, constrained and nonlinear least-squares, root finding, and curve fitting. The problem with the current minimization algorithm is that it converges at local minimum instead of global, i.e. it is not very robust. Recently, Machine Learning has evident development in such optimization tools. The strategy for ahead is that I will work on finding alternatives that potentially accelerate the code, makes it robust.&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=2d16a70cc267" width="1"&gt;
&lt;!-- TEASER_END --&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2021/08/20210803_0200_rashmiraj137/</guid><pubDate>Tue, 03 Aug 2021 01:00:58 GMT</pubDate></item><item><title>Insight of Implementation of JAX to stingray- GSoC coding period!</title><link>http://openastronomy.org/Universe_OA/posts/2021/07/20210705_1420_rashmiraj137/</link><dc:creator>Raj Rashmi</dc:creator><description>&lt;p&gt;In the last blog, I wrote about Introduction to JAX and Automatic Differentiation. In this one, my plan for the next stage of implementation. Currently, I am working on the modeling notebook (&lt;a href="https://github.com/StingraySoftware/notebooks/blob/main/Modeling/ModelingExamples.ipynb"&gt;https://github.com/StingraySoftware/notebooks/blob/main/Modeling/ModelingExamples.ipynb&lt;/a&gt;) to re-design it using JAX, especially to make optimization more robust by having JAX compute gradients on the likelihood function.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/380/1*u_c4S0h60T1IECOBQVTS1A.jpeg"&gt;&lt;/figure&gt;&lt;p&gt;My mentor Daniela highlighted the issue that the current implementation is not robust using NumPy. The plan is to keep working on the current modeling notebook replacing NumPy by jax.numpy and also use grad, jit, vmap, random functionality of JAX.&lt;br&gt;When it comes to re-design, understanding the current design and the possible drawback and issues with corresponding packages comes on you first and I am trying them out. One such challenge is importing emcee into jupyter notebook for sampling. Despite making sure, I download the dependency in the current virtual environment and then making sure I import emcee into the notebook, it is still acting weird and showing an error: emcee not installed! Can’t sample! It looks like a clash of dependencies.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/240/1*JtGB50sLscB1BBPt9k3pfw.jpeg"&gt;&lt;figcaption&gt;Trying to have fun while it lasts!&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;For now, the plan is to solve every bug I face in the journey and then proceed with understanding how everything connects and the next step is to come up with the report of optimization using JAX. Stay tuned for more on how JAX can accelerate and augment the current modeling framework.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;I would recommend one video for anyone who wants to understand the functionality of JAX better and relate more to my study (click &lt;a href="https://www.youtube.com/watch?v=0mVmRHMaOJ4&amp;amp;ab_channel=GoogleCloudTech"&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=1756040fa5ae" width="1"&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2021/07/20210705_1420_rashmiraj137/</guid><pubDate>Mon, 05 Jul 2021 13:20:55 GMT</pubDate></item><item><title>JAX-based automatic differentiation: Introduction of modern statistical modeling to Stingray</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210621_1404_rashmiraj137/</link><dc:creator>Raj Rashmi</dc:creator><description>&lt;p&gt;I assume everyone reading this is already aware of two classical forms of differentiation, namely symbolic and finite differentiation. Symbolic differentiation operates on expanded mathematical expressions which lead to inefficient code and introduction of truncation error while finite differentiation deals with round-off errors. Optimized calculation of derivatives is crucial when it comes to training neural networks or mathematical modeling using bayesian inference. Both classical methods are slow at computing partial derivatives of a function with respect to many inputs, as is needed for gradient-based optimization algorithms. Here, automatic differentiation comes to the rescue. Automatic differentiation is a powerful tool to automate the calculation of derivatives and is preferable to more traditional methods, especially when differentiating complex algorithms and mathematical functions.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*NzyxsrkiLjjyjiIuCf123w.png"&gt;&lt;figcaption&gt;Photo Source: Wikipedia&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Stingray is astrophysical spectral timing software, a library in python built to perform time series analysis and related tasks on astronomical light curves. JAX is a python library designed for high-performance numerical computing. Its API for numerical functions is based on NumPy, a collection of functions used in scientific computing. Both Python and NumPy are widely used and familiar, making JAX simple, flexible, and easy to adopt. It can differentiate through a large subset of python’s features, including loops, ifs, recursion, and closures, and it can even take derivatives of derivatives. Such modern differentiation packages deploy a broad range of computational techniques to improve the applicability, run time, and memory management.&lt;/p&gt;
&lt;p&gt;JAX utilizes the grad function transformation to convert a function into a function that returns the original function’s gradient, just like Autograd. Beyond that, JAX offers a function transformation jit for just-in-time compilation of existing functions and vmap and pmap for vectorization and parallelization, respectively.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*5hEIw7ou4eeAX-HnP_23_A.png"&gt;&lt;figcaption&gt;&lt;em&gt;Mini-MLP(Multiple layer Perceptron) execution time for 10,000 updates with a batch size of 1024. Source: AI Zone&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;As seen in the above figure, PyTorch has much more effective in terms of execution speed than TensorFlow when it came to implementing fully connected neural layers. For low-level implementations, on the other hand, JAX offers impressive speed-ups of an order of magnitude or more over the comparable Autograd library. JAX is faster than any other library when MLP implementation was limited to matrix multiplication operations.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;How do we decide the ideal library to go with?&lt;/strong&gt;&lt;/h4&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/626/1*8-_tFBCgszXxQwkMEwEcjA.jpeg"&gt;&lt;/figure&gt;&lt;p&gt;Our choice will first depend on the history of the project we start working on, if the code already uses PyTorch then most probably we will end up using PyTorch for writing our code. For general differentiable programming with low-level implementations of abstract mathematical concepts, JAX offers substantial advantages in speed and scale over Autograd while retaining much of Autograd’s simplicity and flexibility, while also offering surprisingly competitive performance against PyTorch and TensorFlow.&lt;/p&gt;
&lt;p&gt;I am implementing modern stingray modeling to Stingray software as a part of my GSoC project. Reference to Stingray source code: &lt;a href="https://github.com/StingraySoftware"&gt;https://github.com/StingraySoftware&lt;/a&gt;. Reference to JAX-based automatic differentiation: &lt;a href="https://jax.readthedocs.io/en/latest/jax-101/index.html"&gt;https://jax.readthedocs.io/en/latest/jax-101/index.html&lt;/a&gt;.&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=1bc26da7571f" width="1"&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210621_1404_rashmiraj137/</guid><pubDate>Mon, 21 Jun 2021 13:04:09 GMT</pubDate></item></channel></rss>