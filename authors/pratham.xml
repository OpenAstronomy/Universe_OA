<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Universe OpenAstronomy (Posts by Pratham)</title><link>http://openastronomy.org/Universe_OA/</link><description></description><atom:link href="http://openastronomy.org/Universe_OA/authors/pratham.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Sun, 06 Jul 2025 02:13:26 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Seeking Fast at Any Point in a BZ2 Compressed File</title><link>http://openastronomy.org/Universe_OA/posts/2025/07/20250702_0858_prtm2110/</link><dc:creator>Pratham</dc:creator><description>&lt;p&gt;Hey everyone welcome to the second episode of my Google Summer of Code project series, where I’m working on partial decompression for large datasets.&lt;/p&gt;
&lt;p&gt;So, what’s the big catch here? Well, the CO₂ dataset I’m working with is about 6 GB in its compressed .bz2form, and when you decompress it, it explodes into 50 GB. Most systems struggle to load that much data into memory or parse it into a DataFrame, either due to storage limits, memory, or swap issues.&lt;/p&gt;
&lt;p&gt;And obviously not everyone wants the whole 50 GB anyway. Usually, people need just a 1 GB chunk from somewhere inside. So decompressing the entire thing just to fetch a small part is a massive waste of time and resources.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;That’s where I built a pretty clever mechanism to handle this, saving both storage and a whole lot of time. Here’s how it works:&lt;/p&gt;
&lt;p&gt;The Problem with Seeking in bz2 If a user wants to randomly seek to any point in a decompressed stream (without decompressing the whole thing), technically you can, but there’s a catch bz2 doesn’t support random access. Even if you run something like: &lt;strong&gt;&lt;em&gt;`f.seek(1024*1024*1024)`&lt;/em&gt;&lt;/strong&gt; to jump to the 1 GB mark, under the hood, it still has to decompress from the start up to that point. Which means a lot of wasted time.&lt;/p&gt;
&lt;p&gt;The Trick is &lt;em&gt;Memory Mapping and Indexing&lt;/em&gt;, Here’s where the first optimization comes in that is memory mapping the decompressed byte offsets to the corresponding compressed byte positions. It’s a one-time cost for us maintainers. We generate this mapping and share an index file with users. Thankfully, there’s already a package called indexed_bzip2 that can seek to a compressed offset directly practically instantly.&lt;/p&gt;
&lt;p&gt;But users aren’t going to query in bytes. They’ll request data based on something meaningful in this case, wavenumber (in cm⁻¹).&lt;/p&gt;
&lt;p&gt;So I created another mapping: from wavenumber to the corresponding decompressed byte offset. When a user queries for a wavenumber, it first maps to the decompressed offset, and from there to the memory offset using the indexed_bzip2 mapping we created earlier. This mapping is built at intervals of 500 MB essentially creating fixed virtual blocks in the decompressed stream.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-BWgwHBWz8dmDtlQQY-ntw.png"&gt;&lt;/figure&gt;&lt;p&gt;If a user requests a wavenumber lower than a known value, it finds the largest known offset smaller than the current wavenumber. For a higher wavenumber, it finds the lower bound in the mapping. I finally got to use binary search in a real project for this lookup and it makes the lookup super fast.&lt;/p&gt;
&lt;p&gt;Keeping fixed-size blocks also makes it easier to maintain a caching mechanism later. If we had arbitrary block sizes, caching would be a pain. But with fixed 500 MB chunks, we can decompress extra blocks and combine them as needed, without overcomplicating the system.&lt;/p&gt;
&lt;p&gt;In the next episode, I’ll share some real benchmarks on how much time and storage this optimization saves. It’s been a fun problem to solve and finally a chance to use a binary search outside of textbook problems, stay tuned for the next episode ;)&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=5ee78f20670f" width="1"&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2025/07/20250702_0858_prtm2110/</guid><pubDate>Wed, 02 Jul 2025 07:58:20 GMT</pubDate></item><item><title>Yeah! I have a GSoC project with Radis</title><link>http://openastronomy.org/Universe_OA/posts/2025/06/20250607_0949_prtm2110/</link><dc:creator>Pratham</dc:creator><description>&lt;p&gt;Finally, I have been contributing to scientific open source for about a year now and it has taught me a lot I mean a lot, I still remember searching for a simple documentation issue so I could get it merged and call myself a “contributor” haha, and that got me started in scientific open source. Since then, I have been able to make some truly meaningful contributions to many projects, and here I am writing a blog for GSoC with Radis which is a pythonic library for fast line-by-line code for high resolution infrared molecular spectra, under the OpenAstronomy umbrella.&lt;/p&gt;
&lt;p&gt;So my project is cool ngl, and it is titled “Fast Parsing of Large Databases and Execution Bottlenecks” basically there exists a large highly compressed CO₂ spectroscopic database of size 6 GB file that decompresses to about 50 GB and takes at least 2.5 hours to parse and convert into a DataFrame. As you might expect, my project is about significantly reducing the parsing time and finding a workaround for storing only the “necessary” parts of the decompressed file.&lt;/p&gt;
&lt;p&gt;Radis is pythonic and sometimes python gets real slow if not used the way it is meant to be used, so my initial thought was to first clean up the existing code and use vectorised operations and with numba we should be able to see some real optimisation. But then I realized the current implementation already has the right vectorized operations on DataFrames, and Pandas’ vectorized methods are already implemented in optimized C/Cython loops. So there isn’t much more to do here other than replace a few overheads with other operations. After that, as discussed with my mentors I can use a C++ Single Instruction Multiple Data (SIMD) mechanism to parse the file and create a python interface on top of that with pybind11 or cython and other option but this will cost us portability as compilation will be a thing that is to considered. Other approach which is using vulkan API in python as it supports CPU as well GPU parallelism and its cross platform as it will works on all CPU architectures.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;The first thing I did was profile the hit2df function for the NO₂ molecule, which is much smaller compared to CO₂ but uses the same operations. That gave a good idea of the actual bottlenecks and where I need to work.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*9OApPZeVUuVw2vAwSCqGQg.png"&gt;&lt;figcaption&gt;Profiling of `hit2df` function for NO₂ molecule&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;As you can see, most of the time is spent in post_process_hitran_data and which is expected because this function calls parse_local_quanta and parse_global_quanta both applies regex across multiple string columns, which is slow at scale, so I switced to fixed‐width slicing to avoid that per-row overhead, of course this was possible as the dataset is consistent and doesn’t require regex at all (I really hope).&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/0*4T1gkGX-4N4_CzO3"&gt;&lt;/figure&gt;&lt;p&gt;This resulted in a clean improvement of about 38%, which will serve as the default option, along with the Vulkan-API mechanism, which I expect will yield a huge optimization compared to the default. That’s something we will see that in the next episode, stay tuned ;)&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=113900105c46" width="1"&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2025/06/20250607_0949_prtm2110/</guid><pubDate>Sat, 07 Jun 2025 08:49:18 GMT</pubDate></item></channel></rss>